{"content": "\n\n<!DOCTYPE html>\n<html lang=\"en\" class=\"no-js\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <meta name=\"applicable-device\" content=\"pc,mobile\">\n    <meta name=\"access\" content=\"No\">\n\n    \n\n    <meta name=\"twitter:site\" content=\"@SpringerLink\"/>\n\n    <meta name=\"twitter:card\" content=\"summary\"/>\n\n    <meta name=\"twitter:image:alt\" content=\"Content cover image\"/>\n\n    <meta name=\"twitter:title\" content=\"A review of design intelligence: progress, problems, and challenges\"/>\n\n    <meta name=\"twitter:description\" content=\"Frontiers of Information Technology &amp; Electronic Engineering - Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in...\"/>\n\n    <meta name=\"twitter:image\" content=\"https://static-content.springer.com/cover/journal/11714/20/12.jpg\"/>\n\n    <meta name=\"journal_id\" content=\"11714\"/>\n\n    <meta name=\"dc.title\" content=\"A review of design intelligence: progress, problems, and challenges\"/>\n\n    <meta name=\"dc.source\" content=\"Frontiers of Information Technology &amp; Electronic Engineering 2019 20:12\"/>\n\n    <meta name=\"dc.format\" content=\"text/html\"/>\n\n    <meta name=\"dc.publisher\" content=\"Springer\"/>\n\n    <meta name=\"dc.date\" content=\"2020-02-13\"/>\n\n    <meta name=\"dc.type\" content=\"ReviewPaper\"/>\n\n    <meta name=\"dc.language\" content=\"En\"/>\n\n    <meta name=\"dc.copyright\" content=\"2019 Zhejiang University and Springer-Verlag GmbH Germany, part of Springer Nature\"/>\n\n    <meta name=\"dc.rightsAgent\" content=\"journalpermissions@springernature.com\"/>\n\n    <meta name=\"dc.description\" content=\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.\"/>\n\n    <meta name=\"prism.issn\" content=\"2095-9230\"/>\n\n    <meta name=\"prism.publicationName\" content=\"Frontiers of Information Technology &amp; Electronic Engineering\"/>\n\n    <meta name=\"prism.publicationDate\" content=\"2020-02-13\"/>\n\n    <meta name=\"prism.volume\" content=\"20\"/>\n\n    <meta name=\"prism.number\" content=\"12\"/>\n\n    <meta name=\"prism.section\" content=\"ReviewPaper\"/>\n\n    <meta name=\"prism.startingPage\" content=\"1595\"/>\n\n    <meta name=\"prism.endingPage\" content=\"1617\"/>\n\n    <meta name=\"prism.copyright\" content=\"2019 Zhejiang University and Springer-Verlag GmbH Germany, part of Springer Nature\"/>\n\n    <meta name=\"prism.rightsAgent\" content=\"journalpermissions@springernature.com\"/>\n\n    <meta name=\"prism.url\" content=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>\n\n    <meta name=\"prism.doi\" content=\"doi:10.1631/FITEE.1900398\"/>\n\n    <meta name=\"citation_pdf_url\" content=\"https://link.springer.com/content/pdf/10.1631/FITEE.1900398.pdf\"/>\n\n    <meta name=\"citation_fulltext_html_url\" content=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>\n\n    <meta name=\"citation_journal_title\" content=\"Frontiers of Information Technology &amp; Electronic Engineering\"/>\n\n    <meta name=\"citation_journal_abbrev\" content=\"Front Inform Technol Electron Eng\"/>\n\n    <meta name=\"citation_publisher\" content=\"Zhejiang University Press\"/>\n\n    <meta name=\"citation_issn\" content=\"2095-9230\"/>\n\n    <meta name=\"citation_title\" content=\"A review of design intelligence: progress, problems, and challenges\"/>\n\n    <meta name=\"citation_volume\" content=\"20\"/>\n\n    <meta name=\"citation_issue\" content=\"12\"/>\n\n    <meta name=\"citation_publication_date\" content=\"2019/12\"/>\n\n    <meta name=\"citation_online_date\" content=\"2020/02/13\"/>\n\n    <meta name=\"citation_firstpage\" content=\"1595\"/>\n\n    <meta name=\"citation_lastpage\" content=\"1617\"/>\n\n    <meta name=\"citation_article_type\" content=\"Review\"/>\n\n    <meta name=\"citation_language\" content=\"en\"/>\n\n    <meta name=\"dc.identifier\" content=\"doi:10.1631/FITEE.1900398\"/>\n\n    <meta name=\"DOI\" content=\"10.1631/FITEE.1900398\"/>\n\n    <meta name=\"citation_doi\" content=\"10.1631/FITEE.1900398\"/>\n\n    <meta name=\"description\" content=\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In \"/>\n\n    <meta name=\"dc.creator\" content=\"Tang, Yong-chuan\"/>\n\n    <meta name=\"dc.creator\" content=\"Huang, Jiang-jie\"/>\n\n    <meta name=\"dc.creator\" content=\"Yao, Meng-ting\"/>\n\n    <meta name=\"dc.creator\" content=\"Wei, Jia\"/>\n\n    <meta name=\"dc.creator\" content=\"Li, Wei\"/>\n\n    <meta name=\"dc.creator\" content=\"He, Yong-xing\"/>\n\n    <meta name=\"dc.creator\" content=\"Li, Ze-jian\"/>\n\n    <meta name=\"dc.subject\" content=\"Computer Science, general\"/>\n\n    <meta name=\"dc.subject\" content=\"Electrical Engineering\"/>\n\n    <meta name=\"dc.subject\" content=\"Computer Hardware\"/>\n\n    <meta name=\"dc.subject\" content=\"Computer Systems Organization and Communication Networks\"/>\n\n    <meta name=\"dc.subject\" content=\"Electronics and Microelectronics, Instrumentation\"/>\n\n    <meta name=\"dc.subject\" content=\"Communications Engineering, Networks\"/>\n\n    <meta name=\"citation_reference\" content=\"Arjovsky M, Chintala S, Bottou L, 2017. Wasserstein generative adversarial networks. Proc 34th Int Conf on Machine Learning, p.298&#8211;321.\"/>\n\n    <meta name=\"citation_reference\" content=\"Aubry M, Maturana D, Efros AA, et al., 2014. Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3762&#8211;3769. \nhttps://doi.org/10.1109/CVPR.2014.487\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Image Process; citation_title=Filling-in by joint interpolation of vector fields and gray levels; citation_author=C Ballester, M Bertalmio, V Caselles; citation_volume=10; citation_issue=8; citation_publication_date=2001; citation_pages=1200-1211; citation_id=CR3\"/>\n\n    <meta name=\"citation_reference\" content=\"Bertalmio M, Sapiro G, Caselles V, et al., 2000. Image in-painting. Proc 27th Annual Conf on Computer Graphics and Interactive Techniques, p.417&#8211;424. \nhttps://doi.org/10.1145/344779.344972\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Bharadhwaj H, Park H, Lim BY, 2018. RecGAN: recurrent generative adversarial networks for recommendation systems. Proc 12th ACM Conf on Recommender Systems, p.372&#8211;376. \nhttps://doi.org/10.1145/3240323.3240383\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=AI Mag; citation_title=Computer models of creativity; citation_author=MA Boden; citation_volume=30; citation_issue=3; citation_publication_date=2009; citation_pages=23-34; citation_id=CR6\"/>\n\n    <meta name=\"citation_reference\" content=\"Brock A, Donahue J, Simonyan K, 2018. Large scale GAN training for high fidelity natural image synthesis. \nhttps://doi.org/1809.11096\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Bruna J, Sprechmann P, LeCun Y, 2015. Super-resolution with deep convolutional sufficient statistics. \nhttps://doi.org/1511.05666\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Idea inspire 3.0&#8212;a tool for analogical design; citation_inbook_title=Research into Design for Communities; citation_publication_date=2017; citation_pages=475-485; citation_id=CR9; citation_author=A Chakrabarti; citation_author=L Siddharth; citation_author=M Dinakar; citation_publisher=Springer\"/>\n\n    <meta name=\"citation_reference\" content=\"Champandard AJ, 2016. Semantic style transfer and turning two-bit doodles into fine artworks. \nhttps://doi.org/1603.01768\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Chan C, Ginosar S, Zhou TH, et al., 2018. Everybody dance now. \nhttps://doi.org/1808.07371\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Chen DD, Yuan L, Liao J, et al., 2018. Stereoscopic neural style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.6654&#8211;6663. \nhttps://doi.org/10.1109/CVPR.2018.00696\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=J Vis Commun Image Represent; citation_title=An artificial intelligence based data-driven approach for design ideation; citation_author=LQ Chen, P Wang, H Dong; citation_volume=61; citation_publication_date=2019; citation_pages=10-22; citation_id=CR13\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Finding Image Features Associated with High Aesthetic Value by Machine Learning; citation_inbook_title=Evolutionary and Biologically Inspired Music, Sound, Art and Design; citation_publication_date=2013; citation_pages=47-58; citation_id=CR14; citation_author=Vic Ciesielski; citation_author=Perry Barile; citation_author=Karen Trist; citation_publisher=Springer Berlin Heidelberg\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=The Inmates Are Running the Asylum; citation_publication_date=1999; citation_id=CR15; citation_author=A Cooper; citation_publisher=SAMS\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=About Face 2.0: the Essentials of Interaction Design; citation_publication_date=2003; citation_id=CR16; citation_author=A Cooper; citation_author=RM Reimann; citation_publisher=John Wiley &amp; Sons\"/>\n\n    <meta name=\"citation_reference\" content=\"Dash A, Gamboa JCB, Ahmed S, et al., 2017. TAC-GAN-text conditioned auxiliary classifier generative adversarial network. \nhttps://doi.org/1703.06412\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Studying Aesthetics in Photographic Images Using a Computational Approach; citation_inbook_title=Computer Vision &#8211; ECCV 2006; citation_publication_date=2006; citation_pages=288-301; citation_id=CR18; citation_author=Ritendra Datta; citation_author=Dhiraj Joshi; citation_author=Jia Li; citation_author=James Z. Wang; citation_publisher=Springer Berlin Heidelberg\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=An Evolutionary Approach to Case Adaptation; citation_inbook_title=Case-Based Reasoning Research and Development; citation_publication_date=1999; citation_pages=162-173; citation_id=CR19; citation_author=Andr&#233;s de G&#243;mez Silva Garza; citation_author=Mary Lou Maher; citation_publisher=Springer Berlin Heidelberg\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Artif Intell Rev; citation_title=An introduction to and comparison of computational creativity and design computing; citation_author=AG de Silva Garza; citation_volume=51; citation_issue=1; citation_publication_date=2019; citation_pages=61-76; citation_id=CR20\"/>\n\n    <meta name=\"citation_reference\" content=\"Deng J, Dong W, Socher R, et al., 2009. ImageNet: a large-scale hierarchical image database. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.248&#8211;255. \nhttps://doi.org/10.1109/CVPR.2009.5206848\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Deng YB, Loy CC, Tang XO, 2018. Aesthetic-driven image enhancement by adversarial learning. Proc 26th ACM Int Conf on Multimedia, p.870&#8211;878. \nhttps://doi.org/10.1145/3240508.3240531\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Donahue J, Kr&#228;henb&#252;hl P, Darrell T, 2016. Adversarial feature learning. \nhttps://doi.org/1605.09782\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Hum Comput Stud; citation_title=Webthetics: quantifying webpage aesthetics with deep learning; citation_author=Q Dou, XS Zheng, TF Sun; citation_volume=124; citation_publication_date=2019; citation_pages=56-66; citation_id=CR24\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=J Pers Soc Psychol; citation_title=Cognitive stimulation in brainstorming; citation_author=KL Dugosh, PB Paulus, EJ Roland; citation_volume=79; citation_issue=5; citation_publication_date=2000; citation_pages=722-735; citation_id=CR25\"/>\n\n    <meta name=\"citation_reference\" content=\"Dumoulin V, Visin F, 2016. A guide to convolution arithmetic for deep learning. \nhttps://doi.org/1603.07285\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=MRI: Clinical Magnetic Resonance Imaging; citation_publication_date=1996; citation_id=CR27; citation_author=RR Edelman; citation_author=JR Hesselink; citation_author=MB Zlatkin; citation_publisher=Saunders\"/>\n\n    <meta name=\"citation_reference\" content=\"Efros AA, Freeman WT, 2001. Image quilting for texture synthesis and transfer. Proc 28th Annual Conf on Computer Graphics and Interactive Techniques, p.341&#8211;346. \nhttps://doi.org/10.1145/383259.383296\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Elgammal A, Liu B, Elhoseiny M, et al., 2017. CAN: creative adversarial networks, generating &#8220;art&#8221; by learning about styles and deviating from style norms. \nhttps://doi.org/1706.07068\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Fang H, Zhang M, 2017. Creatism: a deep-learning photographer capable of creating professional work. \nhttps://doi.org/1707.03491\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Faste H, Rachmel N, Essary R, et al., 2013. Brainstorm, chainstorm, cheatstorm, tweetstorm: new ideation strategies for distributed HCI design. Proc Conf on Human Factors in Computing Systems, p.1343&#8211;1352. \nhttps://doi.org/10.1145/2470654.2466177\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Res Eng Des; citation_title=Design-by-analogy: experimental evaluation of a functional analogy search methodology for concept generation improvement; citation_author=K Fu, J Murphy, M Yang; citation_volume=26; citation_issue=1; citation_publication_date=2015; citation_pages=77-95; citation_id=CR32\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Bull Amer Math Soc; citation_title=Birkhoff on aesthetic measure; citation_author=CA Garabedian; citation_volume=40; citation_issue=1; citation_publication_date=1934; citation_pages=7-10; citation_id=CR33\"/>\n\n    <meta name=\"citation_reference\" content=\"Gatys L, Ecker A, Bethge M, 2016a. Image style transfer using convolutional neural networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2414&#8211;2423. \nhttps://doi.org/10.1109/CVPR.2016.265\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=J Vis; citation_title=A neural algorithm of artistic style; citation_author=L Gatys, A Ecker, M Bethge; citation_volume=16; citation_issue=12; citation_publication_date=2016; citation_pages=326; citation_id=CR35\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=AI Mag; citation_title=Design prototypes: a knowledge representation schema for design; citation_author=JS Gero; citation_volume=11; citation_issue=4; citation_publication_date=1990; citation_pages=26-36; citation_id=CR36\"/>\n\n    <meta name=\"citation_reference\" content=\"Gilon K, Chan J, Ng FY, et al., 2018. Analogy mining for specific design needs. Proc CHI Conf on Human Factors in Computing Systems, p.121. \nhttps://doi.org/10.1145/3173574.3173695\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=AI Edam; citation_title=Structure, behavior, and function of complex systems: the structure, behavior, and function modeling language; citation_author=AK Goel, S Rugaber, S Vattam; citation_volume=23; citation_issue=1; citation_publication_date=2009; citation_pages=23-35; citation_id=CR38\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Variances in the impact of visual stimuli on design problem solving performance; citation_author=G Goldschmidt, M Smolkov; citation_volume=27; citation_issue=5; citation_publication_date=2006; citation_pages=549-569; citation_id=CR39\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Non-photorealistic Rendering; citation_publication_date=2001; citation_id=CR40; citation_author=B Gooch; citation_author=A Gooch; citation_publisher=A K Peters/CRC Press\"/>\n\n    <meta name=\"citation_reference\" content=\"Goodfellow I, Pouget-Abadie J, Mirza M, et al., 2014. Generative adversarial nets. Proc 27th Int Conf on Neural Information Processing Systems, p.2672&#8211;2680.\"/>\n\n    <meta name=\"citation_reference\" content=\"Grudin J, Pruitt J, 2002. Personas, participatory design, and product development: an infrastructure for engagement. Proc 7th Biennial Participatory Design Conf, p.144&#8211;152.\"/>\n\n    <meta name=\"citation_reference\" content=\"Gulrajani I, Ahmed F, Arjovsky M, et al., 2017. Improved training of Wasserstein GANs. Advances in Neural Information Proc Systems, p.5767&#8211;5777.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Artif Intell Eng Des Anal Manuf; citation_title=A computational tool for creative idea generation based on analogical reasoning and ontology; citation_author=J Han, F Shi, LQ Chen; citation_volume=32; citation_issue=4; citation_publication_date=2018; citation_pages=462-477; citation_id=CR44\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=J Intell Manuf; citation_title=An evolutionary computation based method for creative design inspiration generation; citation_author=J Hao, YJ Zhou, QF Zhao; citation_volume=30; citation_issue=4; citation_publication_date=2019; citation_pages=1673-1691; citation_id=CR45\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=The UX Book: Process and Guidelines for Ensuring a Quality User Experience; citation_publication_date=2012; citation_id=CR46; citation_author=R Hartson; citation_author=PS Pyla; citation_publisher=Elsevier\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Patt Anal Mach Intell; citation_title=Image completion approaches using the statistics of similar patches; citation_author=KM He, J Sun; citation_volume=36; citation_issue=12; citation_publication_date=2014; citation_pages=2423-2435; citation_id=CR47\"/>\n\n    <meta name=\"citation_reference\" content=\"Hertzmann A, Jacobs CE, Oliver N, et al., 2001. Image analogies. Proc 28th Annual Conf on Computer Graphics and Interactive Techniques, p.327&#8211;340. \nhttps://doi.org/10.1145/383259.383295\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=ACM Comput Surv; citation_title=How generative adversarial networks and their variants work: an overview; citation_author=YJ Hong, U Hwang, J Yoo; citation_volume=52; citation_issue=1; citation_publication_date=2019; citation_pages=10; citation_id=CR49\"/>\n\n    <meta name=\"citation_reference\" content=\"Huang HZ, Wang H, Luo WH, et al., 2017. Real-time neural style transfer for videos. IEEE Conf on Computer Vision and Pattern Recognition, p.7044&#8211;7052. \nhttps://doi.org/10.1109/CVPR.2017.745\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Huang X, Belongie S, 2017. Arbitrary style transfer in realtime with adaptive instance normalization. Proc IEEE Int Conf on Computer Vision, p.1501&#8211;1510. \nhttps://doi.org/10.1109/ICCV.2017.167\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=ACM Transactions on Graphics; citation_title=Globally and locally consistent image completion; citation_author=Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa; citation_volume=36; citation_issue=4; citation_publication_date=2017; citation_pages=1-14; citation_id=CR52\"/>\n\n    <meta name=\"citation_reference\" content=\"Isola P, Zhu JY, Zhou TH, et al., 2017. Image-to-image translation with conditional adversarial networks. IEEE Conf on Computer Vision and Pattern Recognition, p.5967&#8211;5976. \nhttps://doi.org/10.1109/CVPR.2017.632\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Proc Assoc Inform Sci Technol; citation_title=Viewed by too many or viewed too little: using information dissemination for audience segmentation; citation_author=BJ Jansen, SG Jung, J Salminen; citation_volume=54; citation_issue=1; citation_publication_date=2017; citation_pages=189-196; citation_id=CR54\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Design fixation; citation_author=DG Jansson, SM Smith; citation_volume=12; citation_issue=1; citation_publication_date=1991; citation_pages=3-11; citation_id=CR55\"/>\n\n    <meta name=\"citation_reference\" content=\"Jia J, Huang J, Shen GY, et al., 2016. Learning to appreciate the aesthetic effects of clothing. Proc 30th AAAI Conf on Artificial Intelligence, p.1216&#8211;1222.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Des Creat Innov; citation_title=Testing ideation performance on a large set of designers: effects of analogical distance; citation_author=L Jia, N Becattini, G Cascini; citation_volume=8; citation_issue=1; citation_publication_date=2020; citation_pages=31-45; citation_id=CR57\"/>\n\n    <meta name=\"citation_reference\" content=\"Jiang SH, Fu Y, 2017. Fashion style generator. Proc 26th Int Joint Conf on Artificial Intelligence, p.3721&#8211;3727. \nhttps://doi.org/10.24963/ijcai.2017/520\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Jing YC, Yang YZ, Feng ZL, et al., 2019. Neural style transfer: a review. IEEE Trans Vis Comput Graph, in press. \nhttps://doi.org/10.1109/tvcg.2019.2921336\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Jo Y, Park J, 2019. SC-FEGAN: face editing generative adversarial network with user&#8217;s sketch and color. \nhttps://doi.org/1902.06838\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Perceptual Losses for Real-Time Style Transfer and Super-Resolution; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=694-711; citation_id=CR61; citation_author=Justin Johnson; citation_author=Alexandre Alahi; citation_author=Li Fei-Fei; citation_publisher=Springer International Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"Karras T, Laine S, Aila T, 2019. A style-based generator architecture for generative adversarial networks. The IEEE Conf on Computer Vision and Pattern Recognition, p.4401&#8211;4410.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=The International Handbook of Creativity; citation_publication_date=2006; citation_id=CR63; citation_author=JC Kaufman; citation_author=RJ Sternberg; citation_publisher=Edward Elgar Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Acoust Speech Signal Process; citation_title=Cubic convolution interpolation for digital image processing; citation_author=R Keys; citation_volume=29; citation_issue=6; citation_publication_date=1981; citation_pages=1153-1160; citation_id=CR64\"/>\n\n    <meta name=\"citation_reference\" content=\"Kim J, Lee JK, Lee KM, 2016. Accurate image superresolution using very deep convolutional networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1646&#8211;1654. \nhttps://doi.org/10.1109/CVPR.2016.182\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Kingma DP, Welling M, 2013. Auto-encoding variational Bayes. https://arxiv.org/abs/1312.6114\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Photo Aesthetics Ranking Network with Attributes and Content Adaptation; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=662-679; citation_id=CR67; citation_author=Shu Kong; citation_author=Xiaohui Shen; citation_author=Zhe Lin; citation_author=Radomir Mech; citation_author=Charless Fowlkes; citation_publisher=Springer International Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Learning Multiple Layers of Features from Tiny Images; citation_publication_date=2009; citation_id=CR68; citation_author=A Krizhevsky; citation_author=G Hinton; citation_publisher=University of Toronto\"/>\n\n    <meta name=\"citation_reference\" content=\"Kwak H, An J, Jansen BJ, 2017. Automatic generation of personas using YouTube social media data. Proc 50th Hawaii Int Conf on System Sciences, p.833&#8211;842.\"/>\n\n    <meta name=\"citation_reference\" content=\"Larsen ABL, S&#248;nderby SK, Larochelle H, et al., 2016. Autoencoding beyond pixels using a learned similarity metric. Proc 33rd Int Conf on Machine Learning, p.1558&#8211;1566.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Proc IEEE; citation_title=Gradient-based learning applied to document recognition; citation_author=Y LeCun, L Bottou, Y Bengio; citation_volume=86; citation_issue=11; citation_publication_date=1998; citation_pages=2278-2323; citation_id=CR71\"/>\n\n    <meta name=\"citation_reference\" content=\"Ledig C, Theis L, Husz&#225;r F, et al., 2017. Photo-realistic single image super-resolution using a generative adversarial network. IEEE Conf on Computer Vision and Pattern Recognition, p.105&#8211;114. \nhttps://doi.org/10.1109/CVPR.2017.19\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=702-716; citation_id=CR73; citation_author=Chuan Li; citation_author=Michael Wand; citation_publisher=Springer International Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE J Sel Top Signal Process; citation_title=Aesthetic visual quality assessment of paintings; citation_author=CC Li, T Chen; citation_volume=3; citation_issue=2; citation_publication_date=2009; citation_pages=236-252; citation_id=CR74\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=J Opt Soc Am A; citation_title=Polarization-dependent effects of an Airy beam due to the spin-orbit coupling; citation_author=HH Li, JG Wang, MM Tang; citation_volume=34; citation_issue=7; citation_publication_date=2017; citation_pages=1114-1118; citation_id=CR75\"/>\n\n    <meta name=\"citation_reference\" content=\"Li XT, Liu SF, Kautz J, et al., 2019. Learning linear transformations for fast arbitrary style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3809&#8211;3817.\"/>\n\n    <meta name=\"citation_reference\" content=\"Li YJ, Fang C, Yang JM, et al., 2017. Universal style transfer via feature transforms. Proc 31st Conf on Neural Information Processing Systems, p.386&#8211;396.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Image Inpainting for Irregular Holes Using Partial Convolutions; citation_inbook_title=Computer Vision &#8211; ECCV 2018; citation_publication_date=2018; citation_pages=89-105; citation_id=CR78; citation_author=Guilin Liu; citation_author=Fitsum A. Reda; citation_author=Kevin J. Shih; citation_author=Ting-Chun Wang; citation_author=Andrew Tao; citation_author=Bryan Catanzaro; citation_publisher=Springer International Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=BT Technol; citation_title=ConceptNet&#8212;a practical commonsense reasoning tool-kit; citation_author=H Liu, P Singh; citation_volume=22; citation_issue=4; citation_publication_date=2004; citation_pages=211-226; citation_id=CR79\"/>\n\n    <meta name=\"citation_reference\" content=\"Liu MY, Huang X, Mallya A, et al., 2019. Few-shot unsupervised image-to-image translation. \nhttps://doi.org/1905.01723\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Liu ZW, Luo P, Wang XG, et al., 2015. Deep learning face attributes in the wild. Proc IEEE Int Conf on Computer Vision, p.3730&#8211;3738. \nhttps://doi.org/10.1109/ICCV.2015.425\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=User-Centered Design: a Developer&#8217;s Guide to Building User-Friendly Applications; citation_publication_date=2013; citation_id=CR82; citation_author=T Lowdermilk; citation_publisher=O&#8217;Reilly\"/>\n\n    <meta name=\"citation_reference\" content=\"Lu X, Lin Z, Shen XH, et al., 2015. Deep multi-patch aggregation network for image style, aesthetics, and quality estimation. Proc IEEE Int Conf on Computer Vision, p.990&#8211;998. \nhttps://doi.org/10.1109/ICCV.2015.119\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Photo and Video Quality Evaluation: Focusing on the Subject; citation_inbook_title=Lecture Notes in Computer Science; citation_publication_date=2008; citation_pages=386-399; citation_id=CR84; citation_author=Yiwen Luo; citation_author=Xiaoou Tang; citation_publisher=Springer Berlin Heidelberg\"/>\n\n    <meta name=\"citation_reference\" content=\"Ma S, Liu J, Chen WC, 2017. A-lamp: adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. Proc 30th IEEE Conf on Computer Vision and Pattern Recognition, p.722&#8211;731. \nhttps://doi.org/10.1109/CVPR.2017.84\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=User requirements analysis; citation_inbook_title=Usability: Gaining a Competitive Edge; citation_publication_date=2002; citation_pages=133-148; citation_id=CR86; citation_author=M Maguire; citation_author=N Bevan; citation_publisher=Springer\"/>\n\n    <meta name=\"citation_reference\" content=\"Mai L, Jin HL, Liu F, 2016. Composition-preserving deep photo aesthetics assessment. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.497&#8211;506. \nhttps://doi.org/10.1109/CVPR.2016.60\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Matthews T, Judge T, Whittaker S, 2012. How do designers and user experience professionals actually perceive and use personas? Proc Conf on Human Factors in Computing Systems, p.1219&#8211;1228. \nhttps://doi.org/10.1145/2207676.2208573\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Des Creat Innov; citation_title=The obscure features hypothesis in design innovation; citation_author=T McCaffrey, S Krishnamurty; citation_volume=3; citation_issue=1; citation_publication_date=2015; citation_pages=1-28; citation_id=CR89\"/>\n\n    <meta name=\"citation_reference\" content=\"McGinn J, Kotamraju N, 2008. Data-driven persona development. Proc Conf on Human Factors in Computing Systems, p.1521&#8211;1524. \nhttps://doi.org/10.1145/1357054.1357292\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Personas and user-centered design: how can personas benefit product design processes?; citation_author=T Miaskiewicz, KA Kozar; citation_volume=32; citation_issue=5; citation_publication_date=2011; citation_pages=417-430; citation_id=CR91\"/>\n\n    <meta name=\"citation_reference\" content=\"Mikolov T, Chen K, Corrado G, et al., 2013. Efficient estimation of word representations in vector space. \nhttps://doi.org/1301.3781\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Commun ACM; citation_title=Wordnet: a lexical database for English; citation_author=GA Miller; citation_volume=38; citation_issue=11; citation_publication_date=1995; citation_pages=39-41; citation_id=CR93\"/>\n\n    <meta name=\"citation_reference\" content=\"Mirza M, Osindero S, 2014. Conditional generative adversarial nets. \nhttps://doi.org/1411.1784\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Miyato T, Kataoka T, Koyama M, et al., 2018. Spectral normalization for generative adversarial networks. Int Conf on Learning Representations.\"/>\n\n    <meta name=\"citation_reference\" content=\"Murray N, Marchesotti L, Perronnin F, 2012. AVA: a large-scale database for aesthetic visual analysis. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2408&#8211;2415. \nhttps://doi.org/10.1109/CVPR.2012.6247954\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Nazeri K, Ng E, Joseph T, et al., 2019. Edgeconnect: generative image inpainting with adversarial edge learning. \nhttps://doi.org/1901.00212\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Refined metrics for measuring ideation effectiveness; citation_author=BA Nelson, JO Wilson, D Rosen; citation_volume=30; citation_issue=6; citation_publication_date=2009; citation_pages=737-743; citation_id=CR98\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Sociotechnol Knowl Dev; citation_title=A template for design personas: analysis of 47 persona descriptions from Danish industries and organizations; citation_author=L Nielsen, KS Hansen, J Stage; citation_volume=7; citation_issue=1; citation_publication_date=2015; citation_pages=45-61; citation_id=CR99\"/>\n\n    <meta name=\"citation_reference\" content=\"Niles I, Pease A, 2001. Towards a standard upper ontology. Proc Int Conf on Formal Ontology in Information Systems, p.2&#8211;9. \nhttps://doi.org/10.1145/505168.505170\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Nilsback ME, Zisserman A, 2008. Automated flower classification over a large number of classes. Proc 6th Indian Conf on Computer Vision, Graphics &amp; Image Processing, p.722&#8211;729. \nhttps://doi.org/10.1109/ICVGIP.2008.47\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Odena A, Olah C, Shlens J, 2017. Conditional image synthesis with auxiliary classifier GANs. Proc 34th Int Conf on Machine Learning, p.4043&#8211;4055.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Front Inform Technol Electron Eng; citation_title=Special issue on artificial intelligence 2.0; citation_author=YH Pan; citation_volume=18; citation_issue=1; citation_publication_date=2017; citation_pages=1-2; citation_id=CR103\"/>\n\n    <meta name=\"citation_reference\" content=\"Park T, Liu MY, Wang TC, et al., 2019. Semantic image synthesis with spatially-adaptive normalization. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2337&#8211;2346.\"/>\n\n    <meta name=\"citation_reference\" content=\"Pathak D, Kr&#228;henb&#252;hl P, Donahue J, et al., 2016. Context encoders: feature learning by inpainting. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2536&#8211;2544. \nhttps://doi.org/10.1109/CVPR.2016.278\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Peeters JR, Verhaegen PA, Vandevenne D, et al., 2010. Refined metrics for measuring novelty in ideation. ID-MME Virtual Concept Research in Interaction Design, Article 4.\"/>\n\n    <meta name=\"citation_reference\" content=\"Perera D, Zimmermann R, 2019. CNGAN: generative adversarial networks for cross-network user preference generation for non-overlapped users. World Wide Web Conf, p.3144&#8211;3150. \nhttps://doi.org/10.1145/3308558.3313733\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=The Persona Lifecycle: Keeping People in Mind Throughout Product Design; citation_publication_date=2005; citation_id=CR108; citation_author=J Pruitt; citation_author=T Adlin; citation_publisher=Elsevier\"/>\n\n    <meta name=\"citation_reference\" content=\"Radford A, Metz L, Chintala S, 2016. Unsupervised representation learning with deep convolutional generative adversarial networks. Proc 4th Int Conf on Learning Representations.\"/>\n\n    <meta name=\"citation_reference\" content=\"Reed SE, Akata Z, Yan XC, et al., 2016a. Generative adversarial text to image synthesis. Proc 33rd Int Conf on Machine Learning, p.1681&#8211;1690.\"/>\n\n    <meta name=\"citation_reference\" content=\"Reed SE, Akata Z, Mohan S, et al., 2016b. Learning what and where to draw. Advances in Neural Information Processing Systems, p.217&#8211;225.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Comput Graph Appl; citation_title=Informational aesthetics measures; citation_author=J Rigau, M Feixas, M Sbert; citation_volume=28; citation_issue=2; citation_publication_date=2008; citation_pages=24-34; citation_id=CR112\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Artificial Intelligence: a Modern Approach; citation_publication_date=2016; citation_id=CR113; citation_author=SJ Russell; citation_author=P Norvig; citation_publisher=Pearson Education Limited\"/>\n\n    <meta name=\"citation_reference\" content=\"Saleh B, Elgammal A, 2015. Large-scale classification of fine-art paintings: learning the right metric on the right feature. \nhttps://doi.org/1505.00855\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Salimans T, Goodfellow IJ, Zaremba W, et al., 2016. Improved techniques for training GANs. Advances in Neural Information Processing Systems, p.2226&#8211;2234.\"/>\n\n    <meta name=\"citation_reference\" content=\"Salminen J, Seng&#252;n S, Kwak H, et al., 2017. Generating cultural personas from social data: a perspective of middle eastern users. Proc 5th Int Conf on Future Internet of Things and Cloud Workshops, p.120&#8211;125. \nhttps://doi.org/10.1109/FiCloudW.2017.97\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Persona Stud; citation_title=Are personas done? Evaluating their usefulness in the age of digital analytics; citation_author=J Salminen, BJ Jansen, J An; citation_volume=4; citation_issue=2; citation_publication_date=2018; citation_pages=47-65; citation_id=CR117\"/>\n\n    <meta name=\"citation_reference\" content=\"Salminen J, Jung SG, An J, et al., 2018b. Findings of a user study of automatically generated personas. Proc Conf on Human Factors in Computing Systems, p.LBW097. \nhttps://doi.org/10.1145/3170427.3188470\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Salminen J, Eng&#252;n S, Jung SG, et al., 2019. Design issues in automatically generated persona profiles: a qualitative analysis from 38 think-aloud transcripts. Proc Conf on Human Information Interaction and Retrieval, p.225&#8211;229. \nhttps://doi.org/10.1145/3295750.3298942\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Schwarz K, Wieschollek P, Lensch HPA, 2018. Will people like your image? Learning the aesthetic space. Proc IEEE Winter Conf on Applications of Computer Vision, p.2048&#8211;2057. \nhttps://doi.org/10.1109/WACV.2018.00226\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=J Mech Des; citation_title=Evaluation of idea generation methods for conceptual design: effectiveness metrics and design of experiments; citation_author=JJ Shah, SV Kulkarni, N Vargas-Hernandez; citation_volume=122; citation_issue=4; citation_publication_date=2000; citation_pages=377-384; citation_id=CR121\"/>\n\n    <meta name=\"citation_reference\" content=\"Simonyan K, Zisserman A, 2014. Very deep convolutional networks for large-scale image recognition. \nhttps://doi.org/1409.1556\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=brAInstorm: Intelligent Assistance in Group Idea Generation; citation_inbook_title=Lecture Notes in Computer Science; citation_publication_date=2017; citation_pages=457-461; citation_id=CR123; citation_author=Timo Strohmann; citation_author=Dominik Siemon; citation_author=Susanne Robra-Bissantz; citation_publisher=Springer International Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Non-photorealistic Computer Graphics: Modeling, Rendering, and Animation; citation_publication_date=2002; citation_id=CR124; citation_author=T Strothotte; citation_author=S Schlechtweg; citation_publisher=Morgan Kaufmann Publishers Inc.\"/>\n\n    <meta name=\"citation_reference\" content=\"Tang X, Wang ZW, Luo WX, et al., 2018. Face aging with identity-preserved conditional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.7939&#8211;7947. \nhttps://doi.org/10.1109/CVPR.2018.00828\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Multim; citation_title=Content-based photo quality assessment; citation_author=XO Tang, W Luo, XG Wang; citation_volume=15; citation_issue=8; citation_publication_date=2013; citation_pages=1930-1943; citation_id=CR126\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Artif Intell Eng Des Anal Manuf; citation_title=A scalable approach for ideation in biologically inspired design; citation_author=D Vandevenne, PA Verhaegen, S Dewulf; citation_volume=29; citation_issue=1; citation_publication_date=2015; citation_pages=19-31; citation_id=CR127\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IBM J Res Dev; citation_title=A big data approach to computational creativity: the curious case of Chef Watson; citation_author=LR Varshney, F Pinel, KR Varshney; citation_volume=63; citation_issue=1; citation_publication_date=2019; citation_pages=7:1-7:18; citation_id=CR128\"/>\n\n    <meta name=\"citation_reference\" content=\"Verma P, Smith JO, 2018. Neural style transfer for audio spectograms. \nhttps://doi.org/1801.01589\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Wang J, Yu LT, Zhang WN, et al., 2017. IRGAN: a minimax game for unifying generative and discriminative information retrieval models. Proc 40th Int ACM SI-GIR Conf on Research and Development in Information Retrieval, p.515&#8211;524. \nhttps://doi.org/10.1145/3077136.3080786\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Wang TC, Liu MY, Zhu JY, et al., 2018. Video-to-video synthesis. \nhttps://doi.org/1808.06601\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Wang WG, Shen JB, 2017. Deep cropping via attention box prediction and aesthetics assessment. Proc IEEE Int Conf on Computer Vision, p.2205&#8211;2213. \nhttps://doi.org/10.1109/ICCV.2017.240\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=Neurocomputing; citation_title=Synthesized computational aesthetic evaluation of photos; citation_author=WN Wang, D Cai, L Wang; citation_volume=172; citation_publication_date=2016; citation_pages=244-252; citation_id=CR133\"/>\n\n    <meta name=\"citation_reference\" content=\"Wang WS, Yang S, Zhang WS, et al., 2018. Neural aesthetic image reviewer. \nhttps://doi.org/1802.10240\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks; citation_inbook_title=Lecture Notes in Computer Science; citation_publication_date=2019; citation_pages=63-79; citation_id=CR135; citation_author=Xintao Wang; citation_author=Ke Yu; citation_author=Shixiang Wu; citation_author=Jinjin Gu; citation_author=Yihao Liu; citation_author=Chao Dong; citation_author=Yu Qiao; citation_author=Chen Change Loy; citation_publisher=Springer International Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"Wu JJ, Zhang CK, Xue TF, et al., 2016. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. Advances in Neural Information Processing Systems, p.82&#8211;90.\"/>\n\n    <meta name=\"citation_reference\" content=\"Xu T, Zhang PC, Huang QY, et al., 2018. AttnGAN: fine-grained text to image generation with attentional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1316&#8211;1324. \nhttps://doi.org/10.1109/CVPR.2018.00143\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Research on the development of contemporary design intelligence driven by neural network technology; citation_inbook_title=Design, User Experience, and Usability; citation_publication_date=2019; citation_pages=368-381; citation_id=CR138; citation_author=Y Yan; citation_author=JR Wang; citation_author=C Tang; citation_publisher=Springer\"/>\n\n    <meta name=\"citation_reference\" content=\"Yang HY, Huang D, Wang YH, et al., 2018. Learning face age progression: a pyramid architecture of GANs. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.31&#8211;39. \nhttps://doi.org/10.1109/CVPR.2018.00011\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Multim; citation_title=Deep learning for single image super-resolution: a brief review; citation_author=WM Yang, XC Zhang, YP Tian; citation_volume=21; citation_issue=12; citation_publication_date=2019; citation_pages=3106-3121; citation_id=CR140\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Multim; citation_title=Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval; citation_author=Y Yang, YT Zhuang, F Wu; citation_volume=10; citation_issue=3; citation_publication_date=2008; citation_pages=437-446; citation_id=CR141\"/>\n\n    <meta name=\"citation_reference\" content=\"Yi ZL, Zhang H, Tan P, et al., 2017. DualGAN: unsupervised dual learning for image-to-image translation. Proc IEEE Int Conf on Computer Vision, p.2868&#8211;2876. \nhttps://doi.org/10.1109/ICCV.2017.310\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Yoon Y, Jeon HG, Yoo D, et al., 2015. Learning a deep convolutional network for light-field image super-resolution. Proc IEEE Int Conf on Computer Vision, p.57&#8211;65. https://doi.org/10.1109/ICCVW.2015.17\"/>\n\n    <meta name=\"citation_reference\" content=\"You S, You N, Pan MX, 2019. PI-REC: progressive image reconstruction network with edge and color domain. \nhttps://doi.org/1903.10146\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Yu F, Zhang YD, Song SR, et al., 2015. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. \nhttps://doi.org/1506.03365\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Yu JH, Lin Z, Yang JM, et al., 2018a. Free-form image inpainting with gated convolution. \nhttps://doi.org/1806.03589\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Yu JH, Lin Z, Yang JM, et al., 2018b. Generative image inpainting with contextual attention. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.5505&#8211;5514. \nhttps://doi.org/10.1109/CVPR.2018.00577\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Zakharov E, Shysheya A, Burkov E, et al., 2019. Fewshot adversarial learning of realistic neural talking head models. \nhttps://doi.org/1905.08233\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Zeiler MD, Taylor GW, Fergus R, 2011. Adaptive deconvolutional networks for mid and high level feature learning. Proc IEEE Int Conf on Computer Vision, p.2018&#8211;2025. \nhttps://doi.org/10.1109/ICCV.2011.6126474\n\n\n\"/>\n\n    <meta name=\"citation_reference\" content=\"Zhang H, Xu T, Li H, et al., 2017. StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks. Proc IEEE Int Conf on Computer Vision, p.5907&#8211;5915.\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Patt Anal Mach Intell; citation_title=StackGAN++: realistic image synthesis with stacked generative adversarial networks; citation_author=H Zhang, T Xu, H Li; citation_volume=41; citation_issue=8; citation_publication_date=2019; citation_pages=1947-1962; citation_id=CR151\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=ACM Transactions on Applied Perception; citation_title=Computational Aesthetic Evaluation of Logos; citation_author=Jiajing Zhang, Jinhui Yu, Kang Zhang, Xianjun Sam Zheng, Junsong Zhang; citation_volume=14; citation_issue=3; citation_publication_date=2017; citation_pages=1-21; citation_id=CR152\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_title=Colorful Image Colorization; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=649-666; citation_id=CR153; citation_author=Richard Zhang; citation_author=Phillip Isola; citation_author=Alexei A. Efros; citation_publisher=Springer International Publishing\"/>\n\n    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Comput Imag; citation_title=Loss functions for image restoration with neural networks; citation_author=H Zhao, O Gallo, I Frosio; citation_volume=3; citation_issue=1; citation_publication_date=2016; citation_pages=47-57; citation_id=CR154\"/>\n\n    <meta name=\"citation_reference\" content=\"Zhu JY, Park T, Isola P, et al., 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. Proc IEEE Int Conf on Computer Vision, p.2242&#8211;2251. \nhttps://doi.org/10.1109/ICCV.2017.244\n\n\n\"/>\n\n    <meta name=\"citation_author\" content=\"Tang, Yong-chuan\"/>\n\n    <meta name=\"citation_author_email\" content=\"yctang@zju.edu.cn\"/>\n\n    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>\n\n    <meta name=\"citation_author\" content=\"Huang, Jiang-jie\"/>\n\n    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>\n\n    <meta name=\"citation_author\" content=\"Yao, Meng-ting\"/>\n\n    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>\n\n    <meta name=\"citation_author\" content=\"Wei, Jia\"/>\n\n    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>\n\n    <meta name=\"citation_author\" content=\"Li, Wei\"/>\n\n    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>\n\n    <meta name=\"citation_author\" content=\"He, Yong-xing\"/>\n\n    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>\n\n    <meta name=\"citation_author\" content=\"Li, Ze-jian\"/>\n\n    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>\n\n    <meta name=\"citation_author_institution\" content=\"Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China\"/>\n\n    <meta name=\"citation_springer_api_url\" content=\"http://api.springer.com/metadata/pam?q=doi:10.1631/FITEE.1900398&amp;api_key=\"/>\n\n    <meta name=\"format-detection\" content=\"telephone=no\"/>\n\n    <meta name=\"citation_cover_date\" content=\"2019/12/01\"/>\n\n\n    \n        <meta property=\"og:url\" content=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>\n        <meta property=\"og:type\" content=\"article\"/>\n        <meta property=\"og:site_name\" content=\"Frontiers of Information Technology &amp; Electronic Engineering\"/>\n        <meta property=\"og:title\" content=\"A review of design intelligence: progress, problems, and challenges - Frontiers of Information Technology &amp; Electronic Engineering\"/>\n        <meta property=\"og:description\" content=\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.\"/>\n        <meta property=\"og:image\" content=\"https://media.springernature.com/w200/springer-static/cover/journal/11714.jpg\"/>\n    \n\n    <title>A review of design intelligence: progress, problems, and challenges | SpringerLink</title>\n\n    <link rel=\"shortcut icon\" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />\n<link rel=\"icon\" sizes=\"16x16 32x32 48x48\" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />\n<link rel=\"icon\" sizes=\"16x16\" type=\"image/png\" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />\n<link rel=\"icon\" sizes=\"32x32\" type=\"image/png\" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />\n<link rel=\"icon\" sizes=\"48x48\" type=\"image/png\" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />\n<link rel=\"apple-touch-icon\" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />\n<link rel=\"apple-touch-icon\" sizes=\"72x72\" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />\n<link rel=\"apple-touch-icon\" sizes=\"76x76\" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />\n<link rel=\"apple-touch-icon\" sizes=\"114x114\" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />\n<link rel=\"apple-touch-icon\" sizes=\"120x120\" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />\n<link rel=\"apple-touch-icon\" sizes=\"144x144\" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />\n<link rel=\"apple-touch-icon\" sizes=\"152x152\" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />\n<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />\n\n\n    \n    <script>(function(H){H.className=H.className.replace(/\\bno-js\\b/,'js')})(document.documentElement)</script>\n\n    \n        <style>@media only screen and (-webkit-min-device-pixel-ratio: 0) and (min-color-index: 0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio: 0) and (min-resolution: 3 e1dpcm) { a{text-decoration:underline;text-decoration-skip-ink:auto}html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:62.5%;height:100%;line-height:1.61803;overflow-y:scroll}body{background:#fcfcfc;line-height:1.5;max-width:100%;min-height:100%}article,aside,figure,header,main,nav{display:block}h1{font-size:30px;margin:.67em 0}figure{margin:0}a{background-color:transparent;color:#004b83}img{border:0;height:auto;max-width:100%;vertical-align:middle}svg:not(:root){overflow:hidden}button,input{font-family:sans-serif;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=submit],button,html [type=button]{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}h3{font-size:17px}h1{font-family:Georgia,Palatino,serif;font-style:normal}.u-h3,h1,h3{margin-bottom:1em}.u-h3{line-height:1.4}.u-h3,h3{font-family:Georgia,Palatino,serif;font-style:normal}button:focus{outline:4px solid #fc0}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input{margin:0;padding:0}p{padding:0}a>img{vertical-align:middle}h1,h3{line-height:1.4}p{margin:0}h1,h3,ul{margin-top:0}p{margin-bottom:1.5em}p:empty{display:none}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad--300x250{background-color:#f2f2f2;display:none;padding:8px}.c-ad--300x250 .c-ad__inner{min-height:calc(1.5em + 254px)}@media only screen and (min-width:320px){.js .c-ad--300x250{display:block}}.c-ad__label,.c-skip-link{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.4rem}.c-ad__label{color:#333;font-weight:400;line-height:1.5;margin-bottom:4px}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-header{background-color:#fff;border-bottom:4px solid #00285a;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;padding:16px 0}.c-header__container{-webkit-box-align:center;-webkit-box-pack:justify;align-items:center;display:flex;justify-content:space-between;margin:0 auto;max-width:1280px;padding:0 16px}.c-header__brand{margin-right:32px}.c-header__brand a{text-decoration:none}.c-header__menu,.c-header__navigation{display:flex}.c-header__navigation{-webkit-box-align:center;align-items:center}.c-header__menu{list-style:none;margin:0;padding:0}.c-header__item{color:inherit;margin-right:24px}.c-header__item:last-child{margin-right:0}.c-header__link{color:inherit;text-decoration:none}h1{font-size:3.2rem}.u-h3,h1,h3{font-weight:700}.u-h3,h3{font-size:2.8rem;font-size:2.4rem}body{font-size:1.8em}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article__sub-heading{color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:2rem;font-style:normal;font-weight:400;line-height:1.3;margin:0 0 8px}@media only screen and (min-width:768px){.c-article__sub-heading{font-size:2.4rem;line-height:1.24}}@media only screen and (min-width:1024px){.c-pdf-button__container{display:none}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-pdf-download{display:flex;margin-bottom:24px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{-webkit-box-pack:justify;-webkit-box-flex:1;background:linear-gradient(#4d78af,#3365a0);border:1px solid transparent;border-radius:2px;color:#fff;display:flex;flex:1 1 0%;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.6rem;justify-content:space-between;line-height:1.3;padding:13px 24px;text-decoration:none}.c-popup-search{background-color:#eee;box-shadow:0 3px 3px -3px rgba(0,0,0,.21);padding:16px 0;position:relative;z-index:10}@media only screen and (min-width:1024px){.js .c-popup-search{position:absolute;top:100%;width:100%}.c-popup-search__container{margin:auto;max-width:70%}}.app-search__content{display:flex}.app-search__label{color:#666;display:inline-block;font-size:1.4rem;margin-bottom:8px}.app-search__input{-webkit-box-flex:0;border:1px solid #b3b3b3;border-bottom-left-radius:3px;border-top-left-radius:3px;box-shadow:inset 0 1px 3px 0 rgba(0,0,0,.21);flex:0 1 auto;font-size:1.4rem;line-height:1.2;padding:.75em 1em;vertical-align:middle;width:100%}.app-search__button{-webkit-box-align:center;-webkit-box-pack:center;align-items:center;background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);border-radius:0 2px 2px 0;color:#fff;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:50px}.app-search__button svg{fill:currentcolor}.u-display-block{display:block}.u-display-flex{display:flex;width:100%}.u-align-items-center{-webkit-box-align:center;align-items:center}.u-flex-static{-webkit-box-flex:0;flex:0 1 auto;flex:0 0 auto}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-position-relative{position:relative}.u-mt-16{margin-top:16px}.u-mt-32{margin-top:32px}.u-mr-24{margin-right:24px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-ml-8{margin-left:8px}.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-float-left{float:left}.u-text-sm{font-size:16px}.hide{visibility:hidden}.c-article-main-column .c-pdf-button__container .c-pdf-download,.hide{display:none}@media only screen and (max-width:1023px){.c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}} }</style>\n\n\n    \n\n    \n        <link rel=\"stylesheet\" data-inline-css-source=\"critical-css\" href=\"/oscar-static/app-springerlink/css/enhanced-article-75b1d207c3.css\" media=\"print\" onload=\"this.media='only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)';this.onload=null\">\n        \n    \n\n    \n    <script type=\"text/javascript\">\n        window.dataLayer = [{\"GA Key\":\"UA-26408784-1\",\"DOI\":\"10.1631/FITEE.1900398\",\"Page\":\"article\",\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"BR\",\"doi\":\"10.1631-FITEE.1900398\",\"Journal Title\":\"Frontiers of Information Technology \\u0026 Electronic Engineering\",\"Journal Id\":11714,\"Keywords\":\"Design intelligence, Creativity, Personas, Ideation, AI-generated content, Computational aesthetics, TP183\",\"kwrd\":[\"Design_intelligence\",\"Creativity\",\"Personas\",\"Ideation\",\"AI-generated_content\",\"Computational_aesthetics\",\"TP183\"],\"Labs\":\"Y\",\"ksg\":\"Krux.segments\",\"kuid\":\"Krux.uid\",\"Has Body\":\"N\",\"Features\":[],\"Open Access\":\"N\",\"hasAccess\":\"N\",\"bypassPaywall\":\"N\",\"user\":{\"license\":{\"businessPartnerID\":[],\"businessPartnerIDString\":\"\"}},\"Access Type\":\"no-access\",\"Bpids\":\"\",\"Bpnames\":\"\",\"BPID\":[\"1\"],\"VG Wort Identifier\":\"pw-vgzm.415900-10.1631-FITEE.1900398\",\"Full HTML\":\"N\",\"Subject Codes\":[\"SCI\",\"SCI00001\",\"SCT24000\",\"SCI1200X\",\"SCI13006\",\"SCT24027\",\"SCT24035\"],\"pmc\":[\"I\",\"I00001\",\"T24000\",\"I1200X\",\"I13006\",\"T24027\",\"T24035\"],\"session\":{\"authentication\":{\"loginStatus\":\"N\"},\"attributes\":{\"edition\":\"academic\"}},\"content\":{\"serial\":{\"eissn\":\"2095-9230\",\"pissn\":\"2095-9184\"},\"type\":\"Article\",\"category\":{\"pmc\":{\"primarySubject\":\"Computer Science\",\"primarySubjectCode\":\"I\",\"secondarySubjects\":{\"1\":\"Computer Science, general\",\"2\":\"Electrical Engineering\",\"3\":\"Computer Hardware\",\"4\":\"Computer Systems Organization and Communication Networks\",\"5\":\"Electronics and Microelectronics, Instrumentation\",\"6\":\"Communications Engineering, Networks\"},\"secondarySubjectCodes\":{\"1\":\"I00001\",\"2\":\"T24000\",\"3\":\"I1200X\",\"4\":\"I13006\",\"5\":\"T24027\",\"6\":\"T24035\"}},\"sucode\":\"SC6\"},\"attributes\":{\"deliveryPlatform\":\"oscar\"}},\"Event Category\":\"Article\"}];\n    </script>\n\n    \n\n    \n        \n    \n        \n            <script src=/oscar-static/js/jquery-220afd743d.js></script>\n        \n    \n\n    <script data-test=\"onetrust-control\">\n        \n            (function(w,d,t) {\n                var assetPath = '/oscar-static/js/cookie-consent-es5-bundle-c11e114a73.js';\n                function cc() {\n                    var h = w.location.hostname,\n                        e = d.createElement(t),\n                        s = d.getElementsByTagName(t)[0];\n\n                    if (h === \"link.springer.com\") {\n                        e.src = \"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\";\n                        e.setAttribute(\"data-domain-script\", \"4f53bc14-4ee3-45bd-9935-e3d2b6b2a543\");\n                    } else {\n                        e.src = assetPath;\n                        e.setAttribute(\"data-consent\", h);\n                    }\n                    s.parentNode.insertBefore(e, s);\n                }\n                w.google_tag_manager ? cc() : window.addEventListener(\"gtm_loaded\", cc);\n            })(window,document,\"script\");\n        \n    </script>\n    <script>\n        function OptanonWrapper() {\n            var elementInside = function(candidate, element) {\n                if (candidate === element) {\n                    return true;\n                } else if (candidate.nodeName.toLowerCase() === 'body') {\n                    return false;\n                } else {\n                    return elementInside(candidate.parentNode, element);\n                }\n            };\n\n            var disclaimer = document.querySelector('.c-disclaimer[aria-hidden=\"false\"]');\n            window.dataLayer.push({event:'OneTrustGroupsUpdated'});\n            if (disclaimer) {\n                if (!elementInside(document.activeElement, disclaimer)) {\n                    disclaimer.querySelector('button').focus();\n                }\n            } else {\n                document.activeElement.blur();\n            }\n        }\n    </script>\n\n    \n    \n    <script>\n        (function(w, d) {\n            w.config = w.config || {};\n            w.config.mustardcut = false;\n\n            \n            if (w.matchMedia && w.matchMedia('only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)').matches) {\n                w.config.mustardcut = true;\n                d.classList.add('js');\n                d.classList.remove('grade-c');\n            }\n        })(window, document.documentElement);\n    </script>\n\n\n\n    \n<script>\n    (function () {\n        if ( typeof window.CustomEvent === \"function\" ) return false;\n        function CustomEvent ( event, params ) {\n            params = params || { bubbles: false, cancelable: false, detail: null };\n            var evt = document.createEvent( 'CustomEvent' );\n            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );\n            return evt;\n        }\n\n        CustomEvent.prototype = window.Event.prototype;\n\n        window.CustomEvent = CustomEvent;\n    })();\n</script>\n\n    \n        \n    \n        \n            <!-- Google Tag Manager -->\n            <script data-test=\"gtm-head\">\n                if (window.config.mustardcut) {\n                    (function (w, d, s, l, i) {\n                        w[l] = w[l] || [];\n                        w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});\n                        var f = d.getElementsByTagName(s)[0],\n                                j = d.createElement(s),\n                                dl = l != 'dataLayer' ? '&l=' + l : '';\n                        j.async = true;\n                        j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;\n                        \n                        j.addEventListener('load', function() {\n                            var _ge = new CustomEvent('gtm_loaded', { bubbles: true });\n                            d.dispatchEvent(_ge);\n                        });\n                        f.parentNode.insertBefore(j, f);\n                    })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');\n                }\n            </script>\n            <!-- End Google Tag Manager -->\n        \n    \n\n\n    \n    \n    <script class=\"js-entry\">\n        if (window.config.mustardcut) {\n            (function(w, d) {\n                \n                \n                \n                    window.Component = {};\n                    window.suppressShareButton = false;\n                \n\n                var currentScript = d.currentScript || d.head.querySelector('script.js-entry');\n\n                \n                function catchNoModuleSupport() {\n                    var scriptEl = d.createElement('script');\n                    return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)\n                }\n\n                var headScripts = [\n                    {'src': '/oscar-static/js/polyfill-es5-bundle-0cfdbfc67a.js', 'async': false},\n                    {'src': '/oscar-static/js/airbrake-es5-bundle-d227f11622.js', 'async': false},\n                ];\n\n                var bodyScripts = [\n                    {'src': '/oscar-static/js/app-es5-bundle-8c7ca818ff.js', 'async': false, 'module': false},\n                    {'src': '/oscar-static/js/app-es6-bundle-14ad2bbad1.js', 'async': false, 'module': true}\n                    \n                    \n                        , {'src': '/oscar-static/js/global-article-es5-bundle-4af70904c0.js', 'async': false, 'module': false},\n                        {'src': '/oscar-static/js/global-article-es6-bundle-ff57660707.js', 'async': false, 'module': true}\n                    \n                ];\n\n                function createScript(script) {\n                    var scriptEl = d.createElement('script');\n                    scriptEl.src = script.src;\n                    scriptEl.async = script.async;\n                    if (script.module === true) {\n                        scriptEl.type = \"module\";\n                        if (catchNoModuleSupport()) {\n                            scriptEl.src = '';\n                        }\n                    } else if (script.module === false) {\n                        scriptEl.setAttribute('nomodule', true)\n                    }\n                    if (script.charset) {\n                        scriptEl.setAttribute('charset', script.charset);\n                    }\n\n                    return scriptEl;\n                }\n\n                for (var i = 0; i < headScripts.length; ++i) {\n                    var scriptEl = createScript(headScripts[i]);\n                    currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);\n                }\n\n                d.addEventListener('DOMContentLoaded', function() {\n                    for (var i = 0; i < bodyScripts.length; ++i) {\n                        var scriptEl = createScript(bodyScripts[i]);\n                        d.body.appendChild(scriptEl);\n                    }\n                });\n\n                // Webfont repeat view\n                var config = w.config;\n                if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {\n                    d.documentElement.className += ' webfonts-loaded';\n                }\n            })(window, document);\n        }\n    </script>\n\n\n\n    \n    \n    <link rel=\"canonical\" href=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>\n    \n\n    \n    <script type=\"application/ld+json\">{\"mainEntity\":{\"headline\":\"A review of design intelligence: progress, problems, and challenges\",\"description\":\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.\",\"datePublished\":\"2020-02-13\",\"dateModified\":\"2020-02-13\",\"pageStart\":\"1595\",\"pageEnd\":\"1617\",\"sameAs\":\"https://doi.org/10.1631/FITEE.1900398\",\"keywords\":\"Computer Science,general,Electrical Engineering,Computer Hardware,Computer Systems Organization and Communication Networks,Electronics and Microelectronics,Instrumentation,Communications Engineering,Networks\",\"image\":\"\",\"isPartOf\":{\"name\":\"Frontiers of Information Technology & Electronic Engineering\",\"issn\":[\"2095-9230\",\"2095-9184\"],\"volumeNumber\":\"20\",\"@type\":[\"Periodical\",\"PublicationVolume\"]},\"publisher\":{\"name\":\"Zhejiang University Press\",\"logo\":{\"url\":\"https://www.springernature.com/app-sn/public/images/logo-springernature.png\",\"@type\":\"ImageObject\"},\"@type\":\"Organization\"},\"author\":[{\"name\":\"Tang, Yong-chuan\",\"url\":\"http://orcid.org/0000-0002-0157-7771\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Huang, Jiang-jie\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Yao, Meng-ting\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Wei, Jia\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Li, Wei\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"He, Yong-xing\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Li, Ze-jian\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Alibaba-Zhejiang University Joint Institute of Frontier Technologies\",\"address\":{\"name\":\"Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"}],\"@type\":\"ScholarlyArticle\"},\"@context\":\"https://schema.org\",\"@type\":\"WebPage\"}</script>\n\n</head>\n<body class=\"shared-article-renderer\">\n    \n    \n    \n        \n            <!-- Google Tag Manager (noscript) -->\n            <noscript data-test=\"gtm-body\">\n                <iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9\"\n                height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe>\n            </noscript>\n            <!-- End Google Tag Manager (noscript) -->\n        \n    \n\n\n    <div class=\"u-vh-full\">\n        <a class=\"c-skip-link\" href=\"#main-content\">Skip to main content</a>\n        \n        <div class=\"u-hide u-show-following-ad\"></div>\n        <aside class=\"c-ad c-ad--728x90\" data-test=\"springer-doubleclick-ad\">\n            <div class=\"c-ad__inner\">\n                <p class=\"c-ad__label\">Advertisement</p>\n                    <div id=\"div-gpt-ad-LB1\" data-pa11y-ignore data-gpt data-gpt-unitpath=\"/270604982/springerlink/11714/article\" data-gpt-sizes=\"728x90\" style=\"min-width:728px;min-height:90px\" data-gpt-targeting=\"pos=LB1;articleid=FITEE.1900398;\"></div>\n            </div>\n        </aside>\n\n<div class=\"u-position-relative\">\n    <header class=\"c-header u-mb-24\" data-test=\"publisher-header\">\n        <div class=\"c-header__container\">\n            <div class=\"c-header__brand\">\n                \n    <a id=\"logo\" class=\"u-display-block\" href=\"/\" title=\"Go to homepage\" data-test=\"springerlink-logo\">\n        <picture>\n            <source type=\"image/svg+xml\" srcset=/oscar-static/images/springerlink/svg/springerlink-6c9a864b59.svg>\n            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt=\"SpringerLink\" width=\"148\" height=\"30\" data-test=\"header-academic\">\n        </picture>\n        \n        \n    </a>\n\n\n            </div>\n            <div class=\"c-header__navigation\">\n                \n    \n        <button type=\"button\"\n                class=\"c-header__link u-button-reset u-mr-24\"\n                data-expander\n                data-expander-target=\"#popup-search\"\n                data-expander-autofocus=\"firstTabbable\"\n                data-test=\"header-search-button\">\n            <span class=\"u-display-flex u-align-items-center\">\n                <span class=\"u-text-sm\">Search</span>\n                <svg class=\"u-icon u-flex-static u-ml-8\" aria-hidden=\"true\" focusable=\"false\">\n                    <use xlink:href=\"#global-icon-search\"></use>\n                </svg>\n            </span>\n        </button>\n        <nav>\n            <ul class=\"c-header__menu\">\n                \n                <li class=\"c-header__item\">\n                    <a\n                        data-test=\"login-link\"\n                        class=\"c-header__link\"\n                        href=\"//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1631%2FFITEE.1900398\"\n                        data-track=\"click\"\n                        data-track-category=\"header\"\n                        data-track-action=\"login header\"\n                        data-track-label=\"link\">Log in</a>\n                </li>\n                \n\n                \n            </ul>\n        </nav>\n    \n\n    \n\n\n\n            </div>\n        </div>\n    </header>\n\n    \n        <div id=\"popup-search\" class=\"c-popup-search u-mb-16 js-header-search u-js-hide\">\n            <div class=\"c-popup-search__content\">\n                <div class=\"u-container\">\n                    <div class=\"c-popup-search__container\" data-test=\"springerlink-popup-search\">\n                        <div class=\"app-search\">\n    <form role=\"search\" method=\"GET\" action=\"/search\" >\n        <label for=\"search\" class=\"app-search__label\">Search SpringerLink</label>\n        <div class=\"app-search__content\">\n            <input id=\"search\" class=\"app-search__input\" data-search-input autocomplete=\"off\" role=\"textbox\" name=\"query\" type=\"text\" value=\"\">\n            <button class=\"app-search__button\" type=\"submit\">\n                <span class=\"u-visually-hidden\">Search</span>\n                <svg class=\"u-icon\" aria-hidden=\"true\" focusable=\"false\">\n                    <use xlink:href=\"#global-icon-search\"></use>\n                </svg>\n            </button>\n            \n                <input type=\"hidden\" name=\"searchType\" value=\"publisherSearch\">\n            \n            \n        </div>\n    </form>\n</div>\n\n                    </div>\n                </div>\n            </div>\n        </div>\n    \n</div>\n\n        \n\n    <div class=\"u-container u-mt-32 u-mb-32 u-clearfix\" id=\"main-content\" data-component=\"article-container\">\n        <main class=\"c-article-main-column u-float-left js-main-column\" data-track-component=\"article body\">\n            \n\n            <div class=\"c-pdf-button__container\">\n                \n            </div>\n\n            <div class=\"c-article-collection__container\">\n                \n    \n\n            </div>\n\n\n            <article lang=\"en\">\n                <div class=\"c-article-header\">\n                    <header>\n                        <ul class=\"c-article-identifiers\" data-test=\"article-identifier\">\n                            \n    \n        <li class=\"c-article-identifiers__item\" data-test=\"article-category\">Review</li>\n    \n    \n    \n\n                            <li class=\"c-article-identifiers__item\"><a href=\"#article-info\" data-track=\"click\" data-track-action=\"publication date\" data-track-label=\"link\">Published: <time datetime=\"2020-02-13\">13 February 2020</time></a></li>\n                        </ul>\n\n                        \n                        <h1 class=\"c-article-title\" data-test=\"article-title\" data-article-title=\"\">A review of design intelligence: progress, problems, and challenges</h1>\n                        <ul class=\"c-article-author-list js-etal-collapsed\" data-etal=\"25\" data-etal-small=\"3\" data-test=\"authors-list\" data-component-authors-activator=\"authors-list\"><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Yong_chuan-Tang\" data-author-popup=\"auth-Yong_chuan-Tang\" data-corresp-id=\"c1\">Yong-chuan Tang<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#global-icon-email\"></use></svg></a><span class=\"u-js-hide\">\u00a0\n            <a class=\"js-orcid\" href=\"http://orcid.org/0000-0002-0157-7771\"><span class=\"u-visually-hidden\">ORCID: </span>orcid.org/0000-0002-0157-7771</a></span><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a>,<a href=\"#Aff3\">3</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Jiang_jie-Huang\" data-author-popup=\"auth-Jiang_jie-Huang\">Jiang-jie Huang</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Meng_ting-Yao\" data-author-popup=\"auth-Meng_ting-Yao\">Meng-ting Yao</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Jia-Wei\" data-author-popup=\"auth-Jia-Wei\">Jia Wei</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Wei-Li\" data-author-popup=\"auth-Wei-Li\">Wei Li</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a>,<a href=\"#Aff3\">3</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Yong_xing-He\" data-author-popup=\"auth-Yong_xing-He\">Yong-xing He</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a>,<a href=\"#Aff3\">3</a></sup> &amp; </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Ze_jian-Li\" data-author-popup=\"auth-Ze_jian-Li\">Ze-jian Li</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff3\">3</a>,<a href=\"#Aff4\">4</a></sup>\u00a0</li></ul>\n                        <p class=\"c-article-info-details\" data-container-section=\"info\">\n                            \n    <a data-test=\"journal-link\" href=\"/journal/11714\"><i data-test=\"journal-title\">Frontiers of Information Technology &amp; Electronic Engineering</i></a>\n\n                            <b data-test=\"journal-volume\"><span class=\"u-visually-hidden\">volume</span>\u00a020</b>,\u00a0<span class=\"u-visually-hidden\">pages </span>1595\u20131617 (<span data-test=\"article-publication-year\">2019</span>)<a href=\"#citeas\" class=\"c-article-info-details__cite-as u-hide-print\" data-track=\"click\" data-track-action=\"cite this article\" data-track-label=\"link\">Cite this article</a>\n                        </p>\n                        \n    \n\n                        <div data-test=\"article-metrics\">\n                            <div id=\"altmetric-container\">\n    <div class=\"c-article-metrics-bar__wrapper u-clear-both\">\n        <ul class=\"c-article-metrics-bar u-list-reset\">\n            \n                <li class=\" c-article-metrics-bar__item\">\n                    <p class=\"c-article-metrics-bar__count\">282 <span class=\"c-article-metrics-bar__label\">Accesses</span></p>\n                </li>\n            \n            \n                <li class=\"c-article-metrics-bar__item\">\n                    <p class=\"c-article-metrics-bar__count\">4 <span class=\"c-article-metrics-bar__label\">Citations</span></p>\n                </li>\n            \n            \n            <li class=\"c-article-metrics-bar__item\">\n                <p class=\"c-article-metrics-bar__details\"><a href=\"/article/10.1631%2FFITEE.1900398/metrics\" data-track=\"click\" data-track-action=\"view metrics\" data-track-label=\"link\" rel=\"nofollow\">Metrics <span class=\"u-visually-hidden\">details</span></a></p>\n            </li>\n        </ul>\n    </div>\n</div>\n\n                        </div>\n                        \n    \n\n    \n\n                        \n                    </header>\n                </div>\n\n                <div data-article-body=\"true\" data-track-component=\"article body\" class=\"c-article-body\">\n                    <section aria-labelledby=\"Abs1\" data-title=\"Abstract\" lang=\"en\"><div class=\"c-article-section\" id=\"Abs1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Abs1\">Abstract</h2><div class=\"c-article-section__content\" id=\"Abs1-content\"><p>Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.</p></div></div></section>\n                    \n    \n\n\n                    \n                        \n                            <div class=\"c-notes\">\n                                <p class=\"c-notes__text\">This is a preview of subscription content, <a id=\"test-login-banner-link\" href=\"//wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1631%2FFITEE.1900398\" data-track=\"click\" data-track-action=\"login\" data-track-label=\"link\">access via your institution</a>.</p>\n                            </div>\n                        \n                        \n                            <div class=\"c-article-buy-box c-article-buy-box--article\">\n                                <div class=\"sprcom-buybox-articleSidebar\" id=\"sprcom-buybox-articleSidebar\">\n <h2 class=\"c-box__heading\">Access options</h2>\n <article class=\"c-box\" data-test-id=\"buy-article\">\n  <h3 class=\"c-box__heading\">Buy single article</h3>\n  <div class=\"c-box__body\">\n   <div class=\"buybox__info\">\n    <p>Instant access to the full article PDF.</p>\n   </div>\n   <div class=\"buybox__buy\">\n    <p class=\"buybox__price\">USD 39.95</p>\n    <p class=\"buybox__price-info\">Price includes VAT (Brazil)<br>Tax calculation will be finalised during checkout.</p>\n    <form action=\"https://order.springer.com/public/checkout?abtest=v2\" method=\"post\">\n     <input type=\"hidden\" name=\"type\" value=\"article\">\n     <input type=\"hidden\" name=\"doi\" value=\"10.1631/FITEE.1900398\">\n     <input type=\"hidden\" name=\"isxn\" value=\"2095-9230\">\n     <input type=\"hidden\" name=\"contenttitle\" value=\"A review of design intelligence: progress, problems, and challenges\">\n     <input type=\"hidden\" name=\"copyrightyear\" value=\"2019\">\n     <input type=\"hidden\" name=\"year\" value=\"2020\">\n     <input type=\"hidden\" name=\"authors\" value=\"Yong-chuan Tang, et al.\">\n     <input type=\"hidden\" name=\"title\" value=\"Frontiers of Information Technology &amp; Electronic Engineering\">\n     <input type=\"hidden\" name=\"mac\" value=\"82FB53023BBB3109B3CB91533294E4D6\">\n     <input type=\"submit\" class=\"c-box__button\" onclick=\"dataLayer.push({&quot;event&quot;:&quot;addToCart&quot;,&quot;ecommerce&quot;:{&quot;currencyCode&quot;:&quot;USD&quot;,&quot;add&quot;:{&quot;products&quot;:[{&quot;name&quot;:&quot;A review of design intelligence: progress, problems, and challenges&quot;,&quot;id&quot;:&quot;2095-9230&quot;,&quot;price&quot;:39.95,&quot;brand&quot;:&quot;Zhejiang University Press&quot;,&quot;category&quot;:&quot;Computer Science&quot;,&quot;variant&quot;:&quot;ppv-article&quot;,&quot;quantity&quot;:1}]}}});\" value=\"Buy article PDF\">\n    </form>\n   </div>\n  </div>\n  <script>dataLayer.push({\"ecommerce\":{\"currency\":\"USD\",\"impressions\":[{\"name\":\"A review of design intelligence: progress, problems, and challenges\",\"id\":\"2095-9230\",\"price\":39.95,\"brand\":\"Zhejiang University Press\",\"category\":\"Computer Science\",\"variant\":\"ppv-article\",\"quantity\":1}]}});</script>\n </article>\n <article class=\"c-box buybox__rent-article\" id=\"deepdyve\" style=\"display: none\" data-test-id=\"journal-subscription\">\n  <div class=\"c-box__body\">\n   <div class=\"buybox__info\">\n    <p><a class=\"deepdyve-link\" target=\"deepdyve\" rel=\"nofollow\" data-track=\"click\" data-track-action=\"rent article\" data-track-label=\"rent action, new buybox\">Rent this article via DeepDyve.</a></p>\n   </div>\n  </div>\n  <script>\n            function deepDyveResponse(data) {\n                if (data.status === 'ok') {\n                    [].slice.call(document.querySelectorAll('.c-box.buybox__rent-article')).forEach(function (article) {\n                        article.style.display = 'flex'\n                        var link = article.querySelector('.deepdyve-link')\n                        if (link) {\n                          link.setAttribute('href', data.url)\n                        }\n                    })\n                }\n            }\n\n            var script = document.createElement('script')\n            script.src = '//www.deepdyve.com/rental-link?docId=10.1631/FITEE.1900398&journal=2095-9230&fieldName=journal_doi&affiliateId=springer&format=jsonp&callback=deepDyveResponse'\n            document.body.appendChild(script)\n          </script>\n </article>\n <aside class=\"buybox__institutional-sub\">\n  <div class=\"c-box__body\">\n   <div class=\"buybox__info\">\n    <p><a href=\"https://www.springernature.com/gp/librarians/licensing/license-options?&amp;abtest=v2\" data-track=\"click\" data-track-action=\"institutional link\" data-track-label=\"institutional subscriptions, new buybox\">Learn more about Institutional subscriptions</a></p>\n   </div>\n  </div>\n </aside>\n <style>.sprcom-buybox-articleSidebar{\n  box-shadow: 0px 0px 5px rgba(51,51,51,0.101);\n  display: flex;\n  flex-wrap: wrap;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;\n  text-align: center;\n}\n.sprcom-buybox-articleSidebar *{\n  box-sizing: border-box;\n  line-height: calc(100% + 4px);\n  margin: 0px;\n}\n.sprcom-buybox-articleSidebar > *{\n  display: flex;\n  flex-basis: 240px;\n  flex-direction: column;\n  flex-grow: 1;\n  flex-shrink: 1;\n  margin: 0.5px;\n}\n.sprcom-buybox-articleSidebar > *{\n  box-shadow: 0 0 0 1px rgba(204,204,204,0.494);\n}\n.sprcom-buybox-articleSidebar .c-box__body{\n  display: flex;\n  flex-direction: column-reverse;\n  flex-grow: 1;\n  justify-content: space-between;\n  padding: 6%;\n}\n.sprcom-buybox-articleSidebar .c-box__body .buybox__buy{\n  display: flex;\n  flex-direction: column-reverse;\n}\n.sprcom-buybox-articleSidebar p{\n  color: #333;\n  font-size: 15px;\n}\n.sprcom-buybox-articleSidebar .buybox__price{\n  font-size: 24px;\n  font-weight: 500;\n  line-height: calc(100% + 8px);\n  margin: 20px 0;\n  order: 1;\n}\n.sprcom-buybox-articleSidebar form{\n  order: 1;\n}\n.sprcom-buybox-articleSidebar .buybox__price-info{\n  margin-bottom: 20px;\n}\n.sprcom-buybox-articleSidebar .c-box__heading{\n  background-color: #f0f0f0;\n  color: #333;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;\n  font-size: 16px;\n  margin: 0px;\n  padding: 10px 12px;\n  text-align: center;\n}\n.sprcom-buybox-articleSidebar .c-box__button{\n  background-color: #3365A4;\n  border: 1px solid transparent;\n  border-radius: 2px;\n  color: #fff;\n  cursor: pointer;\n  display: inline-block;\n  font-family: inherit;\n  font-size: 16px;\n  max-width: 222px;\n  padding: 10px 12px;\n  text-decoration: none;\n  width: 100%;\n}\n.sprcom-buybox-articleSidebar h3{\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  overflow: hidden;\n  position: absolute;\n  width: 1px;\n}\n.sprcom-buybox-articleSidebar h2{\n  flex-basis: 100%;\n  margin-bottom: 16px;\n  text-align: left;\n}\n.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .c-box__body{\n  flex-direction: row;\n}\n.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .buybox__info{\n  text-align: left;\n}\n.sprcom-buybox-articleSidebar .buybox__institutional-sub{\n  background-color: #f0f0f0;\n}\n.sprcom-buybox-articleSidebar .visually-hidden{\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  overflow: hidden;\n  position: absolute;\n  width: 1px;\n}\n.sprcom-buybox-articleSidebar style{\n  display: none;\n}\n</style>\n</div>\n                            </div>\n                        \n                        <div class=\"u-display-none\">\n                            \n                        </div>\n                    \n\n                    \n\n                    \n\n                    <div id=\"MagazineFulltextArticleBodySuffix\"><section aria-labelledby=\"Bib1\" data-title=\"References\"><div class=\"c-article-section\" id=\"Bib1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Bib1\">References</h2><div class=\"c-article-section__content\" id=\"Bib1-content\"><div data-container-section=\"references\"><ol class=\"c-article-references\"><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR1\">Arjovsky M, Chintala S, Bottou L, 2017. Wasserstein generative adversarial networks. Proc 34<sup>th</sup> Int Conf on Machine Learning, p.298\u2013321.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR2\">Aubry M, Maturana D, Efros AA, et al., 2014. Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3762\u20133769. <a href=\"https://doi.org/10.1109/CVPR.2014.487\">https://doi.org/10.1109/CVPR.2014.487</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR3\">Ballester C, Bertalmio M, Caselles V, et al., 2001. Filling-in by joint interpolation of vector fields and gray levels. <i>IEEE Trans Image Process</i>, 10(8):1200\u20131211. <a href=\"https://doi.org/10.1109/83.935036\">https://doi.org/10.1109/83.935036</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.ams.org/mathscinet-getitem?mr=1851781\" aria-label=\"MathSciNet reference 3\">MathSciNet</a>\u00a0\n    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?1037.68771\" aria-label=\"MATH reference 3\">MATH</a>\u00a0\n    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 3\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Filling-in%20by%20joint%20interpolation%20of%20vector%20fields%20and%20gray%20levels&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=10&amp;issue=8&amp;pages=1200-1211&amp;publication_year=2001&amp;author=Ballester%2CC&amp;author=Bertalmio%2CM&amp;author=Caselles%2CV\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR4\">Bertalmio M, Sapiro G, Caselles V, et al., 2000. Image in-painting. Proc 27<sup>th</sup> Annual Conf on Computer Graphics and Interactive Techniques, p.417\u2013424. <a href=\"https://doi.org/10.1145/344779.344972\">https://doi.org/10.1145/344779.344972</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR5\">Bharadhwaj H, Park H, Lim BY, 2018. RecGAN: recurrent generative adversarial networks for recommendation systems. Proc 12<sup>th</sup> ACM Conf on Recommender Systems, p.372\u2013376. <a href=\"https://doi.org/10.1145/3240323.3240383\">https://doi.org/10.1145/3240323.3240383</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR6\">Boden MA, 2009. Computer models of creativity. <i>AI Mag</i>, 30(3):23\u201334. <a href=\"https://doi.org/10.1609/aimag.v30i3.2254\">https://doi.org/10.1609/aimag.v30i3.2254</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 6\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Computer%20models%20of%20creativity&amp;journal=AI%20Mag&amp;volume=30&amp;issue=3&amp;pages=23-34&amp;publication_year=2009&amp;author=Boden%2CMA\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR7\">Brock A, Donahue J, Simonyan K, 2018. Large scale GAN training for high fidelity natural image synthesis. <a href=\"https://arxiv.org/abs/1809.11096\">https://doi.org/1809.11096</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR8\">Bruna J, Sprechmann P, LeCun Y, 2015. Super-resolution with deep convolutional sufficient statistics. <a href=\"https://arxiv.org/abs/1511.05666\">https://doi.org/1511.05666</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR9\">Chakrabarti A, Siddharth L, Dinakar M, et al., 2017. Idea inspire 3.0\u2014a tool for analogical design. In: Chakrabarti A, Chakrabarti D (Eds.), Research into Design for Communities. Springer, Singapore, p.475\u2013485. <a href=\"https://doi.org/10.1007/978-981-10-3521-0_41\">https://doi.org/10.1007/978-981-10-3521-0_41</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 9\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Idea%20inspire%203.0%E2%80%94a%20tool%20for%20analogical%20design&amp;pages=475-485&amp;publication_year=2017&amp;author=Chakrabarti%2CA&amp;author=Siddharth%2CL&amp;author=Dinakar%2CM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR10\">Champandard AJ, 2016. Semantic style transfer and turning two-bit doodles into fine artworks. <a href=\"https://arxiv.org/abs/1603.01768\">https://doi.org/1603.01768</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR11\">Chan C, Ginosar S, Zhou TH, et al., 2018. Everybody dance now. <a href=\"https://arxiv.org/abs/1808.07371\">https://doi.org/1808.07371</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR12\">Chen DD, Yuan L, Liao J, et al., 2018. Stereoscopic neural style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.6654\u20136663. <a href=\"https://doi.org/10.1109/CVPR.2018.00696\">https://doi.org/10.1109/CVPR.2018.00696</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR13\">Chen LQ, Wang P, Dong H, et al., 2019. An artificial intelligence based data-driven approach for design ideation. <i>J Vis Commun Image Represent</i>, 61:10\u201322. <a href=\"https://doi.org/10.1016/j.jvcir.2019.02.009\">https://doi.org/10.1016/j.jvcir.2019.02.009</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 13\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20artificial%20intelligence%20based%20data-driven%20approach%20for%20design%20ideation&amp;journal=J%20Vis%20Commun%20Image%20Represent&amp;volume=61&amp;pages=10-22&amp;publication_year=2019&amp;author=Chen%2CLQ&amp;author=Wang%2CP&amp;author=Dong%2CH\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR14\">Ciesielski V, Barile P, Trist K, 2013. Finding image features associated with high aesthetic value by machine learning. Proc 2<sup>nd</sup> Int Conf on Evolutionary and Biologically Inspired Music, Sound, Art and Design, p.47\u201358. <a href=\"https://doi.org/10.1007/978-3-642-36955-1_5\">https://doi.org/10.1007/978-3-642-36955-1_5</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 14\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Finding%20Image%20Features%20Associated%20with%20High%20Aesthetic%20Value%20by%20Machine%20Learning&amp;pages=47-58&amp;publication_year=2013&amp;author=Ciesielski%2CVic&amp;author=Barile%2CPerry&amp;author=Trist%2CKaren\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR15\">Cooper A, 1999. The Inmates Are Running the Asylum. SAMS, Indianapolis, USA.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 15\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20Inmates%20Are%20Running%20the%20Asylum&amp;publication_year=1999&amp;author=Cooper%2CA\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR16\">Cooper A, Reimann RM, 2003. About Face 2.0: the Essentials of Interaction Design. John Wiley &amp; Sons, Indianapolis, USA.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 16\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=About%20Face%202.0%3A%20the%20Essentials%20of%20Interaction%20Design&amp;publication_year=2003&amp;author=Cooper%2CA&amp;author=Reimann%2CRM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR17\">Dash A, Gamboa JCB, Ahmed S, et al., 2017. TAC-GAN-text conditioned auxiliary classifier generative adversarial network. <a href=\"https://arxiv.org/abs/1703.06412\">https://doi.org/1703.06412</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR18\">Datta R, Joshi D, Li J, et al., 2006. Studying aesthetics in photographic images using a computational approach. Proc 9<sup>th</sup> European Conf on Computer Vision, p.288\u2013301. <a href=\"https://doi.org/10.1007/11744078_23\">https://doi.org/10.1007/11744078_23</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 18\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Studying%20Aesthetics%20in%20Photographic%20Images%20Using%20a%20Computational%20Approach&amp;pages=288-301&amp;publication_year=2006&amp;author=Datta%2CRitendra&amp;author=Joshi%2CDhiraj&amp;author=Li%2CJia&amp;author=Wang%2CJames%20Z.\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR19\">de G\u00f3mez Silva Garza A, Maher ML, 1999. An evolutionary approach to case adaptation. Proc 3<sup>rd</sup> Int Conf on Case-Based Reasoning, p.162\u2013173. <a href=\"https://doi.org/10.1007/3-540-48508-2_12\">https://doi.org/10.1007/3-540-48508-2_12</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 19\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20Evolutionary%20Approach%20to%20Case%20Adaptation&amp;pages=162-173&amp;publication_year=1999&amp;author=de%20G%C3%B3mez%20Silva%20Garza%2CAndr%C3%A9s&amp;author=Maher%2CMary%20Lou\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR20\">de Silva Garza AG, 2019. An introduction to and comparison of computational creativity and design computing. <i>Artif Intell Rev</i>, 51(1):61\u201376. <a href=\"https://doi.org/10.1007/s10462-017-9557-3\">https://doi.org/10.1007/s10462-017-9557-3</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 20\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20and%20comparison%20of%20computational%20creativity%20and%20design%20computing&amp;journal=Artif%20Intell%20Rev&amp;volume=51&amp;issue=1&amp;pages=61-76&amp;publication_year=2019&amp;author=de%20Silva%20Garza%2CAG\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR21\">Deng J, Dong W, Socher R, et al., 2009. ImageNet: a large-scale hierarchical image database. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.248\u2013255. <a href=\"https://doi.org/10.1109/CVPR.2009.5206848\">https://doi.org/10.1109/CVPR.2009.5206848</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR22\">Deng YB, Loy CC, Tang XO, 2018. Aesthetic-driven image enhancement by adversarial learning. Proc 26<sup>th</sup> ACM Int Conf on Multimedia, p.870\u2013878. <a href=\"https://doi.org/10.1145/3240508.3240531\">https://doi.org/10.1145/3240508.3240531</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR23\">Donahue J, Kr\u00e4henb\u00fchl P, Darrell T, 2016. Adversarial feature learning. <a href=\"https://arxiv.org/abs/1605.09782\">https://doi.org/1605.09782</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR24\">Dou Q, Zheng XS, Sun TF, et al., 2019. Webthetics: quantifying webpage aesthetics with deep learning. <i>Int J Hum Comput Stud</i>, 124:56\u201366. <a href=\"https://doi.org/10.1016/j.ijhcs.2018.11.006\">https://doi.org/10.1016/j.ijhcs.2018.11.006</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 24\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Webthetics%3A%20quantifying%20webpage%20aesthetics%20with%20deep%20learning&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=124&amp;pages=56-66&amp;publication_year=2019&amp;author=Dou%2CQ&amp;author=Zheng%2CXS&amp;author=Sun%2CTF\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR25\">Dugosh KL, Paulus PB, Roland EJ, et al., 2000. Cognitive stimulation in brainstorming. <i>J Pers Soc Psychol</i>, 79(5):722\u2013735. <a href=\"https://doi.org/10.1037/0022-3514.79.5.722\">https://doi.org/10.1037/0022-3514.79.5.722</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 25\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20stimulation%20in%20brainstorming&amp;journal=J%20Pers%20Soc%20Psychol&amp;volume=79&amp;issue=5&amp;pages=722-735&amp;publication_year=2000&amp;author=Dugosh%2CKL&amp;author=Paulus%2CPB&amp;author=Roland%2CEJ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR26\">Dumoulin V, Visin F, 2016. A guide to convolution arithmetic for deep learning. <a href=\"https://arxiv.org/abs/1603.07285\">https://doi.org/1603.07285</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR27\">Edelman RR, Hesselink JR, Zlatkin MB, 1996. MRI: Clinical Magnetic Resonance Imaging. Saunders, Philadelphia.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 27\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=MRI%3A%20Clinical%20Magnetic%20Resonance%20Imaging&amp;publication_year=1996&amp;author=Edelman%2CRR&amp;author=Hesselink%2CJR&amp;author=Zlatkin%2CMB\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR28\">Efros AA, Freeman WT, 2001. Image quilting for texture synthesis and transfer. Proc 28<sup>th</sup> Annual Conf on Computer Graphics and Interactive Techniques, p.341\u2013346. <a href=\"https://doi.org/10.1145/383259.383296\">https://doi.org/10.1145/383259.383296</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR29\">Elgammal A, Liu B, Elhoseiny M, et al., 2017. CAN: creative adversarial networks, generating \u201cart\u201d by learning about styles and deviating from style norms. <a href=\"https://arxiv.org/abs/1706.07068\">https://doi.org/1706.07068</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR30\">Fang H, Zhang M, 2017. Creatism: a deep-learning photographer capable of creating professional work. <a href=\"https://arxiv.org/abs/1707.03491\">https://doi.org/1707.03491</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR31\">Faste H, Rachmel N, Essary R, et al., 2013. Brainstorm, chainstorm, cheatstorm, tweetstorm: new ideation strategies for distributed HCI design. Proc Conf on Human Factors in Computing Systems, p.1343\u20131352. <a href=\"https://doi.org/10.1145/2470654.2466177\">https://doi.org/10.1145/2470654.2466177</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR32\">Fu K, Murphy J, Yang M, et al., 2015. Design-by-analogy: experimental evaluation of a functional analogy search methodology for concept generation improvement. <i>Res Eng Des</i>, 26(1):77\u201395. <a href=\"https://doi.org/10.1007/s00163-014-0186-4\">https://doi.org/10.1007/s00163-014-0186-4</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 32\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Design-by-analogy%3A%20experimental%20evaluation%20of%20a%20functional%20analogy%20search%20methodology%20for%20concept%20generation%20improvement&amp;journal=Res%20Eng%20Des&amp;volume=26&amp;issue=1&amp;pages=77-95&amp;publication_year=2015&amp;author=Fu%2CK&amp;author=Murphy%2CJ&amp;author=Yang%2CM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR33\">Garabedian CA, 1934. Birkhoff on aesthetic measure. <i>Bull Amer Math Soc</i>, 40(1):7\u201310. <a href=\"https://doi.org/10.1090/S0002-9904-1934-05764-1\">https://doi.org/10.1090/S0002-9904-1934-05764-1</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.ams.org/mathscinet-getitem?mr=1562780\" aria-label=\"MathSciNet reference 33\">MathSciNet</a>\u00a0\n    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 33\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Birkhoff%20on%20aesthetic%20measure&amp;journal=Bull%20Amer%20Math%20Soc&amp;volume=40&amp;issue=1&amp;pages=7-10&amp;publication_year=1934&amp;author=Garabedian%2CCA\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR34\">Gatys L, Ecker A, Bethge M, 2016a. Image style transfer using convolutional neural networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2414\u20132423. <a href=\"https://doi.org/10.1109/CVPR.2016.265\">https://doi.org/10.1109/CVPR.2016.265</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR35\">Gatys L, Ecker A, Bethge M, 2016b. A neural algorithm of artistic style. <i>J Vis</i>, 16(12):326. <a href=\"https://doi.org/10.1167/16.12.326\">https://doi.org/10.1167/16.12.326</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 35\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20neural%20algorithm%20of%20artistic%20style&amp;journal=J%20Vis&amp;volume=16&amp;issue=12&amp;publication_year=2016&amp;author=Gatys%2CL&amp;author=Ecker%2CA&amp;author=Bethge%2CM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR36\">Gero JS, 1990. Design prototypes: a knowledge representation schema for design. <i>AI Mag</i>, 11(4):26\u201336.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 36\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Design%20prototypes%3A%20a%20knowledge%20representation%20schema%20for%20design&amp;journal=AI%20Mag&amp;volume=11&amp;issue=4&amp;pages=26-36&amp;publication_year=1990&amp;author=Gero%2CJS\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR37\">Gilon K, Chan J, Ng FY, et al., 2018. Analogy mining for specific design needs. Proc CHI Conf on Human Factors in Computing Systems, p.121. <a href=\"https://doi.org/10.1145/3173574.3173695\">https://doi.org/10.1145/3173574.3173695</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR38\">Goel AK, Rugaber S, Vattam S, 2009. Structure, behavior, and function of complex systems: the structure, behavior, and function modeling language. <i>AI Edam</i>, 23(1):23\u201335. <a href=\"https://doi.org/10.1017/S0890060409000080\">https://doi.org/10.1017/S0890060409000080</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 38\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Structure%2C%20behavior%2C%20and%20function%20of%20complex%20systems%3A%20the%20structure%2C%20behavior%2C%20and%20function%20modeling%20language&amp;journal=AI%20Edam&amp;volume=23&amp;issue=1&amp;pages=23-35&amp;publication_year=2009&amp;author=Goel%2CAK&amp;author=Rugaber%2CS&amp;author=Vattam%2CS\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR39\">Goldschmidt G, Smolkov M, 2006. Variances in the impact of visual stimuli on design problem solving performance. <i>Des Stud</i>, 27(5):549\u2013569. <a href=\"https://doi.org/10.1016/j.destud.2006.01.002\">https://doi.org/10.1016/j.destud.2006.01.002</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 39\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Variances%20in%20the%20impact%20of%20visual%20stimuli%20on%20design%20problem%20solving%20performance&amp;journal=Des%20Stud&amp;volume=27&amp;issue=5&amp;pages=549-569&amp;publication_year=2006&amp;author=Goldschmidt%2CG&amp;author=Smolkov%2CM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR40\">Gooch B, Gooch A, 2001. Non-photorealistic Rendering. A K Peters/CRC Press, New York, USA. <a href=\"https://doi.org/10.1201/9781439864173\">https://doi.org/10.1201/9781439864173</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?1055.68136\" aria-label=\"MATH reference 40\">MATH</a>\u00a0\n    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 40\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Non-photorealistic%20Rendering&amp;publication_year=2001&amp;author=Gooch%2CB&amp;author=Gooch%2CA\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR41\">Goodfellow I, Pouget-Abadie J, Mirza M, et al., 2014. Generative adversarial nets. Proc 27<sup>th</sup> Int Conf on Neural Information Processing Systems, p.2672\u20132680.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR42\">Grudin J, Pruitt J, 2002. Personas, participatory design, and product development: an infrastructure for engagement. Proc 7<sup>th</sup> Biennial Participatory Design Conf, p.144\u2013152.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR43\">Gulrajani I, Ahmed F, Arjovsky M, et al., 2017. Improved training of Wasserstein GANs. Advances in Neural Information Proc Systems, p.5767\u20135777.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR44\">Han J, Shi F, Chen LQ, et al., 2018. A computational tool for creative idea generation based on analogical reasoning and ontology. <i>Artif Intell Eng Des Anal Manuf</i>, 32(4):462\u2013477. <a href=\"https://doi.org/10.1017/S0890060418000082\">https://doi.org/10.1017/S0890060418000082</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 44\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20computational%20tool%20for%20creative%20idea%20generation%20based%20on%20analogical%20reasoning%20and%20ontology&amp;journal=Artif%20Intell%20Eng%20Des%20Anal%20Manuf&amp;volume=32&amp;issue=4&amp;pages=462-477&amp;publication_year=2018&amp;author=Han%2CJ&amp;author=Shi%2CF&amp;author=Chen%2CLQ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR45\">Hao J, Zhou YJ, Zhao QF, et al., 2019. An evolutionary computation based method for creative design inspiration generation. <i>J Intell Manuf</i>, 30(4):1673\u20131691. <a href=\"https://doi.org/10.1007/s10845-017-1347-x\">https://doi.org/10.1007/s10845-017-1347-x</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 45\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20evolutionary%20computation%20based%20method%20for%20creative%20design%20inspiration%20generation&amp;journal=J%20Intell%20Manuf&amp;volume=30&amp;issue=4&amp;pages=1673-1691&amp;publication_year=2019&amp;author=Hao%2CJ&amp;author=Zhou%2CYJ&amp;author=Zhao%2CQF\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR46\">Hartson R, Pyla PS, 2012. The UX Book: Process and Guidelines for Ensuring a Quality User Experience. Elsevier, Amsterdam. <a href=\"https://doi.org/10.1016/C2010-0-66326-7\">https://doi.org/10.1016/C2010-0-66326-7</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 46\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20UX%20Book%3A%20Process%20and%20Guidelines%20for%20Ensuring%20a%20Quality%20User%20Experience&amp;publication_year=2012&amp;author=Hartson%2CR&amp;author=Pyla%2CPS\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR47\">He KM, Sun J, 2014. Image completion approaches using the statistics of similar patches. <i>IEEE Trans Patt Anal Mach Intell</i>, 36(12):2423\u20132435. <a href=\"https://doi.org/10.1109/TPAMI.2014.2330611\">https://doi.org/10.1109/TPAMI.2014.2330611</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 47\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Image%20completion%20approaches%20using%20the%20statistics%20of%20similar%20patches&amp;journal=IEEE%20Trans%20Patt%20Anal%20Mach%20Intell&amp;volume=36&amp;issue=12&amp;pages=2423-2435&amp;publication_year=2014&amp;author=He%2CKM&amp;author=Sun%2CJ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR48\">Hertzmann A, Jacobs CE, Oliver N, et al., 2001. Image analogies. Proc 28<sup>th</sup> Annual Conf on Computer Graphics and Interactive Techniques, p.327\u2013340. <a href=\"https://doi.org/10.1145/383259.383295\">https://doi.org/10.1145/383259.383295</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR49\">Hong YJ, Hwang U, Yoo J, et al., 2019. How generative adversarial networks and their variants work: an overview. <i>ACM Comput Surv</i>, 52(1):10. <a href=\"https://doi.org/10.1145/3301282\">https://doi.org/10.1145/3301282</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 49\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=How%20generative%20adversarial%20networks%20and%20their%20variants%20work%3A%20an%20overview&amp;journal=ACM%20Comput%20Surv&amp;volume=52&amp;issue=1&amp;publication_year=2019&amp;author=Hong%2CYJ&amp;author=Hwang%2CU&amp;author=Yoo%2CJ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR50\">Huang HZ, Wang H, Luo WH, et al., 2017. Real-time neural style transfer for videos. IEEE Conf on Computer Vision and Pattern Recognition, p.7044\u20137052. <a href=\"https://doi.org/10.1109/CVPR.2017.745\">https://doi.org/10.1109/CVPR.2017.745</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR51\">Huang X, Belongie S, 2017. Arbitrary style transfer in realtime with adaptive instance normalization. Proc IEEE Int Conf on Computer Vision, p.1501\u20131510. <a href=\"https://doi.org/10.1109/ICCV.2017.167\">https://doi.org/10.1109/ICCV.2017.167</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR52\">Iizuka S, Simo-Serra E, Ishikawa H, 2017. Globally and locally consistent image completion. <i>ACM Trans Graph</i>, 36(4), Article 107. <a href=\"https://doi.org/10.1145/3072959.3073659\">https://doi.org/10.1145/3072959.3073659</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 52\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Globally%20and%20locally%20consistent%20image%20completion&amp;journal=ACM%20Transactions%20on%20Graphics&amp;volume=36&amp;issue=4&amp;pages=1-14&amp;publication_year=2017&amp;author=Iizuka%2CSatoshi&amp;author=Simo-Serra%2CEdgar&amp;author=Ishikawa%2CHiroshi\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR53\">Isola P, Zhu JY, Zhou TH, et al., 2017. Image-to-image translation with conditional adversarial networks. IEEE Conf on Computer Vision and Pattern Recognition, p.5967\u20135976. <a href=\"https://doi.org/10.1109/CVPR.2017.632\">https://doi.org/10.1109/CVPR.2017.632</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR54\">Jansen BJ, Jung SG, Salminen J, et al., 2017. Viewed by too many or viewed too little: using information dissemination for audience segmentation. <i>Proc Assoc Inform Sci Technol</i>, 54(1):189\u2013196. <a href=\"https://doi.org/10.1002/pra2.2017.14505401021\">https://doi.org/10.1002/pra2.2017.14505401021</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 54\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Viewed%20by%20too%20many%20or%20viewed%20too%20little%3A%20using%20information%20dissemination%20for%20audience%20segmentation&amp;journal=Proc%20Assoc%20Inform%20Sci%20Technol&amp;volume=54&amp;issue=1&amp;pages=189-196&amp;publication_year=2017&amp;author=Jansen%2CBJ&amp;author=Jung%2CSG&amp;author=Salminen%2CJ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR55\">Jansson DG, Smith SM, 1991. Design fixation. <i>Des Stud</i>, 12(1):3\u201311. <a href=\"https://doi.org/10.1016/0142-694X(91)90003-F\">https://doi.org/10.1016/0142-694X(91)90003-F</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 55\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Design%20fixation&amp;journal=Des%20Stud&amp;volume=12&amp;issue=1&amp;pages=3-11&amp;publication_year=1991&amp;author=Jansson%2CDG&amp;author=Smith%2CSM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR56\">Jia J, Huang J, Shen GY, et al., 2016. Learning to appreciate the aesthetic effects of clothing. Proc 30<sup>th</sup> AAAI Conf on Artificial Intelligence, p.1216\u20131222.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR57\">Jia L, Becattini N, Cascini G, et al., 2020. Testing ideation performance on a large set of designers: effects of analogical distance. <i>Int J Des Creat Innov</i>, 8(1):31\u201345. <a href=\"https://doi.org/10.1080/21650349.2019.1618736\">https://doi.org/10.1080/21650349.2019.1618736</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 57\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Testing%20ideation%20performance%20on%20a%20large%20set%20of%20designers%3A%20effects%20of%20analogical%20distance&amp;journal=Int%20J%20Des%20Creat%20Innov&amp;volume=8&amp;issue=1&amp;pages=31-45&amp;publication_year=2020&amp;author=Jia%2CL&amp;author=Becattini%2CN&amp;author=Cascini%2CG\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR58\">Jiang SH, Fu Y, 2017. Fashion style generator. Proc 26<sup>th</sup> Int Joint Conf on Artificial Intelligence, p.3721\u20133727. <a href=\"https://doi.org/10.24963/ijcai.2017/520\">https://doi.org/10.24963/ijcai.2017/520</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR59\">Jing YC, Yang YZ, Feng ZL, et al., 2019. Neural style transfer: a review. <i>IEEE Trans Vis Comput Graph</i>, in press. <a href=\"https://doi.org/10.1109/tvcg.2019.2921336\">https://doi.org/10.1109/tvcg.2019.2921336</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR60\">Jo Y, Park J, 2019. SC-FEGAN: face editing generative adversarial network with user\u2019s sketch and color. <a href=\"https://arxiv.org/abs/1902.06838\">https://doi.org/1902.06838</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR61\">Johnson J, Alahi A, Li FF, 2016. Perceptual losses for real-time style transfer and super-resolution. Proc 14<sup>th</sup> European Conf, p.694\u2013711. <a href=\"https://doi.org/10.1007/978-3-319-46475-6_43\">https://doi.org/10.1007/978-3-319-46475-6_43</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 61\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20Losses%20for%20Real-Time%20Style%20Transfer%20and%20Super-Resolution&amp;pages=694-711&amp;publication_year=2016&amp;author=Johnson%2CJustin&amp;author=Alahi%2CAlexandre&amp;author=Fei-Fei%2CLi\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR62\">Karras T, Laine S, Aila T, 2019. A style-based generator architecture for generative adversarial networks. The IEEE Conf on Computer Vision and Pattern Recognition, p.4401\u20134410.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR63\">Kaufman JC, Sternberg RJ, 2006. The International Handbook of Creativity. Edward Elgar Publishing, Cheltenham, UK.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 63\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20International%20Handbook%20of%20Creativity&amp;publication_year=2006&amp;author=Kaufman%2CJC&amp;author=Sternberg%2CRJ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR64\">Keys R, 1981. Cubic convolution interpolation for digital image processing. <i>IEEE Trans Acoust Speech Signal Process</i>, 29(6):1153\u20131160. <a href=\"https://doi.org/10.1109/TASSP.1981.1163711\">https://doi.org/10.1109/TASSP.1981.1163711</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.ams.org/mathscinet-getitem?mr=642902\" aria-label=\"MathSciNet reference 64\">MathSciNet</a>\u00a0\n    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?0524.65006\" aria-label=\"MATH reference 64\">MATH</a>\u00a0\n    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 64\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Cubic%20convolution%20interpolation%20for%20digital%20image%20processing&amp;journal=IEEE%20Trans%20Acoust%20Speech%20Signal%20Process&amp;volume=29&amp;issue=6&amp;pages=1153-1160&amp;publication_year=1981&amp;author=Keys%2CR\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR65\">Kim J, Lee JK, Lee KM, 2016. Accurate image superresolution using very deep convolutional networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1646\u20131654. <a href=\"https://doi.org/10.1109/CVPR.2016.182\">https://doi.org/10.1109/CVPR.2016.182</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR66\">Kingma DP, Welling M, 2013. Auto-encoding variational Bayes. https://arxiv.org/abs/1312.6114</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR67\">Kong S, Shen XH, Lin Z, et al., 2016. Photo aesthetics ranking network with attributes and content adaptation. Proc 14<sup>th</sup> European Conf on Computer Vision, p.662\u2013679. <a href=\"https://doi.org/10.1007/978-3-319-46448-0_40\">https://doi.org/10.1007/978-3-319-46448-0_40</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 67\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Photo%20Aesthetics%20Ranking%20Network%20with%20Attributes%20and%20Content%20Adaptation&amp;pages=662-679&amp;publication_year=2016&amp;author=Kong%2CShu&amp;author=Shen%2CXiaohui&amp;author=Lin%2CZhe&amp;author=Mech%2CRadomir&amp;author=Fowlkes%2CCharless\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR68\">Krizhevsky A, Hinton G, 2009. Learning Multiple Layers of Features from Tiny Images. Technical Report, University of Toronto, Toronto.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 68\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images&amp;publication_year=2009&amp;author=Krizhevsky%2CA&amp;author=Hinton%2CG\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR69\">Kwak H, An J, Jansen BJ, 2017. Automatic generation of personas using YouTube social media data. Proc 50<sup>th</sup> Hawaii Int Conf on System Sciences, p.833\u2013842.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR70\">Larsen ABL, S\u00f8nderby SK, Larochelle H, et al., 2016. Autoencoding beyond pixels using a learned similarity metric. Proc 33<sup>rd</sup> Int Conf on Machine Learning, p.1558\u20131566.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR71\">LeCun Y, Bottou L, Bengio Y, et al., 1998. Gradient-based learning applied to document recognition. <i>Proc IEEE</i>, 86(11):2278\u20132323. <a href=\"https://doi.org/10.1109/5.726791\">https://doi.org/10.1109/5.726791</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 71\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Gradient-based%20learning%20applied%20to%20document%20recognition&amp;journal=Proc%20IEEE&amp;volume=86&amp;issue=11&amp;pages=2278-2323&amp;publication_year=1998&amp;author=LeCun%2CY&amp;author=Bottou%2CL&amp;author=Bengio%2CY\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR72\">Ledig C, Theis L, Husz\u00e1r F, et al., 2017. Photo-realistic single image super-resolution using a generative adversarial network. IEEE Conf on Computer Vision and Pattern Recognition, p.105\u2013114. <a href=\"https://doi.org/10.1109/CVPR.2017.19\">https://doi.org/10.1109/CVPR.2017.19</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR73\">Li C, Wand M, 2016. Precomputed real-time texture synthesis with Markovian generative adversarial networks. Proc 14<sup>th</sup> European Conf on Computer Vision, p.702\u2013716. <a href=\"https://doi.org/10.1007/978-3-319-46487-9_43\">https://doi.org/10.1007/978-3-319-46487-9_43</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 73\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Precomputed%20Real-Time%20Texture%20Synthesis%20with%20Markovian%20Generative%20Adversarial%20Networks&amp;pages=702-716&amp;publication_year=2016&amp;author=Li%2CChuan&amp;author=Wand%2CMichael\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR74\">Li CC, Chen T, 2009. Aesthetic visual quality assessment of paintings. <i>IEEE J Sel Top Signal Process</i>, 3(2):236\u2013252. <a href=\"https://doi.org/10.1109/JSTSP.2009.2015077\">https://doi.org/10.1109/JSTSP.2009.2015077</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 74\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Aesthetic%20visual%20quality%20assessment%20of%20paintings&amp;journal=IEEE%20J%20Sel%20Top%20Signal%20Process&amp;volume=3&amp;issue=2&amp;pages=236-252&amp;publication_year=2009&amp;author=Li%2CCC&amp;author=Chen%2CT\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR75\">Li HH, Wang JG, Tang MM, et al., 2017. Polarization-dependent effects of an Airy beam due to the spin-orbit coupling. <i>J Opt Soc Am A</i>, 34(7):1114\u20131118. <a href=\"https://doi.org/10.1364/JOSAA.34.001114\">https://doi.org/10.1364/JOSAA.34.001114</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 75\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Polarization-dependent%20effects%20of%20an%20Airy%20beam%20due%20to%20the%20spin-orbit%20coupling&amp;journal=J%20Opt%20Soc%20Am%20A&amp;volume=34&amp;issue=7&amp;pages=1114-1118&amp;publication_year=2017&amp;author=Li%2CHH&amp;author=Wang%2CJG&amp;author=Tang%2CMM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR76\">Li XT, Liu SF, Kautz J, et al., 2019. Learning linear transformations for fast arbitrary style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3809\u20133817.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR77\">Li YJ, Fang C, Yang JM, et al., 2017. Universal style transfer via feature transforms. Proc 31<sup>st</sup> Conf on Neural Information Processing Systems, p.386\u2013396.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR78\">Liu GL, Reda FA, Shih KJ, et al., 2018. Image inpainting for irregular holes using partial convolutions. Proc 15<sup>th</sup> European Conf on Computer Vision, p.85\u2013105. <a href=\"https://doi.org/10.1007/978-3-030-01252-6_6\">https://doi.org/10.1007/978-3-030-01252-6_6</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 78\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Image%20Inpainting%20for%20Irregular%20Holes%20Using%20Partial%20Convolutions&amp;pages=89-105&amp;publication_year=2018&amp;author=Liu%2CGuilin&amp;author=Reda%2CFitsum%20A.&amp;author=Shih%2CKevin%20J.&amp;author=Wang%2CTing-Chun&amp;author=Tao%2CAndrew&amp;author=Catanzaro%2CBryan\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR79\">Liu H, Singh P, 2004. ConceptNet\u2014a practical commonsense reasoning tool-kit. <i>BT Technol</i> J, 22(4):211\u2013226. <a href=\"https://doi.org/10.1023/B:BTTJ.0000047600.45421.6d\">https://doi.org/10.1023/B:BTTJ.0000047600.45421.6d</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 79\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=ConceptNet%E2%80%94a%20practical%20commonsense%20reasoning%20tool-kit&amp;journal=BT%20Technol&amp;volume=22&amp;issue=4&amp;pages=211-226&amp;publication_year=2004&amp;author=Liu%2CH&amp;author=Singh%2CP\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR80\">Liu MY, Huang X, Mallya A, et al., 2019. Few-shot unsupervised image-to-image translation. <a href=\"https://arxiv.org/abs/1905.01723\">https://doi.org/1905.01723</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR81\">Liu ZW, Luo P, Wang XG, et al., 2015. Deep learning face attributes in the wild. Proc IEEE Int Conf on Computer Vision, p.3730\u20133738. <a href=\"https://doi.org/10.1109/ICCV.2015.425\">https://doi.org/10.1109/ICCV.2015.425</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR82\">Lowdermilk T, 2013. User-Centered Design: a Developer\u2019s Guide to Building User-Friendly Applications. O\u2019Reilly, Beijing, China.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 82\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=User-Centered%20Design%3A%20a%20Developer%E2%80%99s%20Guide%20to%20Building%20User-Friendly%20Applications&amp;publication_year=2013&amp;author=Lowdermilk%2CT\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR83\">Lu X, Lin Z, Shen XH, et al., 2015. Deep multi-patch aggregation network for image style, aesthetics, and quality estimation. Proc IEEE Int Conf on Computer Vision, p.990\u2013998. <a href=\"https://doi.org/10.1109/ICCV.2015.119\">https://doi.org/10.1109/ICCV.2015.119</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR84\">Luo YW, Tang XO, 2008. Photo and video quality evaluation: focusing on the subject. Proc 10<sup>th</sup> European Conf on Computer Vision, p.386\u2013399.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 84\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Photo%20and%20Video%20Quality%20Evaluation%3A%20Focusing%20on%20the%20Subject&amp;pages=386-399&amp;publication_year=2008&amp;author=Luo%2CYiwen&amp;author=Tang%2CXiaoou\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR85\">Ma S, Liu J, Chen WC, 2017. A-lamp: adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. Proc 30<sup>th</sup> IEEE Conf on Computer Vision and Pattern Recognition, p.722\u2013731. <a href=\"https://doi.org/10.1109/CVPR.2017.84\">https://doi.org/10.1109/CVPR.2017.84</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR86\">Maguire M, Bevan N, 2002. User requirements analysis. In: Hammond J, Gross T, Wesson J (Eds.), Usability: Gaining a Competitive Edge. Springer, Boston, USA, p.133\u2013148. <a href=\"https://doi.org/10.1007/978-0-387-35610-5_9\">https://doi.org/10.1007/978-0-387-35610-5_9</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 86\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=User%20requirements%20analysis&amp;pages=133-148&amp;publication_year=2002&amp;author=Maguire%2CM&amp;author=Bevan%2CN\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR87\">Mai L, Jin HL, Liu F, 2016. Composition-preserving deep photo aesthetics assessment. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.497\u2013506. <a href=\"https://doi.org/10.1109/CVPR.2016.60\">https://doi.org/10.1109/CVPR.2016.60</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR88\">Matthews T, Judge T, Whittaker S, 2012. How do designers and user experience professionals actually perceive and use personas? Proc Conf on Human Factors in Computing Systems, p.1219\u20131228. <a href=\"https://doi.org/10.1145/2207676.2208573\">https://doi.org/10.1145/2207676.2208573</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR89\">McCaffrey T, Krishnamurty S, 2015. The obscure features hypothesis in design innovation. <i>Int J Des Creat Innov</i>, 3(1):1\u201328. <a href=\"https://doi.org/10.1080/21650349.2014.893840\">https://doi.org/10.1080/21650349.2014.893840</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 89\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20obscure%20features%20hypothesis%20in%20design%20innovation&amp;journal=Int%20J%20Des%20Creat%20Innov&amp;volume=3&amp;issue=1&amp;pages=1-28&amp;publication_year=2015&amp;author=McCaffrey%2CT&amp;author=Krishnamurty%2CS\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR90\">McGinn J, Kotamraju N, 2008. Data-driven persona development. Proc Conf on Human Factors in Computing Systems, p.1521\u20131524. <a href=\"https://doi.org/10.1145/1357054.1357292\">https://doi.org/10.1145/1357054.1357292</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR91\">Miaskiewicz T, Kozar KA, 2011. Personas and user-centered design: how can personas benefit product design processes? <i>Des Stud</i>, 32(5):417\u2013430. <a href=\"https://doi.org/10.1016/j.destud.2011.03.003\">https://doi.org/10.1016/j.destud.2011.03.003</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 91\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Personas%20and%20user-centered%20design%3A%20how%20can%20personas%20benefit%20product%20design%20processes%3F&amp;journal=Des%20Stud&amp;volume=32&amp;issue=5&amp;pages=417-430&amp;publication_year=2011&amp;author=Miaskiewicz%2CT&amp;author=Kozar%2CKA\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR92\">Mikolov T, Chen K, Corrado G, et al., 2013. Efficient estimation of word representations in vector space. <a href=\"https://arxiv.org/abs/1301.3781\">https://doi.org/1301.3781</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR93\">Miller GA, 1995. Wordnet: a lexical database for English. <i>Commun ACM</i>, 38(11):39\u201341. <a href=\"https://doi.org/10.1145/219717.219748\">https://doi.org/10.1145/219717.219748</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 93\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Wordnet%3A%20a%20lexical%20database%20for%20English&amp;journal=Commun%20ACM&amp;volume=38&amp;issue=11&amp;pages=39-41&amp;publication_year=1995&amp;author=Miller%2CGA\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR94\">Mirza M, Osindero S, 2014. Conditional generative adversarial nets. <a href=\"https://arxiv.org/abs/1411.1784\">https://doi.org/1411.1784</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR95\">Miyato T, Kataoka T, Koyama M, et al., 2018. Spectral normalization for generative adversarial networks. Int Conf on Learning Representations.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR96\">Murray N, Marchesotti L, Perronnin F, 2012. AVA: a large-scale database for aesthetic visual analysis. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2408\u20132415. <a href=\"https://doi.org/10.1109/CVPR.2012.6247954\">https://doi.org/10.1109/CVPR.2012.6247954</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR97\">Nazeri K, Ng E, Joseph T, et al., 2019. Edgeconnect: generative image inpainting with adversarial edge learning. <a href=\"https://arxiv.org/abs/1901.00212\">https://doi.org/1901.00212</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR98\">Nelson BA, Wilson JO, Rosen D, et al., 2009. Refined metrics for measuring ideation effectiveness. <i>Des Stud</i>, 30(6):737\u2013743. <a href=\"https://doi.org/10.1016/j.destud.2009.07.002\">https://doi.org/10.1016/j.destud.2009.07.002</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 98\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Refined%20metrics%20for%20measuring%20ideation%20effectiveness&amp;journal=Des%20Stud&amp;volume=30&amp;issue=6&amp;pages=737-743&amp;publication_year=2009&amp;author=Nelson%2CBA&amp;author=Wilson%2CJO&amp;author=Rosen%2CD\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR99\">Nielsen L, Hansen KS, Stage J, et al., 2015. A template for design personas: analysis of 47 persona descriptions from Danish industries and organizations. <i>Int J Sociotechnol Knowl Dev</i>, 7(1):45\u201361. <a href=\"https://doi.org/10.4018/ijskd.2015010104\">https://doi.org/10.4018/ijskd.2015010104</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 99\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20template%20for%20design%20personas%3A%20analysis%20of%2047%20persona%20descriptions%20from%20Danish%20industries%20and%20organizations&amp;journal=Int%20J%20Sociotechnol%20Knowl%20Dev&amp;volume=7&amp;issue=1&amp;pages=45-61&amp;publication_year=2015&amp;author=Nielsen%2CL&amp;author=Hansen%2CKS&amp;author=Stage%2CJ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR100\">Niles I, Pease A, 2001. Towards a standard upper ontology. Proc Int Conf on Formal Ontology in Information Systems, p.2\u20139. <a href=\"https://doi.org/10.1145/505168.505170\">https://doi.org/10.1145/505168.505170</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR101\">Nilsback ME, Zisserman A, 2008. Automated flower classification over a large number of classes. Proc 6<sup>th</sup> Indian Conf on Computer Vision, Graphics &amp; Image Processing, p.722\u2013729. <a href=\"https://doi.org/10.1109/ICVGIP.2008.47\">https://doi.org/10.1109/ICVGIP.2008.47</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR102\">Odena A, Olah C, Shlens J, 2017. Conditional image synthesis with auxiliary classifier GANs. Proc 34<sup>th</sup> Int Conf on Machine Learning, p.4043\u20134055.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR103\">Pan YH, 2017. Special issue on artificial intelligence 2.0. <i>Front Inform Technol Electron Eng</i>, 18(1):1\u20132. <a href=\"https://doi.org/10.1631/FITEE.1710000\">https://doi.org/10.1631/FITEE.1710000</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 103\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Special%20issue%20on%20artificial%20intelligence%202.0&amp;journal=Front%20Inform%20Technol%20Electron%20Eng&amp;volume=18&amp;issue=1&amp;pages=1-2&amp;publication_year=2017&amp;author=Pan%2CYH\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR104\">Park T, Liu MY, Wang TC, et al., 2019. Semantic image synthesis with spatially-adaptive normalization. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2337\u20132346.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR105\">Pathak D, Kr\u00e4henb\u00fchl P, Donahue J, et al., 2016. Context encoders: feature learning by inpainting. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2536\u20132544. <a href=\"https://doi.org/10.1109/CVPR.2016.278\">https://doi.org/10.1109/CVPR.2016.278</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR106\">Peeters JR, Verhaegen PA, Vandevenne D, et al., 2010. Refined metrics for measuring novelty in ideation. ID-MME Virtual Concept Research in Interaction Design, Article 4.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR107\">Perera D, Zimmermann R, 2019. CNGAN: generative adversarial networks for cross-network user preference generation for non-overlapped users. World Wide Web Conf, p.3144\u20133150. <a href=\"https://doi.org/10.1145/3308558.3313733\">https://doi.org/10.1145/3308558.3313733</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR108\">Pruitt J, Adlin T, 2005. The Persona Lifecycle: Keeping People in Mind Throughout Product Design. Elsevier, Amsterdam, p.724.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 108\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20Persona%20Lifecycle%3A%20Keeping%20People%20in%20Mind%20Throughout%20Product%20Design&amp;publication_year=2005&amp;author=Pruitt%2CJ&amp;author=Adlin%2CT\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR109\">Radford A, Metz L, Chintala S, 2016. Unsupervised representation learning with deep convolutional generative adversarial networks. Proc 4<sup>th</sup> Int Conf on Learning Representations.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR110\">Reed SE, Akata Z, Yan XC, et al., 2016a. Generative adversarial text to image synthesis. Proc 33<sup>rd</sup> Int Conf on Machine Learning, p.1681\u20131690.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR111\">Reed SE, Akata Z, Mohan S, et al., 2016b. Learning what and where to draw. Advances in Neural Information Processing Systems, p.217\u2013225.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR112\">Rigau J, Feixas M, Sbert M, 2008. Informational aesthetics measures. <i>IEEE Comput Graph Appl</i>, 28(2):24\u201334. <a href=\"https://doi.org/10.1109/MCG.2008.34\">https://doi.org/10.1109/MCG.2008.34</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 112\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Informational%20aesthetics%20measures&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=28&amp;issue=2&amp;pages=24-34&amp;publication_year=2008&amp;author=Rigau%2CJ&amp;author=Feixas%2CM&amp;author=Sbert%2CM\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR113\">Russell SJ, Norvig P, 2016. Artificial Intelligence: a Modern Approach. Pearson Education Limited, Harlow, Essex.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?0835.68093\" aria-label=\"MATH reference 113\">MATH</a>\u00a0\n    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 113\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20Intelligence%3A%20a%20Modern%20Approach&amp;publication_year=2016&amp;author=Russell%2CSJ&amp;author=Norvig%2CP\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR114\">Saleh B, Elgammal A, 2015. Large-scale classification of fine-art paintings: learning the right metric on the right feature. <a href=\"https://arxiv.org/abs/1505.00855\">https://doi.org/1505.00855</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR115\">Salimans T, Goodfellow IJ, Zaremba W, et al., 2016. Improved techniques for training GANs. Advances in Neural Information Processing Systems, p.2226\u20132234.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR116\">Salminen J, Seng\u00fcn S, Kwak H, et al., 2017. Generating cultural personas from social data: a perspective of middle eastern users. Proc 5<sup>th</sup> Int Conf on Future Internet of Things and Cloud Workshops, p.120\u2013125. <a href=\"https://doi.org/10.1109/FiCloudW.2017.97\">https://doi.org/10.1109/FiCloudW.2017.97</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR117\">Salminen J, Jansen BJ, An J, et al., 2018a. Are personas done? Evaluating their usefulness in the age of digital analytics. <i>Persona Stud</i>, 4(2):47\u201365. <a href=\"https://doi.org/10.21153/psj2018vol4no2art737\">https://doi.org/10.21153/psj2018vol4no2art737</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 117\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Are%20personas%20done%3F%20Evaluating%20their%20usefulness%20in%20the%20age%20of%20digital%20analytics&amp;journal=Persona%20Stud&amp;volume=4&amp;issue=2&amp;pages=47-65&amp;publication_year=2018&amp;author=Salminen%2CJ&amp;author=Jansen%2CBJ&amp;author=An%2CJ\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR118\">Salminen J, Jung SG, An J, et al., 2018b. Findings of a user study of automatically generated personas. Proc Conf on Human Factors in Computing Systems, p.LBW097. <a href=\"https://doi.org/10.1145/3170427.3188470\">https://doi.org/10.1145/3170427.3188470</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR119\">Salminen J, Eng\u00fcn S, Jung SG, et al., 2019. Design issues in automatically generated persona profiles: a qualitative analysis from 38 think-aloud transcripts. Proc Conf on Human Information Interaction and Retrieval, p.225\u2013229. <a href=\"https://doi.org/10.1145/3295750.3298942\">https://doi.org/10.1145/3295750.3298942</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR120\">Schwarz K, Wieschollek P, Lensch HPA, 2018. Will people like your image? Learning the aesthetic space. Proc IEEE Winter Conf on Applications of Computer Vision, p.2048\u20132057. <a href=\"https://doi.org/10.1109/WACV.2018.00226\">https://doi.org/10.1109/WACV.2018.00226</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR121\">Shah JJ, Kulkarni SV, Vargas-Hernandez N, 2000. Evaluation of idea generation methods for conceptual design: effectiveness metrics and design of experiments. <i>J Mech Des</i>, 122(4):377\u2013384. <a href=\"https://doi.org/10.1115/1.1315592\">https://doi.org/10.1115/1.1315592</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 121\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20idea%20generation%20methods%20for%20conceptual%20design%3A%20effectiveness%20metrics%20and%20design%20of%20experiments&amp;journal=J%20Mech%20Des&amp;volume=122&amp;issue=4&amp;pages=377-384&amp;publication_year=2000&amp;author=Shah%2CJJ&amp;author=Kulkarni%2CSV&amp;author=Vargas-Hernandez%2CN\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR122\">Simonyan K, Zisserman A, 2014. Very deep convolutional networks for large-scale image recognition. <a href=\"https://arxiv.org/abs/1409.1556\">https://doi.org/1409.1556</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR123\">Strohmann T, Siemon D, Robra-Bissantz S, 2017. brAInstorm: intelligent assistance in group idea generation. Proc 12<sup>th</sup> Int Conf on Design Science Research in Information System and Technology, p.457\u2013461. <a href=\"https://doi.org/10.1007/978-3-319-59144-5_31\">https://doi.org/10.1007/978-3-319-59144-5_31</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 123\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=brAInstorm%3A%20Intelligent%20Assistance%20in%20Group%20Idea%20Generation&amp;pages=457-461&amp;publication_year=2017&amp;author=Strohmann%2CTimo&amp;author=Siemon%2CDominik&amp;author=Robra-Bissantz%2CSusanne\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR124\">Strothotte T, Schlechtweg S, 2002. Non-photorealistic Computer Graphics: Modeling, Rendering, and Animation. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 124\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Non-photorealistic%20Computer%20Graphics%3A%20Modeling%2C%20Rendering%2C%20and%20Animation&amp;publication_year=2002&amp;author=Strothotte%2CT&amp;author=Schlechtweg%2CS\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR125\">Tang X, Wang ZW, Luo WX, et al., 2018. Face aging with identity-preserved conditional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.7939\u20137947. <a href=\"https://doi.org/10.1109/CVPR.2018.00828\">https://doi.org/10.1109/CVPR.2018.00828</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR126\">Tang XO, Luo W, Wang XG, 2013. Content-based photo quality assessment. <i>IEEE Trans Multim</i>, 15(8):1930\u20131943. <a href=\"https://doi.org/10.1109/TMM.2013.2269899\">https://doi.org/10.1109/TMM.2013.2269899</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 126\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Content-based%20photo%20quality%20assessment&amp;journal=IEEE%20Trans%20Multim&amp;volume=15&amp;issue=8&amp;pages=1930-1943&amp;publication_year=2013&amp;author=Tang%2CXO&amp;author=Luo%2CW&amp;author=Wang%2CXG\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR127\">Vandevenne D, Verhaegen PA, Dewulf S, et al., 2015. A scalable approach for ideation in biologically inspired design. <i>Artif Intell Eng Des Anal Manuf</i>, 29(1):19\u201331. <a href=\"https://doi.org/10.1017/S0890060414000122\">https://doi.org/10.1017/S0890060414000122</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 127\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20scalable%20approach%20for%20ideation%20in%20biologically%20inspired%20design&amp;journal=Artif%20Intell%20Eng%20Des%20Anal%20Manuf&amp;volume=29&amp;issue=1&amp;pages=19-31&amp;publication_year=2015&amp;author=Vandevenne%2CD&amp;author=Verhaegen%2CPA&amp;author=Dewulf%2CS\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR128\">Varshney LR, Pinel F, Varshney KR, et al., 2019. A big data approach to computational creativity: the curious case of Chef Watson. <i>IBM J Res Dev</i>, 63(1):7:1\u20137:18. <a href=\"https://doi.org/10.1147/JRD.2019.2893905\">https://doi.org/10.1147/JRD.2019.2893905</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 128\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20big%20data%20approach%20to%20computational%20creativity%3A%20the%20curious%20case%20of%20Chef%20Watson&amp;journal=IBM%20J%20Res%20Dev&amp;volume=63&amp;issue=1&amp;pages=7%3A1-7%3A18&amp;publication_year=2019&amp;author=Varshney%2CLR&amp;author=Pinel%2CF&amp;author=Varshney%2CKR\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR129\">Verma P, Smith JO, 2018. Neural style transfer for audio spectograms. <a href=\"https://arxiv.org/abs/1801.01589\">https://doi.org/1801.01589</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR130\">Wang J, Yu LT, Zhang WN, et al., 2017. IRGAN: a minimax game for unifying generative and discriminative information retrieval models. Proc 40<sup>th</sup> Int ACM SI-GIR Conf on Research and Development in Information Retrieval, p.515\u2013524. <a href=\"https://doi.org/10.1145/3077136.3080786\">https://doi.org/10.1145/3077136.3080786</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR131\">Wang TC, Liu MY, Zhu JY, et al., 2018. Video-to-video synthesis. <a href=\"https://arxiv.org/abs/1808.06601\">https://doi.org/1808.06601</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR132\">Wang WG, Shen JB, 2017. Deep cropping via attention box prediction and aesthetics assessment. Proc IEEE Int Conf on Computer Vision, p.2205\u20132213. <a href=\"https://doi.org/10.1109/ICCV.2017.240\">https://doi.org/10.1109/ICCV.2017.240</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR133\">Wang WN, Cai D, Wang L, et al., 2016. Synthesized computational aesthetic evaluation of photos. <i>Neurocomputing</i>, 172:244\u2013252. <a href=\"https://doi.org/10.1016/j.neucom.2014.12.106\">https://doi.org/10.1016/j.neucom.2014.12.106</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 133\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Synthesized%20computational%20aesthetic%20evaluation%20of%20photos&amp;journal=Neurocomputing&amp;volume=172&amp;pages=244-252&amp;publication_year=2016&amp;author=Wang%2CWN&amp;author=Cai%2CD&amp;author=Wang%2CL\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR134\">Wang WS, Yang S, Zhang WS, et al., 2018. Neural aesthetic image reviewer. <a href=\"https://arxiv.org/abs/1802.10240\">https://doi.org/1802.10240</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR135\">Wang XT, Yu K, Wu SX, et al., 2018. ESRGAN: enhanced super-resolution generative adversarial networks. European Conf on Computer Vision, p.63\u201379. <a href=\"https://doi.org/10.1007/978-3-030-11021-5_5\">https://doi.org/10.1007/978-3-030-11021-5_5</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 135\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=ESRGAN%3A%20Enhanced%20Super-Resolution%20Generative%20Adversarial%20Networks&amp;pages=63-79&amp;publication_year=2019&amp;author=Wang%2CXintao&amp;author=Yu%2CKe&amp;author=Wu%2CShixiang&amp;author=Gu%2CJinjin&amp;author=Liu%2CYihao&amp;author=Dong%2CChao&amp;author=Qiao%2CYu&amp;author=Loy%2CChen%20Change\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR136\">Wu JJ, Zhang CK, Xue TF, et al., 2016. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. Advances in Neural Information Processing Systems, p.82\u201390.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR137\">Xu T, Zhang PC, Huang QY, et al., 2018. AttnGAN: fine-grained text to image generation with attentional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1316\u20131324. <a href=\"https://doi.org/10.1109/CVPR.2018.00143\">https://doi.org/10.1109/CVPR.2018.00143</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR138\">Yan Y, Wang JR, Tang C, et al., 2019. Research on the development of contemporary design intelligence driven by neural network technology. In: Marcus A, Wang WT (Eds.), Design, User Experience, and Usability. Design Philosophy and Theory. Springer, Cham, p.368\u2013381. <a href=\"https://doi.org/10.1007/978-3-030-23570-3_27\">https://doi.org/10.1007/978-3-030-23570-3_27</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 138\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Research%20on%20the%20development%20of%20contemporary%20design%20intelligence%20driven%20by%20neural%20network%20technology&amp;pages=368-381&amp;publication_year=2019&amp;author=Yan%2CY&amp;author=Wang%2CJR&amp;author=Tang%2CC\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR139\">Yang HY, Huang D, Wang YH, et al., 2018. Learning face age progression: a pyramid architecture of GANs. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.31\u201339. <a href=\"https://doi.org/10.1109/CVPR.2018.00011\">https://doi.org/10.1109/CVPR.2018.00011</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR140\">Yang WM, Zhang XC, Tian YP, et al., 2019. Deep learning for single image super-resolution: a brief review. <i>IEEE Trans Multim</i>, 21(12):3106\u20133121. <a href=\"https://doi.org/10.1109/tmm.2019.2919431\">https://doi.org/10.1109/tmm.2019.2919431</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 140\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20for%20single%20image%20super-resolution%3A%20a%20brief%20review&amp;journal=IEEE%20Trans%20Multim&amp;volume=21&amp;issue=12&amp;pages=3106-3121&amp;publication_year=2019&amp;author=Yang%2CWM&amp;author=Zhang%2CXC&amp;author=Tian%2CYP\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR141\">Yang Y, Zhuang YT, Wu F, et al., 2008. Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval. <i>IEEE Trans Multim</i>, 10(3):437\u2013446. <a href=\"https://doi.org/10.1109/TMM.2008.917359\">https://doi.org/10.1109/TMM.2008.917359</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 141\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Harmonizing%20hierarchical%20manifolds%20for%20multimedia%20document%20semantics%20understanding%20and%20cross-media%20retrieval&amp;journal=IEEE%20Trans%20Multim&amp;volume=10&amp;issue=3&amp;pages=437-446&amp;publication_year=2008&amp;author=Yang%2CY&amp;author=Zhuang%2CYT&amp;author=Wu%2CF\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR142\">Yi ZL, Zhang H, Tan P, et al., 2017. DualGAN: unsupervised dual learning for image-to-image translation. Proc IEEE Int Conf on Computer Vision, p.2868\u20132876. <a href=\"https://doi.org/10.1109/ICCV.2017.310\">https://doi.org/10.1109/ICCV.2017.310</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR143\">Yoon Y, Jeon HG, Yoo D, et al., 2015. Learning a deep convolutional network for light-field image super-resolution. Proc IEEE Int Conf on Computer Vision, p.57\u201365. https://doi.org/10.1109/ICCVW.2015.17</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR144\">You S, You N, Pan MX, 2019. PI-REC: progressive image reconstruction network with edge and color domain. <a href=\"https://arxiv.org/abs/1903.10146\">https://doi.org/1903.10146</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR145\">Yu F, Zhang YD, Song SR, et al., 2015. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. <a href=\"https://arxiv.org/abs/1506.03365\">https://doi.org/1506.03365</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR146\">Yu JH, Lin Z, Yang JM, et al., 2018a. Free-form image inpainting with gated convolution. <a href=\"https://arxiv.org/abs/1806.03589\">https://doi.org/1806.03589</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR147\">Yu JH, Lin Z, Yang JM, et al., 2018b. Generative image inpainting with contextual attention. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.5505\u20135514. <a href=\"https://doi.org/10.1109/CVPR.2018.00577\">https://doi.org/10.1109/CVPR.2018.00577</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR148\">Zakharov E, Shysheya A, Burkov E, et al., 2019. Fewshot adversarial learning of realistic neural talking head models. <a href=\"https://arxiv.org/abs/1905.08233\">https://doi.org/1905.08233</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR149\">Zeiler MD, Taylor GW, Fergus R, 2011. Adaptive deconvolutional networks for mid and high level feature learning. Proc IEEE Int Conf on Computer Vision, p.2018\u20132025. <a href=\"https://doi.org/10.1109/ICCV.2011.6126474\">https://doi.org/10.1109/ICCV.2011.6126474</a>\n</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR150\">Zhang H, Xu T, Li H, et al., 2017. StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks. Proc IEEE Int Conf on Computer Vision, p.5907\u20135915.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR151\">Zhang H, Xu T, Li H, et al., 2019. StackGAN++: realistic image synthesis with stacked generative adversarial networks. <i>IEEE Trans Patt Anal Mach Intell</i>, 41(8):1947\u20131962. <a href=\"https://doi.org/10.1109/TPAMI.2018.2856256\">https://doi.org/10.1109/TPAMI.2018.2856256</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 151\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=StackGAN%2B%2B%3A%20realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks&amp;journal=IEEE%20Trans%20Patt%20Anal%20Mach%20Intell&amp;volume=41&amp;issue=8&amp;pages=1947-1962&amp;publication_year=2019&amp;author=Zhang%2CH&amp;author=Xu%2CT&amp;author=Li%2CH\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR152\">Zhang JJ, Yu JH, Zhang K, et al., 2017. Computational aesthetic evaluation of logos. <i>ACM Trans Appl Perc</i>, 14(3), Article 20. <a href=\"https://doi.org/10.1145/3058982\">https://doi.org/10.1145/3058982</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 152\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Computational%20Aesthetic%20Evaluation%20of%20Logos&amp;journal=ACM%20Transactions%20on%20Applied%20Perception&amp;volume=14&amp;issue=3&amp;pages=1-21&amp;publication_year=2017&amp;author=Zhang%2CJiajing&amp;author=Yu%2CJinhui&amp;author=Zhang%2CKang&amp;author=Zheng%2CXianjun%20Sam&amp;author=Zhang%2CJunsong\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR153\">Zhang R, Isola P, Efros AA, 2016. Colorful image colorization. Proc 14<sup>th</sup> European Conf on Computer Vision, p.649\u2013666. <a href=\"https://doi.org/10.1007/978-3-319-46487-9_40\">https://doi.org/10.1007/978-3-319-46487-9_40</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 153\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Colorful%20Image%20Colorization&amp;pages=649-666&amp;publication_year=2016&amp;author=Zhang%2CRichard&amp;author=Isola%2CPhillip&amp;author=Efros%2CAlexei%20A.\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR154\">Zhao H, Gallo O, Frosio I, et al., 2016. Loss functions for image restoration with neural networks. <i>IEEE Trans Comput Imag</i>, 3(1):47\u201357. <a href=\"https://doi.org/10.1109/tci.2016.2644865\">https://doi.org/10.1109/tci.2016.2644865</a>\n</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 154\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Loss%20functions%20for%20image%20restoration%20with%20neural%20networks&amp;journal=IEEE%20Trans%20Comput%20Imag&amp;volume=3&amp;issue=1&amp;pages=47-57&amp;publication_year=2016&amp;author=Zhao%2CH&amp;author=Gallo%2CO&amp;author=Frosio%2CI\">\n                    Google Scholar</a>\u00a0\n                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR155\">Zhu JY, Park T, Isola P, et al., 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. Proc IEEE Int Conf on Computer Vision, p.2242\u20132251. <a href=\"https://doi.org/10.1109/ICCV.2017.244\">https://doi.org/10.1109/ICCV.2017.244</a>\n</p></li></ol><p class=\"c-article-references__download u-hide-print\"><a data-track=\"click\" data-track-action=\"download citation references\" data-track-label=\"link\" href=\"https://citation-needed.springer.com/v2/references/10.1631/FITEE.1900398?format=refman&amp;flavour=references\">Download references<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#global-icon-download\"></use></svg></a></p></div></div></div></section></div><section data-title=\"Acknowledgements\"><div class=\"c-article-section\" id=\"Ack1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Ack1\">Acknowledgements</h2><div class=\"c-article-section__content\" id=\"Ack1-content\"><p>Figs. 5c, 5d, and 8 in this study were generated by the pre-trained models of Runway toolkit (https://runwayml.com).</p></div></div></section><section aria-labelledby=\"author-information\" data-title=\"Author information\"><div class=\"c-article-section\" id=\"author-information-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"author-information\">Author information</h2><div class=\"c-article-section__content\" id=\"author-information-content\"><h3 class=\"c-article__sub-heading\" id=\"affiliations\">Affiliations</h3><ol class=\"c-article-author-affiliation__list\"><li id=\"Aff1\"><p class=\"c-article-author-affiliation__address\">College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China</p><p class=\"c-article-author-affiliation__authors-list\">Yong-chuan Tang,\u00a0Jiang-jie Huang,\u00a0Meng-ting Yao,\u00a0Jia Wei,\u00a0Wei Li,\u00a0Yong-xing He\u00a0&amp;\u00a0Ze-jian Li</p></li><li id=\"Aff2\"><p class=\"c-article-author-affiliation__address\">Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, 310027, China</p><p class=\"c-article-author-affiliation__authors-list\">Yong-chuan Tang,\u00a0Jiang-jie Huang,\u00a0Meng-ting Yao,\u00a0Jia Wei,\u00a0Wei Li\u00a0&amp;\u00a0Yong-xing He</p></li><li id=\"Aff3\"><p class=\"c-article-author-affiliation__address\">Zhejiang Lab, Hangzhou, 310012, China</p><p class=\"c-article-author-affiliation__authors-list\">Yong-chuan Tang,\u00a0Wei Li,\u00a0Yong-xing He\u00a0&amp;\u00a0Ze-jian Li</p></li><li id=\"Aff4\"><p class=\"c-article-author-affiliation__address\">Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, 310027, China</p><p class=\"c-article-author-affiliation__authors-list\">Ze-jian Li</p></li></ol><div class=\"u-js-hide u-hide-print\" data-test=\"author-info\"><span class=\"c-article__sub-heading\">Authors</span><ol class=\"c-article-authors-search u-list-reset\"><li id=\"auth-Yong_chuan-Tang\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Yong-chuan Tang</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Yong-chuan+Tang&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yong-chuan+Tang\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yong-chuan+Tang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Jiang_jie-Huang\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Jiang-jie Huang</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Jiang-jie+Huang&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jiang-jie+Huang\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jiang-jie+Huang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Meng_ting-Yao\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Meng-ting Yao</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Meng-ting+Yao&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Meng-ting+Yao\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Meng-ting+Yao%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Jia-Wei\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Jia Wei</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Jia+Wei&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jia+Wei\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jia+Wei%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Wei-Li\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Wei Li</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Wei+Li&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Wei+Li\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Wei+Li%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Yong_xing-He\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Yong-xing He</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Yong-xing+He&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yong-xing+He\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yong-xing+He%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Ze_jian-Li\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Ze-jian Li</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Ze-jian+Li&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in\n                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ze-jian+Li\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ze-jian+Li%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li></ol></div><h3 class=\"c-article__sub-heading\" id=\"corresponding-author\">Corresponding author</h3><p id=\"corresponding-author-list\">Correspondence to\n                <a id=\"corresp-c1\" href=\"mailto:yctang@zju.edu.cn\">Yong-chuan Tang</a>.</p></div></div></section><section data-title=\"Additional information\"><div class=\"c-article-section\" id=\"additional-information-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"additional-information\">Additional information</h2><div class=\"c-article-section__content\" id=\"additional-information-content\"><h3 class=\"c-article__sub-heading\">Compliance with ethics guidelines</h3><p>Yong-chuan TANG, Jiang-jie HUANG, Meng-ting YAO, Jia WEI, Wei LI, Yong-xing HE, and Ze-jian LI declare that they have no conflict of interest.</p><p>Project supported by the National Science and Technology Innovation 2030 Major Project of the Ministry of Science and Technology of China (No. 2018AAA0100703), the National Natural Science Foundation of China (Nos. 61773336 and 91748127), the Chinese Academy of Engineering Consulting Project (No. 2018-ZD-12-06), the Provincial Key Research and Development Plan of Zhejiang Province, China (No. 2019C03137), and the Ng Teng Fong Charitable Foundation in the form of ZJU-SUTD IDEA Grant</p></div></div></section><section data-title=\"Rights and permissions\"><div class=\"c-article-section\" id=\"rightslink-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"rightslink\">Rights and permissions</h2><div class=\"c-article-section__content\" id=\"rightslink-content\"><p class=\"c-article-rights\"><a data-track=\"click\" data-track-action=\"view rights and permissions\" data-track-label=\"link\" href=\"https://s100.copyright.com/AppDispatchServlet?title=A%20review%20of%20design%20intelligence%3A%20progress%2C%20problems%2C%20and%20challenges&amp;author=Yong-chuan%20Tang%20et%20al&amp;contentID=10.1631%2FFITEE.1900398&amp;copyright=Zhejiang%20University%20and%20Springer-Verlag%20GmbH%20Germany%2C%20part%20of%20Springer%20Nature&amp;publication=2095-9184&amp;publicationDate=2020-02-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true\">Reprints and Permissions</a></p></div></div></section><section aria-labelledby=\"article-info\" data-title=\"About this article\"><div class=\"c-article-section\" id=\"article-info-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"article-info\">About this article</h2><div class=\"c-article-section__content\" id=\"article-info-content\"><div class=\"c-bibliographic-information\"><div class=\"u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border\"><a data-crossmark=\"10.1631/FITEE.1900398\" target=\"_blank\" rel=\"noopener\" href=\"https://crossmark.crossref.org/dialog/?doi=10.1631/FITEE.1900398\" data-track=\"click\" data-track-action=\"Click Crossmark\" data-track-label=\"link\" data-test=\"crossmark\"><img width=\"57\" height=\"81\" alt=\"Verify currency and authenticity via CrossMark\" src=\"data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>\" /></a></div><div class=\"c-bibliographic-information__column\"><h3 class=\"c-article__sub-heading\" id=\"citeas\">Cite this article</h3><p class=\"c-bibliographic-information__citation\">Tang, Yc., Huang, Jj., Yao, Mt. <i>et al.</i> A review of design intelligence: progress, problems, and challenges.\n                    <i>Front Inform Technol Electron Eng</i> <b>20, </b>1595\u20131617 (2019). https://doi.org/10.1631/FITEE.1900398</p><p class=\"c-bibliographic-information__download-citation u-hide-print\"><a data-test=\"citation-link\" data-track=\"click\" data-track-action=\"download article citation\" data-track-label=\"link\" data-track-external=\"\" href=\"https://citation-needed.springer.com/v2/references/10.1631/FITEE.1900398?format=refman&amp;flavour=citation\">Download citation<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#global-icon-download\"></use></svg></a></p><ul class=\"c-bibliographic-information__list\" data-test=\"publication-history\"><li class=\"c-bibliographic-information__list-item\"><p>Received<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2019-08-06\">06 August 2019</time></span></p></li><li class=\"c-bibliographic-information__list-item\"><p>Accepted<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2019-11-26\">26 November 2019</time></span></p></li><li class=\"c-bibliographic-information__list-item\"><p>Published<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2020-02-13\">13 February 2020</time></span></p></li><li class=\"c-bibliographic-information__list-item\"><p>Issue Date<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2019-12\">December 2019</time></span></p></li><li class=\"c-bibliographic-information__list-item c-bibliographic-information__list-item--doi\"><p><abbr title=\"Digital Object Identifier\">DOI</abbr><span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><a href=\"https://doi.org/10.1631/FITEE.1900398\" data-track=\"click\" data-track-action=\"view doi\" data-track-label=\"link\">https://doi.org/10.1631/FITEE.1900398</a></span></p></li></ul><div data-component=\"share-box\"><div class=\"c-article-share-box u-display-none\" hidden=\"\"><h3 class=\"c-article__sub-heading\">Share this article</h3><p class=\"c-article-share-box__description\">Anyone you share the following link with will be able to read this content:</p><button class=\"js-get-share-url c-article-share-box__button\" id=\"get-share-url\" data-track=\"click\" data-track-label=\"button\" data-track-external=\"\" data-track-action=\"get shareable link\">Get shareable link</button><div class=\"js-no-share-url-container u-display-none\" hidden=\"\"><p class=\"js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info\">Sorry, a shareable link is not currently available for this article.</p></div><div class=\"js-share-url-container u-display-none\" hidden=\"\"><p class=\"js-share-url c-article-share-box__only-read-input\" id=\"share-url\" data-track=\"click\" data-track-label=\"button\" data-track-action=\"select share url\"></p><button class=\"js-copy-share-url c-article-share-box__button--link-like\" id=\"copy-share-url\" data-track=\"click\" data-track-label=\"button\" data-track-action=\"copy share url\" data-track-external=\"\">Copy to clipboard</button></div><p class=\"js-c-article-share-box__additional-info c-article-share-box__additional-info\">\n                            Provided by the Springer Nature SharedIt content-sharing initiative\n                        </p></div></div><h3 class=\"c-article__sub-heading\">Key words</h3><ul class=\"c-article-subject-list\"><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Design intelligence</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Creativity</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Personas</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Ideation</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">AI-generated content</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Computational aesthetics</span></li></ul><h3 class=\"c-article__sub-heading\">CLC number</h3><ul class=\"c-article-subject-list\"><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">TP183</span></li></ul><div data-component=\"article-info-list\"></div></div></div></div></div></section>\n                </div>\n            </article>\n        </main>\n\n        <div class=\"c-article-extras u-text-sm u-hide-print\" id=\"sidebar\" data-container-type=\"reading-companion\" data-track-component=\"reading companion\">\n            <aside>\n                <div data-test=\"download-article-link-wrapper\">\n                    \n                </div>\n\n                <div data-test=\"collections\">\n                    \n    \n\n                </div>\n\n                <div data-test=\"editorial-summary\">\n                    \n                </div>\n\n                <div class=\"c-reading-companion\">\n                    <div class=\"c-reading-companion__sticky\" data-component=\"reading-companion-sticky\" data-test=\"reading-companion-sticky\">\n                        \n                            <div class=\"c-article-buy-box\">\n                                <div class=\"sprcom-buybox-articleSidebar\" id=\"sprcom-buybox-articleSidebar\">\n <h2 class=\"c-box__heading\">Access options</h2>\n <article class=\"c-box\" data-test-id=\"buy-article\">\n  <h3 class=\"c-box__heading\">Buy single article</h3>\n  <div class=\"c-box__body\">\n   <div class=\"buybox__info\">\n    <p>Instant access to the full article PDF.</p>\n   </div>\n   <div class=\"buybox__buy\">\n    <p class=\"buybox__price\">USD 39.95</p>\n    <p class=\"buybox__price-info\">Price includes VAT (Brazil)<br>Tax calculation will be finalised during checkout.</p>\n    <form action=\"https://order.springer.com/public/checkout?abtest=v2\" method=\"post\">\n     <input type=\"hidden\" name=\"type\" value=\"article\">\n     <input type=\"hidden\" name=\"doi\" value=\"10.1631/FITEE.1900398\">\n     <input type=\"hidden\" name=\"isxn\" value=\"2095-9230\">\n     <input type=\"hidden\" name=\"contenttitle\" value=\"A review of design intelligence: progress, problems, and challenges\">\n     <input type=\"hidden\" name=\"copyrightyear\" value=\"2019\">\n     <input type=\"hidden\" name=\"year\" value=\"2020\">\n     <input type=\"hidden\" name=\"authors\" value=\"Yong-chuan Tang, et al.\">\n     <input type=\"hidden\" name=\"title\" value=\"Frontiers of Information Technology &amp; Electronic Engineering\">\n     <input type=\"hidden\" name=\"mac\" value=\"82FB53023BBB3109B3CB91533294E4D6\">\n     <input type=\"submit\" class=\"c-box__button\" onclick=\"dataLayer.push({&quot;event&quot;:&quot;addToCart&quot;,&quot;ecommerce&quot;:{&quot;currencyCode&quot;:&quot;USD&quot;,&quot;add&quot;:{&quot;products&quot;:[{&quot;name&quot;:&quot;A review of design intelligence: progress, problems, and challenges&quot;,&quot;id&quot;:&quot;2095-9230&quot;,&quot;price&quot;:39.95,&quot;brand&quot;:&quot;Zhejiang University Press&quot;,&quot;category&quot;:&quot;Computer Science&quot;,&quot;variant&quot;:&quot;ppv-article&quot;,&quot;quantity&quot;:1}]}}});\" value=\"Buy article PDF\">\n    </form>\n   </div>\n  </div>\n  <script>dataLayer.push({\"ecommerce\":{\"currency\":\"USD\",\"impressions\":[{\"name\":\"A review of design intelligence: progress, problems, and challenges\",\"id\":\"2095-9230\",\"price\":39.95,\"brand\":\"Zhejiang University Press\",\"category\":\"Computer Science\",\"variant\":\"ppv-article\",\"quantity\":1}]}});</script>\n </article>\n <article class=\"c-box buybox__rent-article\" id=\"deepdyve\" style=\"display: none\" data-test-id=\"journal-subscription\">\n  <div class=\"c-box__body\">\n   <div class=\"buybox__info\">\n    <p><a class=\"deepdyve-link\" target=\"deepdyve\" rel=\"nofollow\" data-track=\"click\" data-track-action=\"rent article\" data-track-label=\"rent action, new buybox\">Rent this article via DeepDyve.</a></p>\n   </div>\n  </div>\n  <script>\n            function deepDyveResponse(data) {\n                if (data.status === 'ok') {\n                    [].slice.call(document.querySelectorAll('.c-box.buybox__rent-article')).forEach(function (article) {\n                        article.style.display = 'flex'\n                        var link = article.querySelector('.deepdyve-link')\n                        if (link) {\n                          link.setAttribute('href', data.url)\n                        }\n                    })\n                }\n            }\n\n            var script = document.createElement('script')\n            script.src = '//www.deepdyve.com/rental-link?docId=10.1631/FITEE.1900398&journal=2095-9230&fieldName=journal_doi&affiliateId=springer&format=jsonp&callback=deepDyveResponse'\n            document.body.appendChild(script)\n          </script>\n </article>\n <aside class=\"buybox__institutional-sub\">\n  <div class=\"c-box__body\">\n   <div class=\"buybox__info\">\n    <p><a href=\"https://www.springernature.com/gp/librarians/licensing/license-options?&amp;abtest=v2\" data-track=\"click\" data-track-action=\"institutional link\" data-track-label=\"institutional subscriptions, new buybox\">Learn more about Institutional subscriptions</a></p>\n   </div>\n  </div>\n </aside>\n <style>.sprcom-buybox-articleSidebar{\n  box-shadow: 0px 0px 5px rgba(51,51,51,0.101);\n  display: flex;\n  flex-wrap: wrap;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;\n  text-align: center;\n}\n.sprcom-buybox-articleSidebar *{\n  box-sizing: border-box;\n  line-height: calc(100% + 4px);\n  margin: 0px;\n}\n.sprcom-buybox-articleSidebar > *{\n  display: flex;\n  flex-basis: 240px;\n  flex-direction: column;\n  flex-grow: 1;\n  flex-shrink: 1;\n  margin: 0.5px;\n}\n.sprcom-buybox-articleSidebar > *{\n  box-shadow: 0 0 0 1px rgba(204,204,204,0.494);\n}\n.sprcom-buybox-articleSidebar .c-box__body{\n  display: flex;\n  flex-direction: column-reverse;\n  flex-grow: 1;\n  justify-content: space-between;\n  padding: 6%;\n}\n.sprcom-buybox-articleSidebar .c-box__body .buybox__buy{\n  display: flex;\n  flex-direction: column-reverse;\n}\n.sprcom-buybox-articleSidebar p{\n  color: #333;\n  font-size: 15px;\n}\n.sprcom-buybox-articleSidebar .buybox__price{\n  font-size: 24px;\n  font-weight: 500;\n  line-height: calc(100% + 8px);\n  margin: 20px 0;\n  order: 1;\n}\n.sprcom-buybox-articleSidebar form{\n  order: 1;\n}\n.sprcom-buybox-articleSidebar .buybox__price-info{\n  margin-bottom: 20px;\n}\n.sprcom-buybox-articleSidebar .c-box__heading{\n  background-color: #f0f0f0;\n  color: #333;\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;\n  font-size: 16px;\n  margin: 0px;\n  padding: 10px 12px;\n  text-align: center;\n}\n.sprcom-buybox-articleSidebar .c-box__button{\n  background-color: #3365A4;\n  border: 1px solid transparent;\n  border-radius: 2px;\n  color: #fff;\n  cursor: pointer;\n  display: inline-block;\n  font-family: inherit;\n  font-size: 16px;\n  max-width: 222px;\n  padding: 10px 12px;\n  text-decoration: none;\n  width: 100%;\n}\n.sprcom-buybox-articleSidebar h3{\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  overflow: hidden;\n  position: absolute;\n  width: 1px;\n}\n.sprcom-buybox-articleSidebar h2{\n  flex-basis: 100%;\n  margin-bottom: 16px;\n  text-align: left;\n}\n.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .c-box__body{\n  flex-direction: row;\n}\n.sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .buybox__info{\n  text-align: left;\n}\n.sprcom-buybox-articleSidebar .buybox__institutional-sub{\n  background-color: #f0f0f0;\n}\n.sprcom-buybox-articleSidebar .visually-hidden{\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  overflow: hidden;\n  position: absolute;\n  width: 1px;\n}\n.sprcom-buybox-articleSidebar style{\n  display: none;\n}\n</style>\n</div>\n                            </div>\n                        \n\n                        <div class=\"c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active\" id=\"tabpanel-sections\">\n                            <div class=\"u-mt-16\" data-component-mpu>\n    <aside class=\"c-ad c-ad--300x250\">\n        <div class=\"c-ad__inner\">\n            <p class=\"c-ad__label\">Advertisement</p>\n            <div id=\"div-gpt-ad-MPU1\" data-pa11y-ignore data-gpt data-gpt-unitpath=\"/270604982/springerlink/11714/article\" data-gpt-sizes=\"300x250\" data-gpt-targeting=\"pos=MPU1;articleid=FITEE.1900398;\"></div>\n        </div>\n    </aside>\n</div>\n\n                        </div>\n                        <div class=\"c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width\" id=\"tabpanel-figures\"></div>\n                        <div class=\"c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width\" id=\"tabpanel-references\"></div>\n                    </div>\n                </div>\n            </aside>\n        </div>\n    </div>\n\n    \n    <script src=\"/.cdn-rum/performance.js\" async></script>\n\n    \n        <script>\n            \n        </script>\n    \n\n\n        \n    <footer class=\"app-footer\" role=\"contentinfo\" data-test=\"springerlink-footer\">\n        <div class=\"app-footer__aside-wrapper u-hide-print\">\n            <div class=\"app-footer__container\">\n                <p class=\"app-footer__strapline\">Over 10 million scientific documents at your fingertips</p>\n                \n                    <div class=\"app-footer__edition\" data-component=\"SV.EditionSwitcher\">\n                        <span class=\"u-visually-hidden\" data-role=\"button-dropdown__title\" data-btn-text=\"Switch between Academic & Corporate Edition\">Switch Edition</span>\n                        <ul class=\"app-footer-edition-list\" data-role=\"button-dropdown__content\" data-test=\"footer-edition-switcher-list\">\n                            <li class=\"selected\">\n                                <a data-test=\"footer-academic-link\"\n                                   href=\"/siteEdition/link\"\n                                   id=\"siteedition-academic-link\">Academic Edition</a>\n                            </li>\n                            <li>\n                                <a data-test=\"footer-corporate-link\"\n                                   href=\"/siteEdition/rd\"\n                                   id=\"siteedition-corporate-link\">Corporate Edition</a>\n                            </li>\n                        </ul>\n                    </div>\n                \n            </div>\n        </div>\n        <div class=\"app-footer__container\">\n            <ul class=\"app-footer__nav u-hide-print\">\n                <li><a href=\"/\">Home</a></li>\n                <li><a href=\"/impressum\">Impressum</a></li>\n                <li><a href=\"/termsandconditions\">Legal information</a></li>\n                <li><a href=\"/privacystatement\">Privacy statement</a></li>\n                <li><a href=\"https://www.springernature.com/ccpa\">California Privacy Statement</a></li>\n                <li><a href=\"/cookiepolicy\">How we use cookies</a></li>\n                \n                <li><a class=\"optanon-toggle-display\" data-cc-action=\"preferences\" href=\"javascript:void(0);\">Manage cookies/Do not sell my data</a></li>\n                \n                <li><a href=\"/accessibility\">Accessibility</a></li>\n                <li><a href=\"https://support.springer.com/en/support/home\">FAQ</a></li>\n                <li><a id=\"contactus-footer-link\" href=\"https://support.springer.com/en/support/solutions/articles/6000206179-contacting-us\">Contact us</a></li>\n                <li><a href=\"https://www.springer.com/gp/shop/promo/affiliate/springer-nature\">Affiliate program</a></li>\n            </ul>\n            <div class=\"c-user-metadata\">\n    \n        <p class=\"c-user-metadata__item\">\n            <span data-test=\"footer-user-login-status\">Not logged in</span>\n            <span data-test=\"footer-user-ip\"> - 201.35.134.227</span>\n        </p>\n        <p class=\"c-user-metadata__item\" data-test=\"footer-business-partners\">\n            Not affiliated\n        </p>\n\n    \n</div>\n\n            <a class=\"app-footer__parent-logo\" target=\"_blank\" rel=\"noopener\" href=\"//www.springernature.com\"  title=\"Go to Springer Nature\">\n                <span class=\"u-visually-hidden\">Springer Nature</span>\n                <svg width=\"125\" height=\"12\" focusable=\"false\" aria-hidden=\"true\">\n                    <image width=\"125\" height=\"12\" alt=\"Springer Nature logo\"\n                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png\n                           xmlns:xlink=\"http://www.w3.org/1999/xlink\"\n                           xlink:href=/oscar-static/images/springerlink/svg/springernature-b88bf25ad4.svg>\n                    </image>\n                </svg>\n            </a>\n            <p class=\"app-footer__copyright\">&copy; 2021 Springer Nature Switzerland AG. Part of <a target=\"_blank\" rel=\"noopener\" href=\"//www.springernature.com\">Springer Nature</a>.</p>\n            \n        </div>\n        \n    <svg class=\"u-hide hide\">\n        <symbol id=\"global-icon-chevron-right\" viewBox=\"0 0 16 16\">\n            <path d=\"M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"global-icon-download\" viewBox=\"0 0 16 16\">\n            <path d=\"M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"global-icon-email\" viewBox=\"0 0 18 18\">\n            <path d=\"M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"global-icon-institution\" viewBox=\"0 0 18 18\">\n            <path d=\"M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"global-icon-search\" viewBox=\"0 0 22 22\">\n            <path fill-rule=\"evenodd\" d=\"M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z\"/>\n        </symbol>\n        <symbol id=\"icon-info\" viewBox=\"0 0 18 18\">\n            <path d=\"m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"icon-success\" viewBox=\"0 0 18 18\">\n            <path d=\"M9 0a9 9 0 110 18A9 9 0 019 0zm3.486 4.982l-4.718 5.506L5.14 8.465a.991.991 0 00-1.423.133 1.06 1.06 0 00.13 1.463l3.407 2.733a1 1 0 001.387-.133l5.385-6.334a1.06 1.06 0 00-.116-1.464.991.991 0 00-1.424.119z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"icon-chevron-down\" viewBox=\"0 0 16 16\">\n            <path d=\"m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z\" fill-rule=\"evenodd\" transform=\"matrix(0 1 -1 0 11 1)\"/>\n        </symbol>\n        <symbol id=\"icon-warning\" viewBox=\"0 0 18 18\">\n            <path d=\"m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"icon-plus\" viewBox=\"0 0 16 16\">\n            <path d=\"m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z\" fill-rule=\"evenodd\"/>\n        </symbol>\n        <symbol id=\"icon-minus\" viewBox=\"0 0 16 16\">\n            <path d=\"m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z\" fill-rule=\"evenodd\"/>\n        </symbol>\n    </svg>\n\n    </footer>\n\n\n\n    </div>\n    \n    \n</body>\n</html>\n", "response": ["GET /10.1631/FITEE.1900398 HTTP/1.1", "Host: link.springer.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "HTTP/1.1 302 Found", "Connection: keep-alive", "Content-Length: 0", "Cache-Control: max-age=0", "Expires: Tue, 30 Nov 2021 20:20:23 GMT", "Location: https://idp.springer.com/authorize?redirect_uri=https://link.springer.com/10.1631/FITEE.1900398&client_id=springerlink&response_type=cookie", "Server: Oscar Platform 0.352.0", "X-Environment: live", "X-Frame-Options: DENY", "X-Origin-Server: 4206aa7c-113d-41a2-764f-807f", "X-Vcap-Request-Id: bde3fab2-53c6-487f-4639-5e5f95755884", "Via: 1.1 google, 1.1 varnish", "X-Cdn-Origin: SNPaaS", "Accept-Ranges: bytes", "Date: Tue, 30 Nov 2021 20:20:23 GMT", "Age: 0", "X-Served-By: cache-247323b9-internal, cache-cwb20523-CWB", "X-Cache: MISS, MISS", "X-Cache-Hits: 0", "X-Timer: S1638303624.552748,VS0,VE224", "GET /authorize?redirect_uri=https://link.springer.com/10.1631/FITEE.1900398&client_id=springerlink&response_type=cookie HTTP/1.1", "Host: idp.springer.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "HTTP/1.1 302 Found", "Connection: keep-alive", "Content-Length: 0", "Cache-Control: no-cache, no-store, max-age=0, must-revalidate", "Expires: 0", "Location: https://idp.springer.com/transit?redirect_uri=https%3A%2F%2Flink.springer.com%2F10.1631%2FFITEE.1900398&code=15c8a299-e65a-4112-9c2a-1f985a283d4a", "Pragma: no-cache", "Set-Cookie: idp_session=sVERSION_167b9ddc6-e723-438a-9f35-4bfb74c9df25; Domain=.springer.com; Path=/; Secure; HttpOnly", "Set-Cookie: idp_session_http=hVERSION_1d95f5db4-9dd7-47ca-ab67-0b9f9e85244d; Domain=.springer.com; Path=/; HttpOnly", "Set-Cookie: idp_marker=f12429b0-6dd3-4b78-b908-630cb023754b; Domain=.springer.com; Path=/; Max-Age=315360000; HttpOnly", "Strict-Transport-Security: max-age=31536000 ; includeSubDomains", "X-B3-Spanid: ae58849b5f67e913", "X-B3-Traceid: ae58849b5f67e913", "X-Content-Type-Options: nosniff", "X-Frame-Options: DENY", "X-Vcap-Request-Id: c3cd5ada-6b16-417f-6200-c18dd00246f5", "X-Xss-Protection: 1; mode=block", "Via: 1.1 google, 1.1 varnish", "X-Cdn-Origin: SNPaaS", "Accept-Ranges: bytes", "Date: Tue, 30 Nov 2021 20:20:24 GMT", "X-Served-By: cache-cwb20523-CWB", "X-Cache: MISS", "X-Cache-Hits: 0", "X-Timer: S1638303624.860869,VS0,VE217", "Vary: x-forwarded-proto", "GET /transit?redirect_uri=https%3A%2F%2Flink.springer.com%2F10.1631%2FFITEE.1900398&code=15c8a299-e65a-4112-9c2a-1f985a283d4a HTTP/1.1", "Host: idp.springer.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: idp_session=sVERSION_167b9ddc6-e723-438a-9f35-4bfb74c9df25; idp_session_http=hVERSION_1d95f5db4-9dd7-47ca-ab67-0b9f9e85244d; idp_marker=f12429b0-6dd3-4b78-b908-630cb023754b", "HTTP/1.1 302 Found", "Connection: keep-alive", "Content-Length: 0", "Cache-Control: no-cache, no-store, max-age=0, must-revalidate", "Content-Language: en-US", "Expires: 0", "Location: https://link.springer.com/10.1631/FITEE.1900398", "Pragma: no-cache", "Strict-Transport-Security: max-age=31536000 ; includeSubDomains", "X-B3-Spanid: da8cbd7150e0a0f3", "X-B3-Traceid: da8cbd7150e0a0f3", "X-Content-Type-Options: nosniff", "X-Frame-Options: DENY", "X-Vcap-Request-Id: b1d918ea-31df-423c-4569-4e2a52aa74b5", "X-Xss-Protection: 1; mode=block", "Via: 1.1 google, 1.1 varnish", "X-Cdn-Origin: SNPaaS", "Accept-Ranges: bytes", "Date: Tue, 30 Nov 2021 20:20:24 GMT", "X-Served-By: cache-cwb20523-CWB", "X-Cache: MISS", "X-Cache-Hits: 0", "X-Timer: S1638303624.099418,VS0,VE210", "Vary: x-forwarded-proto", "GET /10.1631/FITEE.1900398 HTTP/1.1", "Host: link.springer.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: idp_marker=f12429b0-6dd3-4b78-b908-630cb023754b; idp_session=sVERSION_167b9ddc6-e723-438a-9f35-4bfb74c9df25; idp_session_http=hVERSION_1d95f5db4-9dd7-47ca-ab67-0b9f9e85244d", "HTTP/1.1 302 Found", "Connection: keep-alive", "Content-Length: 0", "Age: 0", "Cache-Control: max-age=0", "Content-Type: text/html;charset=utf-8", "Expires: Thu, 01 Jan 1970 00:00:00 GMT", "Location: https://link.springer.com/article/10.1631%2FFITEE.1900398", "Server: Oscar Platform 0.352.0", "Set-Cookie: trackid=e8b4877c7f924eb281479a78c; Path=/; Domain=.springer.com; Secure; HttpOnly", "X-Environment: live", "X-Frame-Options: DENY", "X-Origin-Server: 26b94eec-4964-4251-6195-60e8", "X-Vcap-Request-Id: 79fb34ae-a231-4750-4311-1ea99e468e9a", "Via: 1.1 google, 1.1 varnish", "X-Cdn-Origin: SNPaaS", "Accept-Ranges: bytes", "Date: Tue, 30 Nov 2021 20:20:24 GMT", "X-Served-By: cache-247323b9-internal, cache-cwb20523-CWB", "X-Cache: MISS, MISS", "X-Cache-Hits: 0", "X-Timer: S1638303624.336028,VS0,VE240", "GET /article/10.1631%2FFITEE.1900398 HTTP/1.1", "Host: link.springer.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: idp_marker=f12429b0-6dd3-4b78-b908-630cb023754b; idp_session=sVERSION_167b9ddc6-e723-438a-9f35-4bfb74c9df25; idp_session_http=hVERSION_1d95f5db4-9dd7-47ca-ab67-0b9f9e85244d; trackid=e8b4877c7f924eb281479a78c", "HTTP/1.1 200 OK", "Connection: keep-alive", "Accept-Ranges: bytes", "Age: 0", "Content-Encoding: gzip", "Content-Type: text/html;charset=utf-8", "Etag: \"ad60f63d255b7f6b536c88fc9f8e0f6f\"", "Server: Oscar Platform 0.352.0", "Set-Cookie: sim-inst-token=\"\"; Domain=.springer.com; Path=/; secure; HttpOnly", "Traceparent: 00-6a2735ca1a57196b73bc556dcfc62f39-ff75b9932dffeeac-01", "X-B3-Sampled: 1", "X-B3-Spanid: ba4ac5f6dc00bcc7", "X-B3-Traceid: ba4ac5f6dc00bcc7", "X-Dump-Request-Bodies: 0", "X-Frame-Options: DENY", "X-Vcap-Request-Id: af16658a-b69c-47df-50cd-7e1960485e25", "Via: 1.1 google, 1.1 varnish", "X-Cdn-Origin: SNPaaS", "Date: Tue, 30 Nov 2021 20:20:25 GMT", "X-Served-By: cache-5f84e6e5-internal, cache-cwb20523-CWB", "X-Cache: MISS, MISS", "X-Cache-Hits: 0", "X-Timer: S1638303625.597211,VS0,VE577", "Vary: x-frame-options,X-Oscar-Cache-Mode, Accept-Encoding", "transfer-encoding: chunked", "<!DOCTYPE html>", "<html lang=\"en\" class=\"no-js\">", "<head>", "    <meta charset=\"UTF-8\">", "    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">", "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">", "    <meta name=\"applicable-device\" content=\"pc,mobile\">", "    <meta name=\"access\" content=\"No\">", "    ", "    <meta name=\"twitter:site\" content=\"@SpringerLink\"/>", "    <meta name=\"twitter:card\" content=\"summary\"/>", "    <meta name=\"twitter:image:alt\" content=\"Content cover image\"/>", "    <meta name=\"twitter:title\" content=\"A review of design intelligence: progress, problems, and challenges\"/>", "    <meta name=\"twitter:description\" content=\"Frontiers of Information Technology &amp; Electronic Engineering - Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in...\"/>", "    <meta name=\"twitter:image\" content=\"https://static-content.springer.com/cover/journal/11714/20/12.jpg\"/>", "    <meta name=\"journal_id\" content=\"11714\"/>", "    <meta name=\"dc.title\" content=\"A review of design intelligence: progress, problems, and challenges\"/>", "    <meta name=\"dc.source\" content=\"Frontiers of Information Technology &amp; Electronic Engineering 2019 20:12\"/>", "    <meta name=\"dc.format\" content=\"text/html\"/>", "    <meta name=\"dc.publisher\" content=\"Springer\"/>", "    <meta name=\"dc.date\" content=\"2020-02-13\"/>", "    <meta name=\"dc.type\" content=\"ReviewPaper\"/>", "    <meta name=\"dc.language\" content=\"En\"/>", "    <meta name=\"dc.copyright\" content=\"2019 Zhejiang University and Springer-Verlag GmbH Germany, part of Springer Nature\"/>", "    <meta name=\"dc.rightsAgent\" content=\"journalpermissions@springernature.com\"/>", "    <meta name=\"dc.description\" content=\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.\"/>", "    <meta name=\"prism.issn\" content=\"2095-9230\"/>", "    <meta name=\"prism.publicationName\" content=\"Frontiers of Information Technology &amp; Electronic Engineering\"/>", "    <meta name=\"prism.publicationDate\" content=\"2020-02-13\"/>", "    <meta name=\"prism.volume\" content=\"20\"/>", "    <meta name=\"prism.number\" content=\"12\"/>", "    <meta name=\"prism.section\" content=\"ReviewPaper\"/>", "    <meta name=\"prism.startingPage\" content=\"1595\"/>", "    <meta name=\"prism.endingPage\" content=\"1617\"/>", "    <meta name=\"prism.copyright\" content=\"2019 Zhejiang University and Springer-Verlag GmbH Germany, part of Springer Nature\"/>", "    <meta name=\"prism.rightsAgent\" content=\"journalpermissions@springernature.com\"/>", "    <meta name=\"prism.url\" content=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>", "    <meta name=\"prism.doi\" content=\"doi:10.1631/FITEE.1900398\"/>", "    <meta name=\"citation_pdf_url\" content=\"https://link.springer.com/content/pdf/10.1631/FITEE.1900398.pdf\"/>", "    <meta name=\"citation_fulltext_html_url\" content=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>", "    <meta name=\"citation_journal_title\" content=\"Frontiers of Information Technology &amp; Electronic Engineering\"/>", "    <meta name=\"citation_journal_abbrev\" content=\"Front Inform Technol Electron Eng\"/>", "    <meta name=\"citation_publisher\" content=\"Zhejiang University Press\"/>", "    <meta name=\"citation_issn\" content=\"2095-9230\"/>", "    <meta name=\"citation_title\" content=\"A review of design intelligence: progress, problems, and challenges\"/>", "    <meta name=\"citation_volume\" content=\"20\"/>", "    <meta name=\"citation_issue\" content=\"12\"/>", "    <meta name=\"citation_publication_date\" content=\"2019/12\"/>", "    <meta name=\"citation_online_date\" content=\"2020/02/13\"/>", "    <meta name=\"citation_firstpage\" content=\"1595\"/>", "    <meta name=\"citation_lastpage\" content=\"1617\"/>", "    <meta name=\"citation_article_type\" content=\"Review\"/>", "    <meta name=\"citation_language\" content=\"en\"/>", "    <meta name=\"dc.identifier\" content=\"doi:10.1631/FITEE.1900398\"/>", "    <meta name=\"DOI\" content=\"10.1631/FITEE.1900398\"/>", "    <meta name=\"citation_doi\" content=\"10.1631/FITEE.1900398\"/>", "    <meta name=\"description\" content=\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In \"/>", "    <meta name=\"dc.creator\" content=\"Tang, Yong-chuan\"/>", "    <meta name=\"dc.creator\" content=\"Huang, Jiang-jie\"/>", "    <meta name=\"dc.creator\" content=\"Yao, Meng-ting\"/>", "    <meta name=\"dc.creator\" content=\"Wei, Jia\"/>", "    <meta name=\"dc.creator\" content=\"Li, Wei\"/>", "    <meta name=\"dc.creator\" content=\"He, Yong-xing\"/>", "    <meta name=\"dc.creator\" content=\"Li, Ze-jian\"/>", "    <meta name=\"dc.subject\" content=\"Computer Science, general\"/>", "    <meta name=\"dc.subject\" content=\"Electrical Engineering\"/>", "    <meta name=\"dc.subject\" content=\"Computer Hardware\"/>", "    <meta name=\"dc.subject\" content=\"Computer Systems Organization and Communication Networks\"/>", "    <meta name=\"dc.subject\" content=\"Electronics and Microelectronics, Instrumentation\"/>", "    <meta name=\"dc.subject\" content=\"Communications Engineering, Networks\"/>", "    <meta name=\"citation_reference\" content=\"Arjovsky M, Chintala S, Bottou L, 2017. Wasserstein generative adversarial networks. Proc 34th Int Conf on Machine Learning, p.298&#8211;321.\"/>", "    <meta name=\"citation_reference\" content=\"Aubry M, Maturana D, Efros AA, et al., 2014. Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3762&#8211;3769. ", "https://doi.org/10.1109/CVPR.2014.487", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Image Process; citation_title=Filling-in by joint interpolation of vector fields and gray levels; citation_author=C Ballester, M Bertalmio, V Caselles; citation_volume=10; citation_issue=8; citation_publication_date=2001; citation_pages=1200-1211; citation_id=CR3\"/>", "    <meta name=\"citation_reference\" content=\"Bertalmio M, Sapiro G, Caselles V, et al., 2000. Image in-painting. Proc 27th Annual Conf on Computer Graphics and Interactive Techniques, p.417&#8211;424. ", "https://doi.org/10.1145/344779.344972", "\"/>", "    <meta name=\"citation_reference\" content=\"Bharadhwaj H, Park H, Lim BY, 2018. RecGAN: recurrent generative adversarial networks for recommendation systems. Proc 12th ACM Conf on Recommender Systems, p.372&#8211;376. ", "https://doi.org/10.1145/3240323.3240383", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=AI Mag; citation_title=Computer models of creativity; citation_author=MA Boden; citation_volume=30; citation_issue=3; citation_publication_date=2009; citation_pages=23-34; citation_id=CR6\"/>", "    <meta name=\"citation_reference\" content=\"Brock A, Donahue J, Simonyan K, 2018. Large scale GAN training for high fidelity natural image synthesis. ", "https://doi.org/1809.11096", "\"/>", "    <meta name=\"citation_reference\" content=\"Bruna J, Sprechmann P, LeCun Y, 2015. Super-resolution with deep convolutional sufficient statistics. ", "https://doi.org/1511.05666", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Idea inspire 3.0&#8212;a tool for analogical design; citation_inbook_title=Research into Design for Communities; citation_publication_date=2017; citation_pages=475-485; citation_id=CR9; citation_author=A Chakrabarti; citation_author=L Siddharth; citation_author=M Dinakar; citation_publisher=Springer\"/>", "    <meta name=\"citation_reference\" content=\"Champandard AJ, 2016. Semantic style transfer and turning two-bit doodles into fine artworks. ", "https://doi.org/1603.01768", "\"/>", "    <meta name=\"citation_reference\" content=\"Chan C, Ginosar S, Zhou TH, et al., 2018. Everybody dance now. ", "https://doi.org/1808.07371", "\"/>", "    <meta name=\"citation_reference\" content=\"Chen DD, Yuan L, Liao J, et al., 2018. Stereoscopic neural style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.6654&#8211;6663. ", "https://doi.org/10.1109/CVPR.2018.00696", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=J Vis Commun Image Represent; citation_title=An artificial intelligence based data-driven approach for design ideation; citation_author=LQ Chen, P Wang, H Dong; citation_volume=61; citation_publication_date=2019; citation_pages=10-22; citation_id=CR13\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Finding Image Features Associated with High Aesthetic Value by Machine Learning; citation_inbook_title=Evolutionary and Biologically Inspired Music, Sound, Art and Design; citation_publication_date=2013; citation_pages=47-58; citation_id=CR14; citation_author=Vic Ciesielski; citation_author=Perry Barile; citation_author=Karen Trist; citation_publisher=Springer Berlin Heidelberg\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=The Inmates Are Running the Asylum; citation_publication_date=1999; citation_id=CR15; citation_author=A Cooper; citation_publisher=SAMS\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=About Face 2.0: the Essentials of Interaction Design; citation_publication_date=2003; citation_id=CR16; citation_author=A Cooper; citation_author=RM Reimann; citation_publisher=John Wiley &amp; Sons\"/>", "    <meta name=\"citation_reference\" content=\"Dash A, Gamboa JCB, Ahmed S, et al., 2017. TAC-GAN-text conditioned auxiliary classifier generative adversarial network. ", "https://doi.org/1703.06412", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Studying Aesthetics in Photographic Images Using a Computational Approach; citation_inbook_title=Computer Vision &#8211; ECCV 2006; citation_publication_date=2006; citation_pages=288-301; citation_id=CR18; citation_author=Ritendra Datta; citation_author=Dhiraj Joshi; citation_author=Jia Li; citation_author=James Z. Wang; citation_publisher=Springer Berlin Heidelberg\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=An Evolutionary Approach to Case Adaptation; citation_inbook_title=Case-Based Reasoning Research and Development; citation_publication_date=1999; citation_pages=162-173; citation_id=CR19; citation_author=Andr&#233;s de G&#243;mez Silva Garza; citation_author=Mary Lou Maher; citation_publisher=Springer Berlin Heidelberg\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Artif Intell Rev; citation_title=An introduction to and comparison of computational creativity and design computing; citation_author=AG de Silva Garza; citation_volume=51; citation_issue=1; citation_publication_date=2019; citation_pages=61-76; citation_id=CR20\"/>", "    <meta name=\"citation_reference\" content=\"Deng J, Dong W, Socher R, et al., 2009. ImageNet: a large-scale hierarchical image database. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.248&#8211;255. ", "https://doi.org/10.1109/CVPR.2009.5206848", "\"/>", "    <meta name=\"citation_reference\" content=\"Deng YB, Loy CC, Tang XO, 2018. Aesthetic-driven image enhancement by adversarial learning. Proc 26th ACM Int Conf on Multimedia, p.870&#8211;878. ", "https://doi.org/10.1145/3240508.3240531", "\"/>", "    <meta name=\"citation_reference\" content=\"Donahue J, Kr&#228;henb&#252;hl P, Darrell T, 2016. Adversarial feature learning. ", "https://doi.org/1605.09782", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Hum Comput Stud; citation_title=Webthetics: quantifying webpage aesthetics with deep learning; citation_author=Q Dou, XS Zheng, TF Sun; citation_volume=124; citation_publication_date=2019; citation_pages=56-66; citation_id=CR24\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=J Pers Soc Psychol; citation_title=Cognitive stimulation in brainstorming; citation_author=KL Dugosh, PB Paulus, EJ Roland; citation_volume=79; citation_issue=5; citation_publication_date=2000; citation_pages=722-735; citation_id=CR25\"/>", "    <meta name=\"citation_reference\" content=\"Dumoulin V, Visin F, 2016. A guide to convolution arithmetic for deep learning. ", "https://doi.org/1603.07285", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=MRI: Clinical Magnetic Resonance Imaging; citation_publication_date=1996; citation_id=CR27; citation_author=RR Edelman; citation_author=JR Hesselink; citation_author=MB Zlatkin; citation_publisher=Saunders\"/>", "    <meta name=\"citation_reference\" content=\"Efros AA, Freeman WT, 2001. Image quilting for texture synthesis and transfer. Proc 28th Annual Conf on Computer Graphics and Interactive Techniques, p.341&#8211;346. ", "https://doi.org/10.1145/383259.383296", "\"/>", "    <meta name=\"citation_reference\" content=\"Elgammal A, Liu B, Elhoseiny M, et al., 2017. CAN: creative adversarial networks, generating &#8220;art&#8221; by learning about styles and deviating from style norms. ", "https://doi.org/1706.07068", "\"/>", "    <meta name=\"citation_reference\" content=\"Fang H, Zhang M, 2017. Creatism: a deep-learning photographer capable of creating professional work. ", "https://doi.org/1707.03491", "\"/>", "    <meta name=\"citation_reference\" content=\"Faste H, Rachmel N, Essary R, et al., 2013. Brainstorm, chainstorm, cheatstorm, tweetstorm: new ideation strategies for distributed HCI design. Proc Conf on Human Factors in Computing Systems, p.1343&#8211;1352. ", "https://doi.org/10.1145/2470654.2466177", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Res Eng Des; citation_title=Design-by-analogy: experimental evaluation of a functional analogy search methodology for concept generation improvement; citation_author=K Fu, J Murphy, M Yang; citation_volume=26; citation_issue=1; citation_publication_date=2015; citation_pages=77-95; citation_id=CR32\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Bull Amer Math Soc; citation_title=Birkhoff on aesthetic measure; citation_author=CA Garabedian; citation_volume=40; citation_issue=1; citation_publication_date=1934; citation_pages=7-10; citation_id=CR33\"/>", "    <meta name=\"citation_reference\" content=\"Gatys L, Ecker A, Bethge M, 2016a. Image style transfer using convolutional neural networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2414&#8211;2423. ", "https://doi.org/10.1109/CVPR.2016.265", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=J Vis; citation_title=A neural algorithm of artistic style; citation_author=L Gatys, A Ecker, M Bethge; citation_volume=16; citation_issue=12; citation_publication_date=2016; citation_pages=326; citation_id=CR35\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=AI Mag; citation_title=Design prototypes: a knowledge representation schema for design; citation_author=JS Gero; citation_volume=11; citation_issue=4; citation_publication_date=1990; citation_pages=26-36; citation_id=CR36\"/>", "    <meta name=\"citation_reference\" content=\"Gilon K, Chan J, Ng FY, et al., 2018. Analogy mining for specific design needs. Proc CHI Conf on Human Factors in Computing Systems, p.121. ", "https://doi.org/10.1145/3173574.3173695", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=AI Edam; citation_title=Structure, behavior, and function of complex systems: the structure, behavior, and function modeling language; citation_author=AK Goel, S Rugaber, S Vattam; citation_volume=23; citation_issue=1; citation_publication_date=2009; citation_pages=23-35; citation_id=CR38\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Variances in the impact of visual stimuli on design problem solving performance; citation_author=G Goldschmidt, M Smolkov; citation_volume=27; citation_issue=5; citation_publication_date=2006; citation_pages=549-569; citation_id=CR39\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Non-photorealistic Rendering; citation_publication_date=2001; citation_id=CR40; citation_author=B Gooch; citation_author=A Gooch; citation_publisher=A K Peters/CRC Press\"/>", "    <meta name=\"citation_reference\" content=\"Goodfellow I, Pouget-Abadie J, Mirza M, et al., 2014. Generative adversarial nets. Proc 27th Int Conf on Neural Information Processing Systems, p.2672&#8211;2680.\"/>", "    <meta name=\"citation_reference\" content=\"Grudin J, Pruitt J, 2002. Personas, participatory design, and product development: an infrastructure for engagement. Proc 7th Biennial Participatory Design Conf, p.144&#8211;152.\"/>", "    <meta name=\"citation_reference\" content=\"Gulrajani I, Ahmed F, Arjovsky M, et al., 2017. Improved training of Wasserstein GANs. Advances in Neural Information Proc Systems, p.5767&#8211;5777.\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Artif Intell Eng Des Anal Manuf; citation_title=A computational tool for creative idea generation based on analogical reasoning and ontology; citation_author=J Han, F Shi, LQ Chen; citation_volume=32; citation_issue=4; citation_publication_date=2018; citation_pages=462-477; citation_id=CR44\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=J Intell Manuf; citation_title=An evolutionary computation based method for creative design inspiration generation; citation_author=J Hao, YJ Zhou, QF Zhao; citation_volume=30; citation_issue=4; citation_publication_date=2019; citation_pages=1673-1691; citation_id=CR45\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=The UX Book: Process and Guidelines for Ensuring a Quality User Experience; citation_publication_date=2012; citation_id=CR46; citation_author=R Hartson; citation_author=PS Pyla; citation_publisher=Elsevier\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Patt Anal Mach Intell; citation_title=Image completion approaches using the statistics of similar patches; citation_author=KM He, J Sun; citation_volume=36; citation_issue=12; citation_publication_date=2014; citation_pages=2423-2435; citation_id=CR47\"/>", "    <meta name=\"citation_reference\" content=\"Hertzmann A, Jacobs CE, Oliver N, et al., 2001. Image analogies. Proc 28th Annual Conf on Computer Graphics and Interactive Techniques, p.327&#8211;340. ", "https://doi.org/10.1145/383259.383295", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=ACM Comput Surv; citation_title=How generative adversarial networks and their variants work: an overview; citation_author=YJ Hong, U Hwang, J Yoo; citation_volume=52; citation_issue=1; citation_publication_date=2019; citation_pages=10; citation_id=CR49\"/>", "    <meta name=\"citation_reference\" content=\"Huang HZ, Wang H, Luo WH, et al., 2017. Real-time neural style transfer for videos. IEEE Conf on Computer Vision and Pattern Recognition, p.7044&#8211;7052. ", "https://doi.org/10.1109/CVPR.2017.745", "\"/>", "    <meta name=\"citation_reference\" content=\"Huang X, Belongie S, 2017. Arbitrary style transfer in realtime with adaptive instance normalization. Proc IEEE Int Conf on Computer Vision, p.1501&#8211;1510. ", "https://doi.org/10.1109/ICCV.2017.167", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=ACM Transactions on Graphics; citation_title=Globally and locally consistent image completion; citation_author=Satoshi Iizuka, Edgar Simo-Serra, Hiroshi Ishikawa; citation_volume=36; citation_issue=4; citation_publication_date=2017; citation_pages=1-14; citation_id=CR52\"/>", "    <meta name=\"citation_reference\" content=\"Isola P, Zhu JY, Zhou TH, et al., 2017. Image-to-image translation with conditional adversarial networks. IEEE Conf on Computer Vision and Pattern Recognition, p.5967&#8211;5976. ", "https://doi.org/10.1109/CVPR.2017.632", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Proc Assoc Inform Sci Technol; citation_title=Viewed by too many or viewed too little: using information dissemination for audience segmentation; citation_author=BJ Jansen, SG Jung, J Salminen; citation_volume=54; citation_issue=1; citation_publication_date=2017; citation_pages=189-196; citation_id=CR54\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Design fixation; citation_author=DG Jansson, SM Smith; citation_volume=12; citation_issue=1; citation_publication_date=1991; citation_pages=3-11; citation_id=CR55\"/>", "    <meta name=\"citation_reference\" content=\"Jia J, Huang J, Shen GY, et al., 2016. Learning to appreciate the aesthetic effects of clothing. Proc 30th AAAI Conf on Artificial Intelligence, p.1216&#8211;1222.\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Des Creat Innov; citation_title=Testing ideation performance on a large set of designers: effects of analogical distance; citation_author=L Jia, N Becattini, G Cascini; citation_volume=8; citation_issue=1; citation_publication_date=2020; citation_pages=31-45; citation_id=CR57\"/>", "    <meta name=\"citation_reference\" content=\"Jiang SH, Fu Y, 2017. Fashion style generator. Proc 26th Int Joint Conf on Artificial Intelligence, p.3721&#8211;3727. ", "https://doi.org/10.24963/ijcai.2017/520", "\"/>", "    <meta name=\"citation_reference\" content=\"Jing YC, Yang YZ, Feng ZL, et al., 2019. Neural style transfer: a review. IEEE Trans Vis Comput Graph, in press. ", "https://doi.org/10.1109/tvcg.2019.2921336", "\"/>", "    <meta name=\"citation_reference\" content=\"Jo Y, Park J, 2019. SC-FEGAN: face editing generative adversarial network with user&#8217;s sketch and color. ", "https://doi.org/1902.06838", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Perceptual Losses for Real-Time Style Transfer and Super-Resolution; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=694-711; citation_id=CR61; citation_author=Justin Johnson; citation_author=Alexandre Alahi; citation_author=Li Fei-Fei; citation_publisher=Springer International Publishing\"/>", "    <meta name=\"citation_reference\" content=\"Karras T, Laine S, Aila T, 2019. A style-based generator architecture for generative adversarial networks. The IEEE Conf on Computer Vision and Pattern Recognition, p.4401&#8211;4410.\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=The International Handbook of Creativity; citation_publication_date=2006; citation_id=CR63; citation_author=JC Kaufman; citation_author=RJ Sternberg; citation_publisher=Edward Elgar Publishing\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Acoust Speech Signal Process; citation_title=Cubic convolution interpolation for digital image processing; citation_author=R Keys; citation_volume=29; citation_issue=6; citation_publication_date=1981; citation_pages=1153-1160; citation_id=CR64\"/>", "    <meta name=\"citation_reference\" content=\"Kim J, Lee JK, Lee KM, 2016. Accurate image superresolution using very deep convolutional networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1646&#8211;1654. ", "https://doi.org/10.1109/CVPR.2016.182", "\"/>", "    <meta name=\"citation_reference\" content=\"Kingma DP, Welling M, 2013. Auto-encoding variational Bayes. https://arxiv.org/abs/1312.6114\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Photo Aesthetics Ranking Network with Attributes and Content Adaptation; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=662-679; citation_id=CR67; citation_author=Shu Kong; citation_author=Xiaohui Shen; citation_author=Zhe Lin; citation_author=Radomir Mech; citation_author=Charless Fowlkes; citation_publisher=Springer International Publishing\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Learning Multiple Layers of Features from Tiny Images; citation_publication_date=2009; citation_id=CR68; citation_author=A Krizhevsky; citation_author=G Hinton; citation_publisher=University of Toronto\"/>", "    <meta name=\"citation_reference\" content=\"Kwak H, An J, Jansen BJ, 2017. Automatic generation of personas using YouTube social media data. Proc 50th Hawaii Int Conf on System Sciences, p.833&#8211;842.\"/>", "    <meta name=\"citation_reference\" content=\"Larsen ABL, S&#248;nderby SK, Larochelle H, et al., 2016. Autoencoding beyond pixels using a learned similarity metric. Proc 33rd Int Conf on Machine Learning, p.1558&#8211;1566.\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Proc IEEE; citation_title=Gradient-based learning applied to document recognition; citation_author=Y LeCun, L Bottou, Y Bengio; citation_volume=86; citation_issue=11; citation_publication_date=1998; citation_pages=2278-2323; citation_id=CR71\"/>", "    <meta name=\"citation_reference\" content=\"Ledig C, Theis L, Husz&#225;r F, et al., 2017. Photo-realistic single image super-resolution using a generative adversarial network. IEEE Conf on Computer Vision and Pattern Recognition, p.105&#8211;114. ", "https://doi.org/10.1109/CVPR.2017.19", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=702-716; citation_id=CR73; citation_author=Chuan Li; citation_author=Michael Wand; citation_publisher=Springer International Publishing\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE J Sel Top Signal Process; citation_title=Aesthetic visual quality assessment of paintings; citation_author=CC Li, T Chen; citation_volume=3; citation_issue=2; citation_publication_date=2009; citation_pages=236-252; citation_id=CR74\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=J Opt Soc Am A; citation_title=Polarization-dependent effects of an Airy beam due to the spin-orbit coupling; citation_author=HH Li, JG Wang, MM Tang; citation_volume=34; citation_issue=7; citation_publication_date=2017; citation_pages=1114-1118; citation_id=CR75\"/>", "    <meta name=\"citation_reference\" content=\"Li XT, Liu SF, Kautz J, et al., 2019. Learning linear transformations for fast arbitrary style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3809&#8211;3817.\"/>", "    <meta name=\"citation_reference\" content=\"Li YJ, Fang C, Yang JM, et al., 2017. Universal style transfer via feature transforms. Proc 31st Conf on Neural Information Processing Systems, p.386&#8211;396.\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Image Inpainting for Irregular Holes Using Partial Convolutions; citation_inbook_title=Computer Vision &#8211; ECCV 2018; citation_publication_date=2018; citation_pages=89-105; citation_id=CR78; citation_author=Guilin Liu; citation_author=Fitsum A. Reda; citation_author=Kevin J. Shih; citation_author=Ting-Chun Wang; citation_author=Andrew Tao; citation_author=Bryan Catanzaro; citation_publisher=Springer International Publishing\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=BT Technol; citation_title=ConceptNet&#8212;a practical commonsense reasoning tool-kit; citation_author=H Liu, P Singh; citation_volume=22; citation_issue=4; citation_publication_date=2004; citation_pages=211-226; citation_id=CR79\"/>", "    <meta name=\"citation_reference\" content=\"Liu MY, Huang X, Mallya A, et al., 2019. Few-shot unsupervised image-to-image translation. ", "https://doi.org/1905.01723", "\"/>", "    <meta name=\"citation_reference\" content=\"Liu ZW, Luo P, Wang XG, et al., 2015. Deep learning face attributes in the wild. Proc IEEE Int Conf on Computer Vision, p.3730&#8211;3738. ", "https://doi.org/10.1109/ICCV.2015.425", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=User-Centered Design: a Developer&#8217;s Guide to Building User-Friendly Applications; citation_publication_date=2013; citation_id=CR82; citation_author=T Lowdermilk; citation_publisher=O&#8217;Reilly\"/>", "    <meta name=\"citation_reference\" content=\"Lu X, Lin Z, Shen XH, et al., 2015. Deep multi-patch aggregation network for image style, aesthetics, and quality estimation. Proc IEEE Int Conf on Computer Vision, p.990&#8211;998. ", "https://doi.org/10.1109/ICCV.2015.119", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Photo and Video Quality Evaluation: Focusing on the Subject; citation_inbook_title=Lecture Notes in Computer Science; citation_publication_date=2008; citation_pages=386-399; citation_id=CR84; citation_author=Yiwen Luo; citation_author=Xiaoou Tang; citation_publisher=Springer Berlin Heidelberg\"/>", "    <meta name=\"citation_reference\" content=\"Ma S, Liu J, Chen WC, 2017. A-lamp: adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. Proc 30th IEEE Conf on Computer Vision and Pattern Recognition, p.722&#8211;731. ", "https://doi.org/10.1109/CVPR.2017.84", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=User requirements analysis; citation_inbook_title=Usability: Gaining a Competitive Edge; citation_publication_date=2002; citation_pages=133-148; citation_id=CR86; citation_author=M Maguire; citation_author=N Bevan; citation_publisher=Springer\"/>", "    <meta name=\"citation_reference\" content=\"Mai L, Jin HL, Liu F, 2016. Composition-preserving deep photo aesthetics assessment. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.497&#8211;506. ", "https://doi.org/10.1109/CVPR.2016.60", "\"/>", "    <meta name=\"citation_reference\" content=\"Matthews T, Judge T, Whittaker S, 2012. How do designers and user experience professionals actually perceive and use personas? Proc Conf on Human Factors in Computing Systems, p.1219&#8211;1228. ", "https://doi.org/10.1145/2207676.2208573", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Des Creat Innov; citation_title=The obscure features hypothesis in design innovation; citation_author=T McCaffrey, S Krishnamurty; citation_volume=3; citation_issue=1; citation_publication_date=2015; citation_pages=1-28; citation_id=CR89\"/>", "    <meta name=\"citation_reference\" content=\"McGinn J, Kotamraju N, 2008. Data-driven persona development. Proc Conf on Human Factors in Computing Systems, p.1521&#8211;1524. ", "https://doi.org/10.1145/1357054.1357292", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Personas and user-centered design: how can personas benefit product design processes?; citation_author=T Miaskiewicz, KA Kozar; citation_volume=32; citation_issue=5; citation_publication_date=2011; citation_pages=417-430; citation_id=CR91\"/>", "    <meta name=\"citation_reference\" content=\"Mikolov T, Chen K, Corrado G, et al., 2013. Efficient estimation of word representations in vector space. ", "https://doi.org/1301.3781", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Commun ACM; citation_title=Wordnet: a lexical database for English; citation_author=GA Miller; citation_volume=38; citation_issue=11; citation_publication_date=1995; citation_pages=39-41; citation_id=CR93\"/>", "    <meta name=\"citation_reference\" content=\"Mirza M, Osindero S, 2014. Conditional generative adversarial nets. ", "https://doi.org/1411.1784", "\"/>", "    <meta name=\"citation_reference\" content=\"Miyato T, Kataoka T, Koyama M, et al., 2018. Spectral normalization for generative adversarial networks. Int Conf on Learning Representations.\"/>", "    <meta name=\"citation_reference\" content=\"Murray N, Marchesotti L, Perronnin F, 2012. AVA: a large-scale database for aesthetic visual analysis. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2408&#8211;2415. ", "https://doi.org/10.1109/CVPR.2012.6247954", "\"/>", "    <meta name=\"citation_reference\" content=\"Nazeri K, Ng E, Joseph T, et al., 2019. Edgeconnect: generative image inpainting with adversarial edge learning. ", "https://doi.org/1901.00212", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Des Stud; citation_title=Refined metrics for measuring ideation effectiveness; citation_author=BA Nelson, JO Wilson, D Rosen; citation_volume=30; citation_issue=6; citation_publication_date=2009; citation_pages=737-743; citation_id=CR98\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Int J Sociotechnol Knowl Dev; citation_title=A template for design personas: analysis of 47 persona descriptions from Danish industries and organizations; citation_author=L Nielsen, KS Hansen, J Stage; citation_volume=7; citation_issue=1; citation_publication_date=2015; citation_pages=45-61; citation_id=CR99\"/>", "    <meta name=\"citation_reference\" content=\"Niles I, Pease A, 2001. Towards a standard upper ontology. Proc Int Conf on Formal Ontology in Information Systems, p.2&#8211;9. ", "https://doi.org/10.1145/505168.505170", "\"/>", "    <meta name=\"citation_reference\" content=\"Nilsback ME, Zisserman A, 2008. Automated flower classification over a large number of classes. Proc 6th Indian Conf on Computer Vision, Graphics &amp; Image Processing, p.722&#8211;729. ", "https://doi.org/10.1109/ICVGIP.2008.47", "\"/>", "    <meta name=\"citation_reference\" content=\"Odena A, Olah C, Shlens J, 2017. Conditional image synthesis with auxiliary classifier GANs. Proc 34th Int Conf on Machine Learning, p.4043&#8211;4055.\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Front Inform Technol Electron Eng; citation_title=Special issue on artificial intelligence 2.0; citation_author=YH Pan; citation_volume=18; citation_issue=1; citation_publication_date=2017; citation_pages=1-2; citation_id=CR103\"/>", "    <meta name=\"citation_reference\" content=\"Park T, Liu MY, Wang TC, et al., 2019. Semantic image synthesis with spatially-adaptive normalization. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2337&#8211;2346.\"/>", "    <meta name=\"citation_reference\" content=\"Pathak D, Kr&#228;henb&#252;hl P, Donahue J, et al., 2016. Context encoders: feature learning by inpainting. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2536&#8211;2544. ", "https://doi.org/10.1109/CVPR.2016.278", "\"/>", "    <meta name=\"citation_reference\" content=\"Peeters JR, Verhaegen PA, Vandevenne D, et al., 2010. Refined metrics for measuring novelty in ideation. ID-MME Virtual Concept Research in Interaction Design, Article 4.\"/>", "    <meta name=\"citation_reference\" content=\"Perera D, Zimmermann R, 2019. CNGAN: generative adversarial networks for cross-network user preference generation for non-overlapped users. World Wide Web Conf, p.3144&#8211;3150. ", "https://doi.org/10.1145/3308558.3313733", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=The Persona Lifecycle: Keeping People in Mind Throughout Product Design; citation_publication_date=2005; citation_id=CR108; citation_author=J Pruitt; citation_author=T Adlin; citation_publisher=Elsevier\"/>", "    <meta name=\"citation_reference\" content=\"Radford A, Metz L, Chintala S, 2016. Unsupervised representation learning with deep convolutional generative adversarial networks. Proc 4th Int Conf on Learning Representations.\"/>", "    <meta name=\"citation_reference\" content=\"Reed SE, Akata Z, Yan XC, et al., 2016a. Generative adversarial text to image synthesis. Proc 33rd Int Conf on Machine Learning, p.1681&#8211;1690.\"/>", "    <meta name=\"citation_reference\" content=\"Reed SE, Akata Z, Mohan S, et al., 2016b. Learning what and where to draw. Advances in Neural Information Processing Systems, p.217&#8211;225.\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Comput Graph Appl; citation_title=Informational aesthetics measures; citation_author=J Rigau, M Feixas, M Sbert; citation_volume=28; citation_issue=2; citation_publication_date=2008; citation_pages=24-34; citation_id=CR112\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Artificial Intelligence: a Modern Approach; citation_publication_date=2016; citation_id=CR113; citation_author=SJ Russell; citation_author=P Norvig; citation_publisher=Pearson Education Limited\"/>", "    <meta name=\"citation_reference\" content=\"Saleh B, Elgammal A, 2015. Large-scale classification of fine-art paintings: learning the right metric on the right feature. ", "https://doi.org/1505.00855", "\"/>", "    <meta name=\"citation_reference\" content=\"Salimans T, Goodfellow IJ, Zaremba W, et al., 2016. Improved techniques for training GANs. Advances in Neural Information Processing Systems, p.2226&#8211;2234.\"/>", "    <meta name=\"citation_reference\" content=\"Salminen J, Seng&#252;n S, Kwak H, et al., 2017. Generating cultural personas from social data: a perspective of middle eastern users. Proc 5th Int Conf on Future Internet of Things and Cloud Workshops, p.120&#8211;125. ", "https://doi.org/10.1109/FiCloudW.2017.97", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Persona Stud; citation_title=Are personas done? Evaluating their usefulness in the age of digital analytics; citation_author=J Salminen, BJ Jansen, J An; citation_volume=4; citation_issue=2; citation_publication_date=2018; citation_pages=47-65; citation_id=CR117\"/>", "    <meta name=\"citation_reference\" content=\"Salminen J, Jung SG, An J, et al., 2018b. Findings of a user study of automatically generated personas. Proc Conf on Human Factors in Computing Systems, p.LBW097. ", "https://doi.org/10.1145/3170427.3188470", "\"/>", "    <meta name=\"citation_reference\" content=\"Salminen J, Eng&#252;n S, Jung SG, et al., 2019. Design issues in automatically generated persona profiles: a qualitative analysis from 38 think-aloud transcripts. Proc Conf on Human Information Interaction and Retrieval, p.225&#8211;229. ", "https://doi.org/10.1145/3295750.3298942", "\"/>", "    <meta name=\"citation_reference\" content=\"Schwarz K, Wieschollek P, Lensch HPA, 2018. Will people like your image? Learning the aesthetic space. Proc IEEE Winter Conf on Applications of Computer Vision, p.2048&#8211;2057. ", "https://doi.org/10.1109/WACV.2018.00226", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=J Mech Des; citation_title=Evaluation of idea generation methods for conceptual design: effectiveness metrics and design of experiments; citation_author=JJ Shah, SV Kulkarni, N Vargas-Hernandez; citation_volume=122; citation_issue=4; citation_publication_date=2000; citation_pages=377-384; citation_id=CR121\"/>", "    <meta name=\"citation_reference\" content=\"Simonyan K, Zisserman A, 2014. Very deep convolutional networks for large-scale image recognition. ", "https://doi.org/1409.1556", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=brAInstorm: Intelligent Assistance in Group Idea Generation; citation_inbook_title=Lecture Notes in Computer Science; citation_publication_date=2017; citation_pages=457-461; citation_id=CR123; citation_author=Timo Strohmann; citation_author=Dominik Siemon; citation_author=Susanne Robra-Bissantz; citation_publisher=Springer International Publishing\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Non-photorealistic Computer Graphics: Modeling, Rendering, and Animation; citation_publication_date=2002; citation_id=CR124; citation_author=T Strothotte; citation_author=S Schlechtweg; citation_publisher=Morgan Kaufmann Publishers Inc.\"/>", "    <meta name=\"citation_reference\" content=\"Tang X, Wang ZW, Luo WX, et al., 2018. Face aging with identity-preserved conditional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.7939&#8211;7947. ", "https://doi.org/10.1109/CVPR.2018.00828", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Multim; citation_title=Content-based photo quality assessment; citation_author=XO Tang, W Luo, XG Wang; citation_volume=15; citation_issue=8; citation_publication_date=2013; citation_pages=1930-1943; citation_id=CR126\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Artif Intell Eng Des Anal Manuf; citation_title=A scalable approach for ideation in biologically inspired design; citation_author=D Vandevenne, PA Verhaegen, S Dewulf; citation_volume=29; citation_issue=1; citation_publication_date=2015; citation_pages=19-31; citation_id=CR127\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IBM J Res Dev; citation_title=A big data approach to computational creativity: the curious case of Chef Watson; citation_author=LR Varshney, F Pinel, KR Varshney; citation_volume=63; citation_issue=1; citation_publication_date=2019; citation_pages=7:1-7:18; citation_id=CR128\"/>", "    <meta name=\"citation_reference\" content=\"Verma P, Smith JO, 2018. Neural style transfer for audio spectograms. ", "https://doi.org/1801.01589", "\"/>", "    <meta name=\"citation_reference\" content=\"Wang J, Yu LT, Zhang WN, et al., 2017. IRGAN: a minimax game for unifying generative and discriminative information retrieval models. Proc 40th Int ACM SI-GIR Conf on Research and Development in Information Retrieval, p.515&#8211;524. ", "https://doi.org/10.1145/3077136.3080786", "\"/>", "    <meta name=\"citation_reference\" content=\"Wang TC, Liu MY, Zhu JY, et al., 2018. Video-to-video synthesis. ", "https://doi.org/1808.06601", "\"/>", "    <meta name=\"citation_reference\" content=\"Wang WG, Shen JB, 2017. Deep cropping via attention box prediction and aesthetics assessment. Proc IEEE Int Conf on Computer Vision, p.2205&#8211;2213. ", "https://doi.org/10.1109/ICCV.2017.240", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=Neurocomputing; citation_title=Synthesized computational aesthetic evaluation of photos; citation_author=WN Wang, D Cai, L Wang; citation_volume=172; citation_publication_date=2016; citation_pages=244-252; citation_id=CR133\"/>", "    <meta name=\"citation_reference\" content=\"Wang WS, Yang S, Zhang WS, et al., 2018. Neural aesthetic image reviewer. ", "https://doi.org/1802.10240", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks; citation_inbook_title=Lecture Notes in Computer Science; citation_publication_date=2019; citation_pages=63-79; citation_id=CR135; citation_author=Xintao Wang; citation_author=Ke Yu; citation_author=Shixiang Wu; citation_author=Jinjin Gu; citation_author=Yihao Liu; citation_author=Chao Dong; citation_author=Yu Qiao; citation_author=Chen Change Loy; citation_publisher=Springer International Publishing\"/>", "    <meta name=\"citation_reference\" content=\"Wu JJ, Zhang CK, Xue TF, et al., 2016. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. Advances in Neural Information Processing Systems, p.82&#8211;90.\"/>", "    <meta name=\"citation_reference\" content=\"Xu T, Zhang PC, Huang QY, et al., 2018. AttnGAN: fine-grained text to image generation with attentional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1316&#8211;1324. ", "https://doi.org/10.1109/CVPR.2018.00143", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Research on the development of contemporary design intelligence driven by neural network technology; citation_inbook_title=Design, User Experience, and Usability; citation_publication_date=2019; citation_pages=368-381; citation_id=CR138; citation_author=Y Yan; citation_author=JR Wang; citation_author=C Tang; citation_publisher=Springer\"/>", "    <meta name=\"citation_reference\" content=\"Yang HY, Huang D, Wang YH, et al., 2018. Learning face age progression: a pyramid architecture of GANs. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.31&#8211;39. ", "https://doi.org/10.1109/CVPR.2018.00011", "\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Multim; citation_title=Deep learning for single image super-resolution: a brief review; citation_author=WM Yang, XC Zhang, YP Tian; citation_volume=21; citation_issue=12; citation_publication_date=2019; citation_pages=3106-3121; citation_id=CR140\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Multim; citation_title=Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval; citation_author=Y Yang, YT Zhuang, F Wu; citation_volume=10; citation_issue=3; citation_publication_date=2008; citation_pages=437-446; citation_id=CR141\"/>", "    <meta name=\"citation_reference\" content=\"Yi ZL, Zhang H, Tan P, et al., 2017. DualGAN: unsupervised dual learning for image-to-image translation. Proc IEEE Int Conf on Computer Vision, p.2868&#8211;2876. ", "https://doi.org/10.1109/ICCV.2017.310", "\"/>", "    <meta name=\"citation_reference\" content=\"Yoon Y, Jeon HG, Yoo D, et al., 2015. Learning a deep convolutional network for light-field image super-resolution. Proc IEEE Int Conf on Computer Vision, p.57&#8211;65. https://doi.org/10.1109/ICCVW.2015.17\"/>", "    <meta name=\"citation_reference\" content=\"You S, You N, Pan MX, 2019. PI-REC: progressive image reconstruction network with edge and color domain. ", "https://doi.org/1903.10146", "\"/>", "    <meta name=\"citation_reference\" content=\"Yu F, Zhang YD, Song SR, et al., 2015. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. ", "https://doi.org/1506.03365", "\"/>", "    <meta name=\"citation_reference\" content=\"Yu JH, Lin Z, Yang JM, et al., 2018a. Free-form image inpainting with gated convolution. ", "https://doi.org/1806.03589", "\"/>", "    <meta name=\"citation_reference\" content=\"Yu JH, Lin Z, Yang JM, et al., 2018b. Generative image inpainting with contextual attention. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.5505&#8211;5514. ", "https://doi.org/10.1109/CVPR.2018.00577", "\"/>", "    <meta name=\"citation_reference\" content=\"Zakharov E, Shysheya A, Burkov E, et al., 2019. Fewshot adversarial learning of realistic neural talking head models. ", "https://doi.org/1905.08233", "\"/>", "    <meta name=\"citation_reference\" content=\"Zeiler MD, Taylor GW, Fergus R, 2011. Adaptive deconvolutional networks for mid and high level feature learning. Proc IEEE Int Conf on Computer Vision, p.2018&#8211;2025. ", "https://doi.org/10.1109/ICCV.2011.6126474", "\"/>", "    <meta name=\"citation_reference\" content=\"Zhang H, Xu T, Li H, et al., 2017. StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks. Proc IEEE Int Conf on Computer Vision, p.5907&#8211;5915.\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Patt Anal Mach Intell; citation_title=StackGAN++: realistic image synthesis with stacked generative adversarial networks; citation_author=H Zhang, T Xu, H Li; citation_volume=41; citation_issue=8; citation_publication_date=2019; citation_pages=1947-1962; citation_id=CR151\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=ACM Transactions on Applied Perception; citation_title=Computational Aesthetic Evaluation of Logos; citation_author=Jiajing Zhang, Jinhui Yu, Kang Zhang, Xianjun Sam Zheng, Junsong Zhang; citation_volume=14; citation_issue=3; citation_publication_date=2017; citation_pages=1-21; citation_id=CR152\"/>", "    <meta name=\"citation_reference\" content=\"citation_title=Colorful Image Colorization; citation_inbook_title=Computer Vision &#8211; ECCV 2016; citation_publication_date=2016; citation_pages=649-666; citation_id=CR153; citation_author=Richard Zhang; citation_author=Phillip Isola; citation_author=Alexei A. Efros; citation_publisher=Springer International Publishing\"/>", "    <meta name=\"citation_reference\" content=\"citation_journal_title=IEEE Trans Comput Imag; citation_title=Loss functions for image restoration with neural networks; citation_author=H Zhao, O Gallo, I Frosio; citation_volume=3; citation_issue=1; citation_publication_date=2016; citation_pages=47-57; citation_id=CR154\"/>", "    <meta name=\"citation_reference\" content=\"Zhu JY, Park T, Isola P, et al., 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. Proc IEEE Int Conf on Computer Vision, p.2242&#8211;2251. ", "https://doi.org/10.1109/ICCV.2017.244", "\"/>", "    <meta name=\"citation_author\" content=\"Tang, Yong-chuan\"/>", "    <meta name=\"citation_author_email\" content=\"yctang@zju.edu.cn\"/>", "    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>", "    <meta name=\"citation_author\" content=\"Huang, Jiang-jie\"/>", "    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>", "    <meta name=\"citation_author\" content=\"Yao, Meng-ting\"/>", "    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>", "    <meta name=\"citation_author\" content=\"Wei, Jia\"/>", "    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>", "    <meta name=\"citation_author\" content=\"Li, Wei\"/>", "    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>", "    <meta name=\"citation_author\" content=\"He, Yong-xing\"/>", "    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>", "    <meta name=\"citation_author\" content=\"Li, Ze-jian\"/>", "    <meta name=\"citation_author_institution\" content=\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Zhejiang Lab, Hangzhou, China\"/>", "    <meta name=\"citation_author_institution\" content=\"Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China\"/>", "    <meta name=\"citation_springer_api_url\" content=\"http://api.springer.com/metadata/pam?q=doi:10.1631/FITEE.1900398&amp;api_key=\"/>", "    <meta name=\"format-detection\" content=\"telephone=no\"/>", "    <meta name=\"citation_cover_date\" content=\"2019/12/01\"/>", "    ", "        <meta property=\"og:url\" content=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>", "        <meta property=\"og:type\" content=\"article\"/>", "        <meta property=\"og:site_name\" content=\"Frontiers of Information Technology &amp; Electronic Engineering\"/>", "        <meta property=\"og:title\" content=\"A review of design intelligence: progress, problems, and challenges - Frontiers of Information Technology &amp; Electronic Engineering\"/>", "        <meta property=\"og:description\" content=\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.\"/>", "        <meta property=\"og:image\" content=\"https://media.springernature.com/w200/springer-static/cover/journal/11714.jpg\"/>", "    ", "    <title>A review of design intelligence: progress, problems, and challenges | SpringerLink</title>", "    <link rel=\"shortcut icon\" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />", "<link rel=\"icon\" sizes=\"16x16 32x32 48x48\" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />", "<link rel=\"icon\" sizes=\"16x16\" type=\"image/png\" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />", "<link rel=\"icon\" sizes=\"32x32\" type=\"image/png\" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />", "<link rel=\"icon\" sizes=\"48x48\" type=\"image/png\" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />", "<link rel=\"apple-touch-icon\" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />", "<link rel=\"apple-touch-icon\" sizes=\"72x72\" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />", "<link rel=\"apple-touch-icon\" sizes=\"76x76\" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />", "<link rel=\"apple-touch-icon\" sizes=\"114x114\" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />", "<link rel=\"apple-touch-icon\" sizes=\"120x120\" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />", "<link rel=\"apple-touch-icon\" sizes=\"144x144\" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />", "<link rel=\"apple-touch-icon\" sizes=\"152x152\" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />", "<link rel=\"apple-touch-icon\" sizes=\"180x180\" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />", "    ", "    <script>(function(H){H.className=H.className.replace(/\\bno-js\\b/,'js')})(document.documentElement)</script>", "    ", "        <style>@media only screen and (-webkit-min-device-pixel-ratio: 0) and (min-color-index: 0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio: 0) and (min-resolution: 3 e1dpcm) { a{text-decoration:underline;text-decoration-skip-ink:auto}html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:62.5%;height:100%;line-height:1.61803;overflow-y:scroll}body{background:#fcfcfc;line-height:1.5;max-width:100%;min-height:100%}article,aside,figure,header,main,nav{display:block}h1{font-size:30px;margin:.67em 0}figure{margin:0}a{background-color:transparent;color:#004b83}img{border:0;height:auto;max-width:100%;vertical-align:middle}svg:not(:root){overflow:hidden}button,input{font-family:sans-serif;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=submit],button,html [type=button]{-webkit-appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}h3{font-size:17px}h1{font-family:Georgia,Palatino,serif;font-style:normal}.u-h3,h1,h3{margin-bottom:1em}.u-h3{line-height:1.4}.u-h3,h3{font-family:Georgia,Palatino,serif;font-style:normal}button:focus{outline:4px solid #fc0}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input{margin:0;padding:0}p{padding:0}a>img{vertical-align:middle}h1,h3{line-height:1.4}p{margin:0}h1,h3,ul{margin-top:0}p{margin-bottom:1.5em}p:empty{display:none}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad--300x250{background-color:#f2f2f2;display:none;padding:8px}.c-ad--300x250 .c-ad__inner{min-height:calc(1.5em + 254px)}@media only screen and (min-width:320px){.js .c-ad--300x250{display:block}}.c-ad__label,.c-skip-link{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.4rem}.c-ad__label{color:#333;font-weight:400;line-height:1.5;margin-bottom:4px}.c-skip-link{background:#f7fbfe;bottom:auto;color:#004b83;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-header{background-color:#fff;border-bottom:4px solid #00285a;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;padding:16px 0}.c-header__container{-webkit-box-align:center;-webkit-box-pack:justify;align-items:center;display:flex;justify-content:space-between;margin:0 auto;max-width:1280px;padding:0 16px}.c-header__brand{margin-right:32px}.c-header__brand a{text-decoration:none}.c-header__menu,.c-header__navigation{display:flex}.c-header__navigation{-webkit-box-align:center;align-items:center}.c-header__menu{list-style:none;margin:0;padding:0}.c-header__item{color:inherit;margin-right:24px}.c-header__item:last-child{margin-right:0}.c-header__link{color:inherit;text-decoration:none}h1{font-size:3.2rem}.u-h3,h1,h3{font-weight:700}.u-h3,h3{font-size:2.8rem;font-size:2.4rem}body{font-size:1.8em}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article__sub-heading{color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:2rem;font-style:normal;font-weight:400;line-height:1.3;margin:0 0 8px}@media only screen and (min-width:768px){.c-article__sub-heading{font-size:2.4rem;line-height:1.24}}@media only screen and (min-width:1024px){.c-pdf-button__container{display:none}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-pdf-download{display:flex;margin-bottom:24px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{-webkit-box-pack:justify;-webkit-box-flex:1;background:linear-gradient(#4d78af,#3365a0);border:1px solid transparent;border-radius:2px;color:#fff;display:flex;flex:1 1 0%;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.6rem;justify-content:space-between;line-height:1.3;padding:13px 24px;text-decoration:none}.c-popup-search{background-color:#eee;box-shadow:0 3px 3px -3px rgba(0,0,0,.21);padding:16px 0;position:relative;z-index:10}@media only screen and (min-width:1024px){.js .c-popup-search{position:absolute;top:100%;width:100%}.c-popup-search__container{margin:auto;max-width:70%}}.app-search__content{display:flex}.app-search__label{color:#666;display:inline-block;font-size:1.4rem;margin-bottom:8px}.app-search__input{-webkit-box-flex:0;border:1px solid #b3b3b3;border-bottom-left-radius:3px;border-top-left-radius:3px;box-shadow:inset 0 1px 3px 0 rgba(0,0,0,.21);flex:0 1 auto;font-size:1.4rem;line-height:1.2;padding:.75em 1em;vertical-align:middle;width:100%}.app-search__button{-webkit-box-align:center;-webkit-box-pack:center;align-items:center;background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);border-radius:0 2px 2px 0;color:#fff;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:50px}.app-search__button svg{fill:currentcolor}.u-display-block{display:block}.u-display-flex{display:flex;width:100%}.u-align-items-center{-webkit-box-align:center;align-items:center}.u-flex-static{-webkit-box-flex:0;flex:0 1 auto;flex:0 0 auto}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-button-reset{background-color:transparent;border:0;padding:0}.u-sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-position-relative{position:relative}.u-mt-16{margin-top:16px}.u-mt-32{margin-top:32px}.u-mr-24{margin-right:24px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-ml-8{margin-left:8px}.u-hide{display:none;visibility:hidden}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-float-left{float:left}.u-text-sm{font-size:16px}.hide{visibility:hidden}.c-article-main-column .c-pdf-button__container .c-pdf-download,.hide{display:none}@media only screen and (max-width:1023px){.c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}} }</style>", "    ", "    ", "        <link rel=\"stylesheet\" data-inline-css-source=\"critical-css\" href=\"/oscar-static/app-springerlink/css/enhanced-article-75b1d207c3.css\" media=\"print\" onload=\"this.media='only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)';this.onload=null\">", "        ", "    ", "    ", "    <script type=\"text/javascript\">", "        window.dataLayer = [{\"GA Key\":\"UA-26408784-1\",\"DOI\":\"10.1631/FITEE.1900398\",\"Page\":\"article\",\"page\":{\"attributes\":{\"environment\":\"live\"}},\"Country\":\"BR\",\"doi\":\"10.1631-FITEE.1900398\",\"Journal Title\":\"Frontiers of Information Technology \\u0026 Electronic Engineering\",\"Journal Id\":11714,\"Keywords\":\"Design intelligence, Creativity, Personas, Ideation, AI-generated content, Computational aesthetics, TP183\",\"kwrd\":[\"Design_intelligence\",\"Creativity\",\"Personas\",\"Ideation\",\"AI-generated_content\",\"Computational_aesthetics\",\"TP183\"],\"Labs\":\"Y\",\"ksg\":\"Krux.segments\",\"kuid\":\"Krux.uid\",\"Has Body\":\"N\",\"Features\":[],\"Open Access\":\"N\",\"hasAccess\":\"N\",\"bypassPaywall\":\"N\",\"user\":{\"license\":{\"businessPartnerID\":[],\"businessPartnerIDString\":\"\"}},\"Access Type\":\"no-access\",\"Bpids\":\"\",\"Bpnames\":\"\",\"BPID\":[\"1\"],\"VG Wort Identifier\":\"pw-vgzm.415900-10.1631-FITEE.1900398\",\"Full HTML\":\"N\",\"Subject Codes\":[\"SCI\",\"SCI00001\",\"SCT24000\",\"SCI1200X\",\"SCI13006\",\"SCT24027\",\"SCT24035\"],\"pmc\":[\"I\",\"I00001\",\"T24000\",\"I1200X\",\"I13006\",\"T24027\",\"T24035\"],\"session\":{\"authentication\":{\"loginStatus\":\"N\"},\"attributes\":{\"edition\":\"academic\"}},\"content\":{\"serial\":{\"eissn\":\"2095-9230\",\"pissn\":\"2095-9184\"},\"type\":\"Article\",\"category\":{\"pmc\":{\"primarySubject\":\"Computer Science\",\"primarySubjectCode\":\"I\",\"secondarySubjects\":{\"1\":\"Computer Science, general\",\"2\":\"Electrical Engineering\",\"3\":\"Computer Hardware\",\"4\":\"Computer Systems Organization and Communication Networks\",\"5\":\"Electronics and Microelectronics, Instrumentation\",\"6\":\"Communications Engineering, Networks\"},\"secondarySubjectCodes\":{\"1\":\"I00001\",\"2\":\"T24000\",\"3\":\"I1200X\",\"4\":\"I13006\",\"5\":\"T24027\",\"6\":\"T24035\"}},\"sucode\":\"SC6\"},\"attributes\":{\"deliveryPlatform\":\"oscar\"}},\"Event Category\":\"Article\"}];", "    </script>", "    ", "    ", "        ", "    ", "        ", "            <script src=/oscar-static/js/jquery-220afd743d.js></script>", "        ", "    ", "    <script data-test=\"onetrust-control\">", "        ", "            (function(w,d,t) {", "                var assetPath = '/oscar-static/js/cookie-consent-es5-bundle-c11e114a73.js';", "                function cc() {", "                    var h = w.location.hostname,", "                        e = d.createElement(t),", "                        s = d.getElementsByTagName(t)[0];", "                    if (h === \"link.springer.com\") {", "                        e.src = \"https://cdn.cookielaw.org/scripttemplates/otSDKStub.js\";", "                        e.setAttribute(\"data-domain-script\", \"4f53bc14-4ee3-45bd-9935-e3d2b6b2a543\");", "                    } else {", "                        e.src = assetPath;", "                        e.setAttribute(\"data-consent\", h);", "                    }", "                    s.parentNode.insertBefore(e, s);", "                }", "                w.google_tag_manager ? cc() : window.addEventListener(\"gtm_loaded\", cc);", "            })(window,document,\"script\");", "        ", "    </script>", "    <script>", "        function OptanonWrapper() {", "            var elementInside = function(candidate, element) {", "                if (candidate === element) {", "                    return true;", "                } else if (candidate.nodeName.toLowerCase() === 'body') {", "                    return false;", "                } else {", "                    return elementInside(candidate.parentNode, element);", "                }", "            };", "            var disclaimer = document.querySelector('.c-disclaimer[aria-hidden=\"false\"]');", "            window.dataLayer.push({event:'OneTrustGroupsUpdated'});", "            if (disclaimer) {", "                if (!elementInside(document.activeElement, disclaimer)) {", "                    disclaimer.querySelector('button').focus();", "                }", "            } else {", "                document.activeElement.blur();", "            }", "        }", "    </script>", "    ", "    ", "    <script>", "        (function(w, d) {", "            w.config = w.config || {};", "            w.config.mustardcut = false;", "            ", "            if (w.matchMedia && w.matchMedia('only screen and (-webkit-min-device-pixel-ratio:0) and (min-color-index:0), (-ms-high-contrast: none), only all and (min--moz-device-pixel-ratio:0) and (min-resolution: 3e1dpcm)').matches) {", "                w.config.mustardcut = true;", "                d.classList.add('js');", "                d.classList.remove('grade-c');", "            }", "        })(window, document.documentElement);", "    </script>", "    ", "<script>", "    (function () {", "        if ( typeof window.CustomEvent === \"function\" ) return false;", "        function CustomEvent ( event, params ) {", "            params = params || { bubbles: false, cancelable: false, detail: null };", "            var evt = document.createEvent( 'CustomEvent' );", "            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );", "            return evt;", "        }", "        CustomEvent.prototype = window.Event.prototype;", "        window.CustomEvent = CustomEvent;", "    })();", "</script>", "    ", "        ", "    ", "        ", "            <!-- Google Tag Manager -->", "            <script data-test=\"gtm-head\">", "                if (window.config.mustardcut) {", "                    (function (w, d, s, l, i) {", "                        w[l] = w[l] || [];", "                        w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});", "                        var f = d.getElementsByTagName(s)[0],", "                                j = d.createElement(s),", "                                dl = l != 'dataLayer' ? '&l=' + l : '';", "                        j.async = true;", "                        j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;", "                        ", "                        j.addEventListener('load', function() {", "                            var _ge = new CustomEvent('gtm_loaded', { bubbles: true });", "                            d.dispatchEvent(_ge);", "                        });", "                        f.parentNode.insertBefore(j, f);", "                    })(window, document, 'script', 'dataLayer', 'GTM-WCF9Z9');", "                }", "            </script>", "            <!-- End Google Tag Manager -->", "        ", "    ", "    ", "    ", "    <script class=\"js-entry\">", "        if (window.config.mustardcut) {", "            (function(w, d) {", "                ", "                ", "                ", "                    window.Component = {};", "                    window.suppressShareButton = false;", "                ", "                var currentScript = d.currentScript || d.head.querySelector('script.js-entry');", "                ", "                function catchNoModuleSupport() {", "                    var scriptEl = d.createElement('script');", "                    return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)", "                }", "                var headScripts = [", "                    {'src': '/oscar-static/js/polyfill-es5-bundle-0cfdbfc67a.js', 'async': false},", "                    {'src': '/oscar-static/js/airbrake-es5-bundle-d227f11622.js', 'async': false},", "                ];", "                var bodyScripts = [", "                    {'src': '/oscar-static/js/app-es5-bundle-8c7ca818ff.js', 'async': false, 'module': false},", "                    {'src': '/oscar-static/js/app-es6-bundle-14ad2bbad1.js', 'async': false, 'module': true}", "                    ", "                    ", "                        , {'src': '/oscar-static/js/global-article-es5-bundle-4af70904c0.js', 'async': false, 'module': false},", "                        {'src': '/oscar-static/js/global-article-es6-bundle-ff57660707.js', 'async': false, 'module': true}", "                    ", "                ];", "                function createScript(script) {", "                    var scriptEl = d.createElement('script');", "                    scriptEl.src = script.src;", "                    scriptEl.async = script.async;", "                    if (script.module === true) {", "                        scriptEl.type = \"module\";", "                        if (catchNoModuleSupport()) {", "                            scriptEl.src = '';", "                        }", "                    } else if (script.module === false) {", "                        scriptEl.setAttribute('nomodule', true)", "                    }", "                    if (script.charset) {", "                        scriptEl.setAttribute('charset', script.charset);", "                    }", "                    return scriptEl;", "                }", "                for (var i = 0; i < headScripts.length; ++i) {", "                    var scriptEl = createScript(headScripts[i]);", "                    currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);", "                }", "                d.addEventListener('DOMContentLoaded', function() {", "                    for (var i = 0; i < bodyScripts.length; ++i) {", "                        var scriptEl = createScript(bodyScripts[i]);", "                        d.body.appendChild(scriptEl);", "                    }", "                });", "                // Webfont repeat view", "                var config = w.config;", "                if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {", "                    d.documentElement.className += ' webfonts-loaded';", "                }", "            })(window, document);", "        }", "    </script>", "    ", "    ", "    <link rel=\"canonical\" href=\"https://link.springer.com/article/10.1631/FITEE.1900398\"/>", "    ", "    ", "    <script type=\"application/ld+json\">{\"mainEntity\":{\"headline\":\"A review of design intelligence: progress, problems, and challenges\",\"description\":\"Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.\",\"datePublished\":\"2020-02-13\",\"dateModified\":\"2020-02-13\",\"pageStart\":\"1595\",\"pageEnd\":\"1617\",\"sameAs\":\"https://doi.org/10.1631/FITEE.1900398\",\"keywords\":\"Computer Science,general,Electrical Engineering,Computer Hardware,Computer Systems Organization and Communication Networks,Electronics and Microelectronics,Instrumentation,Communications Engineering,Networks\",\"image\":\"\",\"isPartOf\":{\"name\":\"Frontiers of Information Technology & Electronic Engineering\",\"issn\":[\"2095-9230\",\"2095-9184\"],\"volumeNumber\":\"20\",\"@type\":[\"Periodical\",\"PublicationVolume\"]},\"publisher\":{\"name\":\"Zhejiang University Press\",\"logo\":{\"url\":\"https://www.springernature.com/app-sn/public/images/logo-springernature.png\",\"@type\":\"ImageObject\"},\"@type\":\"Organization\"},\"author\":[{\"name\":\"Tang, Yong-chuan\",\"url\":\"http://orcid.org/0000-0002-0157-7771\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Huang, Jiang-jie\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Yao, Meng-ting\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Wei, Jia\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Li, Wei\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"He, Yong-xing\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province\",\"address\":{\"name\":\"Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"},{\"name\":\"Li, Ze-jian\",\"affiliation\":[{\"name\":\"Zhejiang University\",\"address\":{\"name\":\"College of Computer Science and Technology, Zhejiang University, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Zhejiang Lab\",\"address\":{\"name\":\"Zhejiang Lab, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"},{\"name\":\"Alibaba-Zhejiang University Joint Institute of Frontier Technologies\",\"address\":{\"name\":\"Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, China\",\"@type\":\"PostalAddress\"},\"@type\":\"Organization\"}],\"@type\":\"Person\"}],\"@type\":\"ScholarlyArticle\"},\"@context\":\"https://schema.org\",\"@type\":\"WebPage\"}</script>", "</head>", "<body class=\"shared-article-renderer\">", "    ", "    ", "    ", "        ", "            <!-- Google Tag Manager (noscript) -->", "            <noscript data-test=\"gtm-body\">", "                <iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-WCF9Z9\"", "                height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe>", "            </noscript>", "            <!-- End Google Tag Manager (noscript) -->", "        ", "    ", "    <div class=\"u-vh-full\">", "        <a class=\"c-skip-link\" href=\"#main-content\">Skip to main content</a>", "        ", "        <div class=\"u-hide u-show-following-ad\"></div>", "        <aside class=\"c-ad c-ad--728x90\" data-test=\"springer-doubleclick-ad\">", "            <div class=\"c-ad__inner\">", "                <p class=\"c-ad__label\">Advertisement</p>", "                    <div id=\"div-gpt-ad-LB1\" data-pa11y-ignore data-gpt data-gpt-unitpath=\"/270604982/springerlink/11714/article\" data-gpt-sizes=\"728x90\" style=\"min-width:728px;min-height:90px\" data-gpt-targeting=\"pos=LB1;articleid=FITEE.1900398;\"></div>", "            </div>", "        </aside>", "<div class=\"u-position-relative\">", "    <header class=\"c-header u-mb-24\" data-test=\"publisher-header\">", "        <div class=\"c-header__container\">", "            <div class=\"c-header__brand\">", "                ", "    <a id=\"logo\" class=\"u-display-block\" href=\"/\" title=\"Go to homepage\" data-test=\"springerlink-logo\">", "        <picture>", "            <source type=\"image/svg+xml\" srcset=/oscar-static/images/springerlink/svg/springerlink-6c9a864b59.svg>", "            <img src=/oscar-static/images/springerlink/png/springerlink-1db8a5b8b1.png alt=\"SpringerLink\" width=\"148\" height=\"30\" data-test=\"header-academic\">", "        </picture>", "        ", "        ", "    </a>", "            </div>", "            <div class=\"c-header__navigation\">", "                ", "    ", "        <button type=\"button\"", "                class=\"c-header__link u-button-reset u-mr-24\"", "                data-expander", "                data-expander-target=\"#popup-search\"", "                data-expander-autofocus=\"firstTabbable\"", "                data-test=\"header-search-button\">", "            <span class=\"u-display-flex u-align-items-center\">", "                <span class=\"u-text-sm\">Search</span>", "                <svg class=\"u-icon u-flex-static u-ml-8\" aria-hidden=\"true\" focusable=\"false\">", "                    <use xlink:href=\"#global-icon-search\"></use>", "                </svg>", "            </span>", "        </button>", "        <nav>", "            <ul class=\"c-header__menu\">", "                ", "                <li class=\"c-header__item\">", "                    <a", "                        data-test=\"login-link\"", "                        class=\"c-header__link\"", "                        href=\"//link.springer.com/signup-login?previousUrl=https%3A%2F%2Flink.springer.com%2Farticle%2F10.1631%2FFITEE.1900398\"", "                        data-track=\"click\"", "                        data-track-category=\"header\"", "                        data-track-action=\"login header\"", "                        data-track-label=\"link\">Log in</a>", "                </li>", "                ", "                ", "            </ul>", "        </nav>", "    ", "    ", "            </div>", "        </div>", "    </header>", "    ", "        <div id=\"popup-search\" class=\"c-popup-search u-mb-16 js-header-search u-js-hide\">", "            <div class=\"c-popup-search__content\">", "                <div class=\"u-container\">", "                    <div class=\"c-popup-search__container\" data-test=\"springerlink-popup-search\">", "                        <div class=\"app-search\">", "    <form role=\"search\" method=\"GET\" action=\"/search\" >", "        <label for=\"search\" class=\"app-search__label\">Search SpringerLink</label>", "        <div class=\"app-search__content\">", "            <input id=\"search\" class=\"app-search__input\" data-search-input autocomplete=\"off\" role=\"textbox\" name=\"query\" type=\"text\" value=\"\">", "            <button class=\"app-search__button\" type=\"submit\">", "                <span class=\"u-visually-hidden\">Search</span>", "                <svg class=\"u-icon\" aria-hidden=\"true\" focusable=\"false\">", "                    <use xlink:href=\"#global-icon-search\"></use>", "                </svg>", "            </button>", "            ", "                <input type=\"hidden\" name=\"searchType\" value=\"publisherSearch\">", "            ", "            ", "        </div>", "    </form>", "</div>", "                    </div>", "                </div>", "            </div>", "        </div>", "    ", "</div>", "        ", "    <div class=\"u-container u-mt-32 u-mb-32 u-clearfix\" id=\"main-content\" data-component=\"article-container\">", "        <main class=\"c-article-main-column u-float-left js-main-column\" data-track-component=\"article body\">", "            ", "            <div class=\"c-pdf-button__container\">", "                ", "            </div>", "            <div class=\"c-article-collection__container\">", "                ", "    ", "            </div>", "            <article lang=\"en\">", "                <div class=\"c-article-header\">", "                    <header>", "                        <ul class=\"c-article-identifiers\" data-test=\"article-identifier\">", "                            ", "    ", "        <li class=\"c-article-identifiers__item\" data-test=\"article-category\">Review</li>", "    ", "    ", "    ", "                            <li class=\"c-article-identifiers__item\"><a href=\"#article-info\" data-track=\"click\" data-track-action=\"publication date\" data-track-label=\"link\">Published: <time datetime=\"2020-02-13\">13 February 2020</time></a></li>", "                        </ul>", "                        ", "                        <h1 class=\"c-article-title\" data-test=\"article-title\" data-article-title=\"\">A review of design intelligence: progress, problems, and challenges</h1>", "                        <ul class=\"c-article-author-list js-etal-collapsed\" data-etal=\"25\" data-etal-small=\"3\" data-test=\"authors-list\" data-component-authors-activator=\"authors-list\"><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Yong_chuan-Tang\" data-author-popup=\"auth-Yong_chuan-Tang\" data-corresp-id=\"c1\">Yong-chuan Tang<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#global-icon-email\"></use></svg></a><span class=\"u-js-hide\">\u00a0", "            <a class=\"js-orcid\" href=\"http://orcid.org/0000-0002-0157-7771\"><span class=\"u-visually-hidden\">ORCID: </span>orcid.org/0000-0002-0157-7771</a></span><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a>,<a href=\"#Aff3\">3</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Jiang_jie-Huang\" data-author-popup=\"auth-Jiang_jie-Huang\">Jiang-jie Huang</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Meng_ting-Yao\" data-author-popup=\"auth-Meng_ting-Yao\">Meng-ting Yao</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Jia-Wei\" data-author-popup=\"auth-Jia-Wei\">Jia Wei</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Wei-Li\" data-author-popup=\"auth-Wei-Li\">Wei Li</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a>,<a href=\"#Aff3\">3</a></sup>, </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Yong_xing-He\" data-author-popup=\"auth-Yong_xing-He\">Yong-xing He</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff2\">2</a>,<a href=\"#Aff3\">3</a></sup> &amp; </li><li class=\"c-article-author-list__item\"><a data-test=\"author-name\" data-track=\"click\" data-track-action=\"open author\" data-track-label=\"link\" href=\"#auth-Ze_jian-Li\" data-author-popup=\"auth-Ze_jian-Li\">Ze-jian Li</a><sup class=\"u-js-hide\"><a href=\"#Aff1\">1</a>,<a href=\"#Aff3\">3</a>,<a href=\"#Aff4\">4</a></sup>\u00a0</li></ul>", "                        <p class=\"c-article-info-details\" data-container-section=\"info\">", "                            ", "    <a data-test=\"journal-link\" href=\"/journal/11714\"><i data-test=\"journal-title\">Frontiers of Information Technology &amp; Electronic Engineering</i></a>", "                            <b data-test=\"journal-volume\"><span class=\"u-visually-hidden\">volume</span>\u00a020</b>,\u00a0<span class=\"u-visually-hidden\">pages </span>1595\u20131617 (<span data-test=\"article-publication-year\">2019</span>)<a href=\"#citeas\" class=\"c-article-info-details__cite-as u-hide-print\" data-track=\"click\" data-track-action=\"cite this article\" data-track-label=\"link\">Cite this article</a>", "                        </p>", "                        ", "    ", "                        <div data-test=\"article-metrics\">", "                            <div id=\"altmetric-container\">", "    <div class=\"c-article-metrics-bar__wrapper u-clear-both\">", "        <ul class=\"c-article-metrics-bar u-list-reset\">", "            ", "                <li class=\" c-article-metrics-bar__item\">", "                    <p class=\"c-article-metrics-bar__count\">282 <span class=\"c-article-metrics-bar__label\">Accesses</span></p>", "                </li>", "            ", "            ", "                <li class=\"c-article-metrics-bar__item\">", "                    <p class=\"c-article-metrics-bar__count\">4 <span class=\"c-article-metrics-bar__label\">Citations</span></p>", "                </li>", "            ", "            ", "            <li class=\"c-article-metrics-bar__item\">", "                <p class=\"c-article-metrics-bar__details\"><a href=\"/article/10.1631%2FFITEE.1900398/metrics\" data-track=\"click\" data-track-action=\"view metrics\" data-track-label=\"link\" rel=\"nofollow\">Metrics <span class=\"u-visually-hidden\">details</span></a></p>", "            </li>", "        </ul>", "    </div>", "</div>", "                        </div>", "                        ", "    ", "    ", "                        ", "                    </header>", "                </div>", "                <div data-article-body=\"true\" data-track-component=\"article body\" class=\"c-article-body\">", "                    <section aria-labelledby=\"Abs1\" data-title=\"Abstract\" lang=\"en\"><div class=\"c-article-section\" id=\"Abs1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Abs1\">Abstract</h2><div class=\"c-article-section__content\" id=\"Abs1-content\"><p>Design intelligence is an important branch of artificial intelligence (AI), focusing on the intelligent models and algorithms in creativity and design. In the context of AI 2.0, studies on design intelligence have developed rapidly. We summarize mainly the current emerging framework of design intelligence and review the state-of-the-art techniques of related topics, including user needs analysis, ideation, content generation, and design evaluation. Specifically, the models and methods of intelligence-generated content are reviewed in detail. Finally, we discuss some open problems and challenges for future research in design intelligence.</p></div></div></section>", "                    ", "    ", "                    ", "                        ", "                            <div class=\"c-notes\">", "                                <p class=\"c-notes__text\">This is a preview of subscription content, <a id=\"test-login-banner-link\" href=\"//wayf.springernature.com?redirect_uri&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1631%2FFITEE.1900398\" data-track=\"click\" data-track-action=\"login\" data-track-label=\"link\">access via your institution</a>.</p>", "                            </div>", "                        ", "                        ", "                            <div class=\"c-article-buy-box c-article-buy-box--article\">", "                                <div class=\"sprcom-buybox-articleSidebar\" id=\"sprcom-buybox-articleSidebar\">", " <h2 class=\"c-box__heading\">Access options</h2>", " <article class=\"c-box\" data-test-id=\"buy-article\">", "  <h3 class=\"c-box__heading\">Buy single article</h3>", "  <div class=\"c-box__body\">", "   <div class=\"buybox__info\">", "    <p>Instant access to the full article PDF.</p>", "   </div>", "   <div class=\"buybox__buy\">", "    <p class=\"buybox__price\">USD 39.95</p>", "    <p class=\"buybox__price-info\">Price includes VAT (Brazil)<br>Tax calculation will be finalised during checkout.</p>", "    <form action=\"https://order.springer.com/public/checkout?abtest=v2\" method=\"post\">", "     <input type=\"hidden\" name=\"type\" value=\"article\">", "     <input type=\"hidden\" name=\"doi\" value=\"10.1631/FITEE.1900398\">", "     <input type=\"hidden\" name=\"isxn\" value=\"2095-9230\">", "     <input type=\"hidden\" name=\"contenttitle\" value=\"A review of design intelligence: progress, problems, and challenges\">", "     <input type=\"hidden\" name=\"copyrightyear\" value=\"2019\">", "     <input type=\"hidden\" name=\"year\" value=\"2020\">", "     <input type=\"hidden\" name=\"authors\" value=\"Yong-chuan Tang, et al.\">", "     <input type=\"hidden\" name=\"title\" value=\"Frontiers of Information Technology &amp; Electronic Engineering\">", "     <input type=\"hidden\" name=\"mac\" value=\"82FB53023BBB3109B3CB91533294E4D6\">", "     <input type=\"submit\" class=\"c-box__button\" onclick=\"dataLayer.push({&quot;event&quot;:&quot;addToCart&quot;,&quot;ecommerce&quot;:{&quot;currencyCode&quot;:&quot;USD&quot;,&quot;add&quot;:{&quot;products&quot;:[{&quot;name&quot;:&quot;A review of design intelligence: progress, problems, and challenges&quot;,&quot;id&quot;:&quot;2095-9230&quot;,&quot;price&quot;:39.95,&quot;brand&quot;:&quot;Zhejiang University Press&quot;,&quot;category&quot;:&quot;Computer Science&quot;,&quot;variant&quot;:&quot;ppv-article&quot;,&quot;quantity&quot;:1}]}}});\" value=\"Buy article PDF\">", "    </form>", "   </div>", "  </div>", "  <script>dataLayer.push({\"ecommerce\":{\"currency\":\"USD\",\"impressions\":[{\"name\":\"A review of design intelligence: progress, problems, and challenges\",\"id\":\"2095-9230\",\"price\":39.95,\"brand\":\"Zhejiang University Press\",\"category\":\"Computer Science\",\"variant\":\"ppv-article\",\"quantity\":1}]}});</script>", " </article>", " <article class=\"c-box buybox__rent-article\" id=\"deepdyve\" style=\"display: none\" data-test-id=\"journal-subscription\">", "  <div class=\"c-box__body\">", "   <div class=\"buybox__info\">", "    <p><a class=\"deepdyve-link\" target=\"deepdyve\" rel=\"nofollow\" data-track=\"click\" data-track-action=\"rent article\" data-track-label=\"rent action, new buybox\">Rent this article via DeepDyve.</a></p>", "   </div>", "  </div>", "  <script>", "            function deepDyveResponse(data) {", "                if (data.status === 'ok') {", "                    [].slice.call(document.querySelectorAll('.c-box.buybox__rent-article')).forEach(function (article) {", "                        article.style.display = 'flex'", "                        var link = article.querySelector('.deepdyve-link')", "                        if (link) {", "                          link.setAttribute('href', data.url)", "                        }", "                    })", "                }", "            }", "            var script = document.createElement('script')", "            script.src = '//www.deepdyve.com/rental-link?docId=10.1631/FITEE.1900398&journal=2095-9230&fieldName=journal_doi&affiliateId=springer&format=jsonp&callback=deepDyveResponse'", "            document.body.appendChild(script)", "          </script>", " </article>", " <aside class=\"buybox__institutional-sub\">", "  <div class=\"c-box__body\">", "   <div class=\"buybox__info\">", "    <p><a href=\"https://www.springernature.com/gp/librarians/licensing/license-options?&amp;abtest=v2\" data-track=\"click\" data-track-action=\"institutional link\" data-track-label=\"institutional subscriptions, new buybox\">Learn more about Institutional subscriptions</a></p>", "   </div>", "  </div>", " </aside>", " <style>.sprcom-buybox-articleSidebar{", "  box-shadow: 0px 0px 5px rgba(51,51,51,0.101);", "  display: flex;", "  flex-wrap: wrap;", "  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;", "  text-align: center;", "}", ".sprcom-buybox-articleSidebar *{", "  box-sizing: border-box;", "  line-height: calc(100% + 4px);", "  margin: 0px;", "}", ".sprcom-buybox-articleSidebar > *{", "  display: flex;", "  flex-basis: 240px;", "  flex-direction: column;", "  flex-grow: 1;", "  flex-shrink: 1;", "  margin: 0.5px;", "}", ".sprcom-buybox-articleSidebar > *{", "  box-shadow: 0 0 0 1px rgba(204,204,204,0.494);", "}", ".sprcom-buybox-articleSidebar .c-box__body{", "  display: flex;", "  flex-direction: column-reverse;", "  flex-grow: 1;", "  justify-content: space-between;", "  padding: 6%;", "}", ".sprcom-buybox-articleSidebar .c-box__body .buybox__buy{", "  display: flex;", "  flex-direction: column-reverse;", "}", ".sprcom-buybox-articleSidebar p{", "  color: #333;", "  font-size: 15px;", "}", ".sprcom-buybox-articleSidebar .buybox__price{", "  font-size: 24px;", "  font-weight: 500;", "  line-height: calc(100% + 8px);", "  margin: 20px 0;", "  order: 1;", "}", ".sprcom-buybox-articleSidebar form{", "  order: 1;", "}", ".sprcom-buybox-articleSidebar .buybox__price-info{", "  margin-bottom: 20px;", "}", ".sprcom-buybox-articleSidebar .c-box__heading{", "  background-color: #f0f0f0;", "  color: #333;", "  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;", "  font-size: 16px;", "  margin: 0px;", "  padding: 10px 12px;", "  text-align: center;", "}", ".sprcom-buybox-articleSidebar .c-box__button{", "  background-color: #3365A4;", "  border: 1px solid transparent;", "  border-radius: 2px;", "  color: #fff;", "  cursor: pointer;", "  display: inline-block;", "  font-family: inherit;", "  font-size: 16px;", "  max-width: 222px;", "  padding: 10px 12px;", "  text-decoration: none;", "  width: 100%;", "}", ".sprcom-buybox-articleSidebar h3{", "  clip: rect(1px, 1px, 1px, 1px);", "  height: 1px;", "  overflow: hidden;", "  position: absolute;", "  width: 1px;", "}", ".sprcom-buybox-articleSidebar h2{", "  flex-basis: 100%;", "  margin-bottom: 16px;", "  text-align: left;", "}", ".sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .c-box__body{", "  flex-direction: row;", "}", ".sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .buybox__info{", "  text-align: left;", "}", ".sprcom-buybox-articleSidebar .buybox__institutional-sub{", "  background-color: #f0f0f0;", "}", ".sprcom-buybox-articleSidebar .visually-hidden{", "  clip: rect(1px, 1px, 1px, 1px);", "  height: 1px;", "  overflow: hidden;", "  position: absolute;", "  width: 1px;", "}", ".sprcom-buybox-articleSidebar style{", "  display: none;", "}", "</style>", "</div>", "                            </div>", "                        ", "                        <div class=\"u-display-none\">", "                            ", "                        </div>", "                    ", "                    ", "                    ", "                    <div id=\"MagazineFulltextArticleBodySuffix\"><section aria-labelledby=\"Bib1\" data-title=\"References\"><div class=\"c-article-section\" id=\"Bib1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Bib1\">References</h2><div class=\"c-article-section__content\" id=\"Bib1-content\"><div data-container-section=\"references\"><ol class=\"c-article-references\"><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR1\">Arjovsky M, Chintala S, Bottou L, 2017. Wasserstein generative adversarial networks. Proc 34<sup>th</sup> Int Conf on Machine Learning, p.298\u2013321.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR2\">Aubry M, Maturana D, Efros AA, et al., 2014. Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3762\u20133769. <a href=\"https://doi.org/10.1109/CVPR.2014.487\">https://doi.org/10.1109/CVPR.2014.487</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR3\">Ballester C, Bertalmio M, Caselles V, et al., 2001. Filling-in by joint interpolation of vector fields and gray levels. <i>IEEE Trans Image Process</i>, 10(8):1200\u20131211. <a href=\"https://doi.org/10.1109/83.935036\">https://doi.org/10.1109/83.935036</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.ams.org/mathscinet-getitem?mr=1851781\" aria-label=\"MathSciNet reference 3\">MathSciNet</a>\u00a0", "    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?1037.68771\" aria-label=\"MATH reference 3\">MATH</a>\u00a0", "    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 3\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Filling-in%20by%20joint%20interpolation%20of%20vector%20fields%20and%20gray%20levels&amp;journal=IEEE%20Trans%20Image%20Process&amp;volume=10&amp;issue=8&amp;pages=1200-1211&amp;publication_year=2001&amp;author=Ballester%2CC&amp;author=Bertalmio%2CM&amp;author=Caselles%2CV\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR4\">Bertalmio M, Sapiro G, Caselles V, et al., 2000. Image in-painting. Proc 27<sup>th</sup> Annual Conf on Computer Graphics and Interactive Techniques, p.417\u2013424. <a href=\"https://doi.org/10.1145/344779.344972\">https://doi.org/10.1145/344779.344972</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR5\">Bharadhwaj H, Park H, Lim BY, 2018. RecGAN: recurrent generative adversarial networks for recommendation systems. Proc 12<sup>th</sup> ACM Conf on Recommender Systems, p.372\u2013376. <a href=\"https://doi.org/10.1145/3240323.3240383\">https://doi.org/10.1145/3240323.3240383</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR6\">Boden MA, 2009. Computer models of creativity. <i>AI Mag</i>, 30(3):23\u201334. <a href=\"https://doi.org/10.1609/aimag.v30i3.2254\">https://doi.org/10.1609/aimag.v30i3.2254</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 6\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Computer%20models%20of%20creativity&amp;journal=AI%20Mag&amp;volume=30&amp;issue=3&amp;pages=23-34&amp;publication_year=2009&amp;author=Boden%2CMA\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR7\">Brock A, Donahue J, Simonyan K, 2018. Large scale GAN training for high fidelity natural image synthesis. <a href=\"https://arxiv.org/abs/1809.11096\">https://doi.org/1809.11096</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR8\">Bruna J, Sprechmann P, LeCun Y, 2015. Super-resolution with deep convolutional sufficient statistics. <a href=\"https://arxiv.org/abs/1511.05666\">https://doi.org/1511.05666</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR9\">Chakrabarti A, Siddharth L, Dinakar M, et al., 2017. Idea inspire 3.0\u2014a tool for analogical design. In: Chakrabarti A, Chakrabarti D (Eds.), Research into Design for Communities. Springer, Singapore, p.475\u2013485. <a href=\"https://doi.org/10.1007/978-981-10-3521-0_41\">https://doi.org/10.1007/978-981-10-3521-0_41</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 9\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Idea%20inspire%203.0%E2%80%94a%20tool%20for%20analogical%20design&amp;pages=475-485&amp;publication_year=2017&amp;author=Chakrabarti%2CA&amp;author=Siddharth%2CL&amp;author=Dinakar%2CM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR10\">Champandard AJ, 2016. Semantic style transfer and turning two-bit doodles into fine artworks. <a href=\"https://arxiv.org/abs/1603.01768\">https://doi.org/1603.01768</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR11\">Chan C, Ginosar S, Zhou TH, et al., 2018. Everybody dance now. <a href=\"https://arxiv.org/abs/1808.07371\">https://doi.org/1808.07371</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR12\">Chen DD, Yuan L, Liao J, et al., 2018. Stereoscopic neural style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.6654\u20136663. <a href=\"https://doi.org/10.1109/CVPR.2018.00696\">https://doi.org/10.1109/CVPR.2018.00696</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR13\">Chen LQ, Wang P, Dong H, et al., 2019. An artificial intelligence based data-driven approach for design ideation. <i>J Vis Commun Image Represent</i>, 61:10\u201322. <a href=\"https://doi.org/10.1016/j.jvcir.2019.02.009\">https://doi.org/10.1016/j.jvcir.2019.02.009</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 13\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20artificial%20intelligence%20based%20data-driven%20approach%20for%20design%20ideation&amp;journal=J%20Vis%20Commun%20Image%20Represent&amp;volume=61&amp;pages=10-22&amp;publication_year=2019&amp;author=Chen%2CLQ&amp;author=Wang%2CP&amp;author=Dong%2CH\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR14\">Ciesielski V, Barile P, Trist K, 2013. Finding image features associated with high aesthetic value by machine learning. Proc 2<sup>nd</sup> Int Conf on Evolutionary and Biologically Inspired Music, Sound, Art and Design, p.47\u201358. <a href=\"https://doi.org/10.1007/978-3-642-36955-1_5\">https://doi.org/10.1007/978-3-642-36955-1_5</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 14\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Finding%20Image%20Features%20Associated%20with%20High%20Aesthetic%20Value%20by%20Machine%20Learning&amp;pages=47-58&amp;publication_year=2013&amp;author=Ciesielski%2CVic&amp;author=Barile%2CPerry&amp;author=Trist%2CKaren\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR15\">Cooper A, 1999. The Inmates Are Running the Asylum. SAMS, Indianapolis, USA.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 15\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20Inmates%20Are%20Running%20the%20Asylum&amp;publication_year=1999&amp;author=Cooper%2CA\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR16\">Cooper A, Reimann RM, 2003. About Face 2.0: the Essentials of Interaction Design. John Wiley &amp; Sons, Indianapolis, USA.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 16\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=About%20Face%202.0%3A%20the%20Essentials%20of%20Interaction%20Design&amp;publication_year=2003&amp;author=Cooper%2CA&amp;author=Reimann%2CRM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR17\">Dash A, Gamboa JCB, Ahmed S, et al., 2017. TAC-GAN-text conditioned auxiliary classifier generative adversarial network. <a href=\"https://arxiv.org/abs/1703.06412\">https://doi.org/1703.06412</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR18\">Datta R, Joshi D, Li J, et al., 2006. Studying aesthetics in photographic images using a computational approach. Proc 9<sup>th</sup> European Conf on Computer Vision, p.288\u2013301. <a href=\"https://doi.org/10.1007/11744078_23\">https://doi.org/10.1007/11744078_23</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 18\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Studying%20Aesthetics%20in%20Photographic%20Images%20Using%20a%20Computational%20Approach&amp;pages=288-301&amp;publication_year=2006&amp;author=Datta%2CRitendra&amp;author=Joshi%2CDhiraj&amp;author=Li%2CJia&amp;author=Wang%2CJames%20Z.\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR19\">de G\u00f3mez Silva Garza A, Maher ML, 1999. An evolutionary approach to case adaptation. Proc 3<sup>rd</sup> Int Conf on Case-Based Reasoning, p.162\u2013173. <a href=\"https://doi.org/10.1007/3-540-48508-2_12\">https://doi.org/10.1007/3-540-48508-2_12</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 19\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20Evolutionary%20Approach%20to%20Case%20Adaptation&amp;pages=162-173&amp;publication_year=1999&amp;author=de%20G%C3%B3mez%20Silva%20Garza%2CAndr%C3%A9s&amp;author=Maher%2CMary%20Lou\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR20\">de Silva Garza AG, 2019. An introduction to and comparison of computational creativity and design computing. <i>Artif Intell Rev</i>, 51(1):61\u201376. <a href=\"https://doi.org/10.1007/s10462-017-9557-3\">https://doi.org/10.1007/s10462-017-9557-3</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 20\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20introduction%20to%20and%20comparison%20of%20computational%20creativity%20and%20design%20computing&amp;journal=Artif%20Intell%20Rev&amp;volume=51&amp;issue=1&amp;pages=61-76&amp;publication_year=2019&amp;author=de%20Silva%20Garza%2CAG\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR21\">Deng J, Dong W, Socher R, et al., 2009. ImageNet: a large-scale hierarchical image database. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.248\u2013255. <a href=\"https://doi.org/10.1109/CVPR.2009.5206848\">https://doi.org/10.1109/CVPR.2009.5206848</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR22\">Deng YB, Loy CC, Tang XO, 2018. Aesthetic-driven image enhancement by adversarial learning. Proc 26<sup>th</sup> ACM Int Conf on Multimedia, p.870\u2013878. <a href=\"https://doi.org/10.1145/3240508.3240531\">https://doi.org/10.1145/3240508.3240531</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR23\">Donahue J, Kr\u00e4henb\u00fchl P, Darrell T, 2016. Adversarial feature learning. <a href=\"https://arxiv.org/abs/1605.09782\">https://doi.org/1605.09782</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR24\">Dou Q, Zheng XS, Sun TF, et al., 2019. Webthetics: quantifying webpage aesthetics with deep learning. <i>Int J Hum Comput Stud</i>, 124:56\u201366. <a href=\"https://doi.org/10.1016/j.ijhcs.2018.11.006\">https://doi.org/10.1016/j.ijhcs.2018.11.006</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 24\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Webthetics%3A%20quantifying%20webpage%20aesthetics%20with%20deep%20learning&amp;journal=Int%20J%20Hum%20Comput%20Stud&amp;volume=124&amp;pages=56-66&amp;publication_year=2019&amp;author=Dou%2CQ&amp;author=Zheng%2CXS&amp;author=Sun%2CTF\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR25\">Dugosh KL, Paulus PB, Roland EJ, et al., 2000. Cognitive stimulation in brainstorming. <i>J Pers Soc Psychol</i>, 79(5):722\u2013735. <a href=\"https://doi.org/10.1037/0022-3514.79.5.722\">https://doi.org/10.1037/0022-3514.79.5.722</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 25\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Cognitive%20stimulation%20in%20brainstorming&amp;journal=J%20Pers%20Soc%20Psychol&amp;volume=79&amp;issue=5&amp;pages=722-735&amp;publication_year=2000&amp;author=Dugosh%2CKL&amp;author=Paulus%2CPB&amp;author=Roland%2CEJ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR26\">Dumoulin V, Visin F, 2016. A guide to convolution arithmetic for deep learning. <a href=\"https://arxiv.org/abs/1603.07285\">https://doi.org/1603.07285</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR27\">Edelman RR, Hesselink JR, Zlatkin MB, 1996. MRI: Clinical Magnetic Resonance Imaging. Saunders, Philadelphia.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 27\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=MRI%3A%20Clinical%20Magnetic%20Resonance%20Imaging&amp;publication_year=1996&amp;author=Edelman%2CRR&amp;author=Hesselink%2CJR&amp;author=Zlatkin%2CMB\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR28\">Efros AA, Freeman WT, 2001. Image quilting for texture synthesis and transfer. Proc 28<sup>th</sup> Annual Conf on Computer Graphics and Interactive Techniques, p.341\u2013346. <a href=\"https://doi.org/10.1145/383259.383296\">https://doi.org/10.1145/383259.383296</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR29\">Elgammal A, Liu B, Elhoseiny M, et al., 2017. CAN: creative adversarial networks, generating \u201cart\u201d by learning about styles and deviating from style norms. <a href=\"https://arxiv.org/abs/1706.07068\">https://doi.org/1706.07068</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR30\">Fang H, Zhang M, 2017. Creatism: a deep-learning photographer capable of creating professional work. <a href=\"https://arxiv.org/abs/1707.03491\">https://doi.org/1707.03491</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR31\">Faste H, Rachmel N, Essary R, et al., 2013. Brainstorm, chainstorm, cheatstorm, tweetstorm: new ideation strategies for distributed HCI design. Proc Conf on Human Factors in Computing Systems, p.1343\u20131352. <a href=\"https://doi.org/10.1145/2470654.2466177\">https://doi.org/10.1145/2470654.2466177</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR32\">Fu K, Murphy J, Yang M, et al., 2015. Design-by-analogy: experimental evaluation of a functional analogy search methodology for concept generation improvement. <i>Res Eng Des</i>, 26(1):77\u201395. <a href=\"https://doi.org/10.1007/s00163-014-0186-4\">https://doi.org/10.1007/s00163-014-0186-4</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 32\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Design-by-analogy%3A%20experimental%20evaluation%20of%20a%20functional%20analogy%20search%20methodology%20for%20concept%20generation%20improvement&amp;journal=Res%20Eng%20Des&amp;volume=26&amp;issue=1&amp;pages=77-95&amp;publication_year=2015&amp;author=Fu%2CK&amp;author=Murphy%2CJ&amp;author=Yang%2CM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR33\">Garabedian CA, 1934. Birkhoff on aesthetic measure. <i>Bull Amer Math Soc</i>, 40(1):7\u201310. <a href=\"https://doi.org/10.1090/S0002-9904-1934-05764-1\">https://doi.org/10.1090/S0002-9904-1934-05764-1</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.ams.org/mathscinet-getitem?mr=1562780\" aria-label=\"MathSciNet reference 33\">MathSciNet</a>\u00a0", "    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 33\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Birkhoff%20on%20aesthetic%20measure&amp;journal=Bull%20Amer%20Math%20Soc&amp;volume=40&amp;issue=1&amp;pages=7-10&amp;publication_year=1934&amp;author=Garabedian%2CCA\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR34\">Gatys L, Ecker A, Bethge M, 2016a. Image style transfer using convolutional neural networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2414\u20132423. <a href=\"https://doi.org/10.1109/CVPR.2016.265\">https://doi.org/10.1109/CVPR.2016.265</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR35\">Gatys L, Ecker A, Bethge M, 2016b. A neural algorithm of artistic style. <i>J Vis</i>, 16(12):326. <a href=\"https://doi.org/10.1167/16.12.326\">https://doi.org/10.1167/16.12.326</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 35\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20neural%20algorithm%20of%20artistic%20style&amp;journal=J%20Vis&amp;volume=16&amp;issue=12&amp;publication_year=2016&amp;author=Gatys%2CL&amp;author=Ecker%2CA&amp;author=Bethge%2CM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR36\">Gero JS, 1990. Design prototypes: a knowledge representation schema for design. <i>AI Mag</i>, 11(4):26\u201336.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 36\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Design%20prototypes%3A%20a%20knowledge%20representation%20schema%20for%20design&amp;journal=AI%20Mag&amp;volume=11&amp;issue=4&amp;pages=26-36&amp;publication_year=1990&amp;author=Gero%2CJS\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR37\">Gilon K, Chan J, Ng FY, et al., 2018. Analogy mining for specific design needs. Proc CHI Conf on Human Factors in Computing Systems, p.121. <a href=\"https://doi.org/10.1145/3173574.3173695\">https://doi.org/10.1145/3173574.3173695</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR38\">Goel AK, Rugaber S, Vattam S, 2009. Structure, behavior, and function of complex systems: the structure, behavior, and function modeling language. <i>AI Edam</i>, 23(1):23\u201335. <a href=\"https://doi.org/10.1017/S0890060409000080\">https://doi.org/10.1017/S0890060409000080</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 38\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Structure%2C%20behavior%2C%20and%20function%20of%20complex%20systems%3A%20the%20structure%2C%20behavior%2C%20and%20function%20modeling%20language&amp;journal=AI%20Edam&amp;volume=23&amp;issue=1&amp;pages=23-35&amp;publication_year=2009&amp;author=Goel%2CAK&amp;author=Rugaber%2CS&amp;author=Vattam%2CS\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR39\">Goldschmidt G, Smolkov M, 2006. Variances in the impact of visual stimuli on design problem solving performance. <i>Des Stud</i>, 27(5):549\u2013569. <a href=\"https://doi.org/10.1016/j.destud.2006.01.002\">https://doi.org/10.1016/j.destud.2006.01.002</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 39\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Variances%20in%20the%20impact%20of%20visual%20stimuli%20on%20design%20problem%20solving%20performance&amp;journal=Des%20Stud&amp;volume=27&amp;issue=5&amp;pages=549-569&amp;publication_year=2006&amp;author=Goldschmidt%2CG&amp;author=Smolkov%2CM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR40\">Gooch B, Gooch A, 2001. Non-photorealistic Rendering. A K Peters/CRC Press, New York, USA. <a href=\"https://doi.org/10.1201/9781439864173\">https://doi.org/10.1201/9781439864173</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?1055.68136\" aria-label=\"MATH reference 40\">MATH</a>\u00a0", "    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 40\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Non-photorealistic%20Rendering&amp;publication_year=2001&amp;author=Gooch%2CB&amp;author=Gooch%2CA\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR41\">Goodfellow I, Pouget-Abadie J, Mirza M, et al., 2014. Generative adversarial nets. Proc 27<sup>th</sup> Int Conf on Neural Information Processing Systems, p.2672\u20132680.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR42\">Grudin J, Pruitt J, 2002. Personas, participatory design, and product development: an infrastructure for engagement. Proc 7<sup>th</sup> Biennial Participatory Design Conf, p.144\u2013152.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR43\">Gulrajani I, Ahmed F, Arjovsky M, et al., 2017. Improved training of Wasserstein GANs. Advances in Neural Information Proc Systems, p.5767\u20135777.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR44\">Han J, Shi F, Chen LQ, et al., 2018. A computational tool for creative idea generation based on analogical reasoning and ontology. <i>Artif Intell Eng Des Anal Manuf</i>, 32(4):462\u2013477. <a href=\"https://doi.org/10.1017/S0890060418000082\">https://doi.org/10.1017/S0890060418000082</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 44\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20computational%20tool%20for%20creative%20idea%20generation%20based%20on%20analogical%20reasoning%20and%20ontology&amp;journal=Artif%20Intell%20Eng%20Des%20Anal%20Manuf&amp;volume=32&amp;issue=4&amp;pages=462-477&amp;publication_year=2018&amp;author=Han%2CJ&amp;author=Shi%2CF&amp;author=Chen%2CLQ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR45\">Hao J, Zhou YJ, Zhao QF, et al., 2019. An evolutionary computation based method for creative design inspiration generation. <i>J Intell Manuf</i>, 30(4):1673\u20131691. <a href=\"https://doi.org/10.1007/s10845-017-1347-x\">https://doi.org/10.1007/s10845-017-1347-x</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 45\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=An%20evolutionary%20computation%20based%20method%20for%20creative%20design%20inspiration%20generation&amp;journal=J%20Intell%20Manuf&amp;volume=30&amp;issue=4&amp;pages=1673-1691&amp;publication_year=2019&amp;author=Hao%2CJ&amp;author=Zhou%2CYJ&amp;author=Zhao%2CQF\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR46\">Hartson R, Pyla PS, 2012. The UX Book: Process and Guidelines for Ensuring a Quality User Experience. Elsevier, Amsterdam. <a href=\"https://doi.org/10.1016/C2010-0-66326-7\">https://doi.org/10.1016/C2010-0-66326-7</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 46\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20UX%20Book%3A%20Process%20and%20Guidelines%20for%20Ensuring%20a%20Quality%20User%20Experience&amp;publication_year=2012&amp;author=Hartson%2CR&amp;author=Pyla%2CPS\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR47\">He KM, Sun J, 2014. Image completion approaches using the statistics of similar patches. <i>IEEE Trans Patt Anal Mach Intell</i>, 36(12):2423\u20132435. <a href=\"https://doi.org/10.1109/TPAMI.2014.2330611\">https://doi.org/10.1109/TPAMI.2014.2330611</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 47\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Image%20completion%20approaches%20using%20the%20statistics%20of%20similar%20patches&amp;journal=IEEE%20Trans%20Patt%20Anal%20Mach%20Intell&amp;volume=36&amp;issue=12&amp;pages=2423-2435&amp;publication_year=2014&amp;author=He%2CKM&amp;author=Sun%2CJ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR48\">Hertzmann A, Jacobs CE, Oliver N, et al., 2001. Image analogies. Proc 28<sup>th</sup> Annual Conf on Computer Graphics and Interactive Techniques, p.327\u2013340. <a href=\"https://doi.org/10.1145/383259.383295\">https://doi.org/10.1145/383259.383295</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR49\">Hong YJ, Hwang U, Yoo J, et al., 2019. How generative adversarial networks and their variants work: an overview. <i>ACM Comput Surv</i>, 52(1):10. <a href=\"https://doi.org/10.1145/3301282\">https://doi.org/10.1145/3301282</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 49\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=How%20generative%20adversarial%20networks%20and%20their%20variants%20work%3A%20an%20overview&amp;journal=ACM%20Comput%20Surv&amp;volume=52&amp;issue=1&amp;publication_year=2019&amp;author=Hong%2CYJ&amp;author=Hwang%2CU&amp;author=Yoo%2CJ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR50\">Huang HZ, Wang H, Luo WH, et al., 2017. Real-time neural style transfer for videos. IEEE Conf on Computer Vision and Pattern Recognition, p.7044\u20137052. <a href=\"https://doi.org/10.1109/CVPR.2017.745\">https://doi.org/10.1109/CVPR.2017.745</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR51\">Huang X, Belongie S, 2017. Arbitrary style transfer in realtime with adaptive instance normalization. Proc IEEE Int Conf on Computer Vision, p.1501\u20131510. <a href=\"https://doi.org/10.1109/ICCV.2017.167\">https://doi.org/10.1109/ICCV.2017.167</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR52\">Iizuka S, Simo-Serra E, Ishikawa H, 2017. Globally and locally consistent image completion. <i>ACM Trans Graph</i>, 36(4), Article 107. <a href=\"https://doi.org/10.1145/3072959.3073659\">https://doi.org/10.1145/3072959.3073659</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 52\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Globally%20and%20locally%20consistent%20image%20completion&amp;journal=ACM%20Transactions%20on%20Graphics&amp;volume=36&amp;issue=4&amp;pages=1-14&amp;publication_year=2017&amp;author=Iizuka%2CSatoshi&amp;author=Simo-Serra%2CEdgar&amp;author=Ishikawa%2CHiroshi\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR53\">Isola P, Zhu JY, Zhou TH, et al., 2017. Image-to-image translation with conditional adversarial networks. IEEE Conf on Computer Vision and Pattern Recognition, p.5967\u20135976. <a href=\"https://doi.org/10.1109/CVPR.2017.632\">https://doi.org/10.1109/CVPR.2017.632</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR54\">Jansen BJ, Jung SG, Salminen J, et al., 2017. Viewed by too many or viewed too little: using information dissemination for audience segmentation. <i>Proc Assoc Inform Sci Technol</i>, 54(1):189\u2013196. <a href=\"https://doi.org/10.1002/pra2.2017.14505401021\">https://doi.org/10.1002/pra2.2017.14505401021</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 54\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Viewed%20by%20too%20many%20or%20viewed%20too%20little%3A%20using%20information%20dissemination%20for%20audience%20segmentation&amp;journal=Proc%20Assoc%20Inform%20Sci%20Technol&amp;volume=54&amp;issue=1&amp;pages=189-196&amp;publication_year=2017&amp;author=Jansen%2CBJ&amp;author=Jung%2CSG&amp;author=Salminen%2CJ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR55\">Jansson DG, Smith SM, 1991. Design fixation. <i>Des Stud</i>, 12(1):3\u201311. <a href=\"https://doi.org/10.1016/0142-694X(91)90003-F\">https://doi.org/10.1016/0142-694X(91)90003-F</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 55\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Design%20fixation&amp;journal=Des%20Stud&amp;volume=12&amp;issue=1&amp;pages=3-11&amp;publication_year=1991&amp;author=Jansson%2CDG&amp;author=Smith%2CSM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR56\">Jia J, Huang J, Shen GY, et al., 2016. Learning to appreciate the aesthetic effects of clothing. Proc 30<sup>th</sup> AAAI Conf on Artificial Intelligence, p.1216\u20131222.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR57\">Jia L, Becattini N, Cascini G, et al., 2020. Testing ideation performance on a large set of designers: effects of analogical distance. <i>Int J Des Creat Innov</i>, 8(1):31\u201345. <a href=\"https://doi.org/10.1080/21650349.2019.1618736\">https://doi.org/10.1080/21650349.2019.1618736</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 57\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Testing%20ideation%20performance%20on%20a%20large%20set%20of%20designers%3A%20effects%20of%20analogical%20distance&amp;journal=Int%20J%20Des%20Creat%20Innov&amp;volume=8&amp;issue=1&amp;pages=31-45&amp;publication_year=2020&amp;author=Jia%2CL&amp;author=Becattini%2CN&amp;author=Cascini%2CG\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR58\">Jiang SH, Fu Y, 2017. Fashion style generator. Proc 26<sup>th</sup> Int Joint Conf on Artificial Intelligence, p.3721\u20133727. <a href=\"https://doi.org/10.24963/ijcai.2017/520\">https://doi.org/10.24963/ijcai.2017/520</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR59\">Jing YC, Yang YZ, Feng ZL, et al., 2019. Neural style transfer: a review. <i>IEEE Trans Vis Comput Graph</i>, in press. <a href=\"https://doi.org/10.1109/tvcg.2019.2921336\">https://doi.org/10.1109/tvcg.2019.2921336</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR60\">Jo Y, Park J, 2019. SC-FEGAN: face editing generative adversarial network with user\u2019s sketch and color. <a href=\"https://arxiv.org/abs/1902.06838\">https://doi.org/1902.06838</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR61\">Johnson J, Alahi A, Li FF, 2016. Perceptual losses for real-time style transfer and super-resolution. Proc 14<sup>th</sup> European Conf, p.694\u2013711. <a href=\"https://doi.org/10.1007/978-3-319-46475-6_43\">https://doi.org/10.1007/978-3-319-46475-6_43</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 61\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Perceptual%20Losses%20for%20Real-Time%20Style%20Transfer%20and%20Super-Resolution&amp;pages=694-711&amp;publication_year=2016&amp;author=Johnson%2CJustin&amp;author=Alahi%2CAlexandre&amp;author=Fei-Fei%2CLi\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR62\">Karras T, Laine S, Aila T, 2019. A style-based generator architecture for generative adversarial networks. The IEEE Conf on Computer Vision and Pattern Recognition, p.4401\u20134410.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR63\">Kaufman JC, Sternberg RJ, 2006. The International Handbook of Creativity. Edward Elgar Publishing, Cheltenham, UK.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 63\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20International%20Handbook%20of%20Creativity&amp;publication_year=2006&amp;author=Kaufman%2CJC&amp;author=Sternberg%2CRJ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR64\">Keys R, 1981. Cubic convolution interpolation for digital image processing. <i>IEEE Trans Acoust Speech Signal Process</i>, 29(6):1153\u20131160. <a href=\"https://doi.org/10.1109/TASSP.1981.1163711\">https://doi.org/10.1109/TASSP.1981.1163711</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.ams.org/mathscinet-getitem?mr=642902\" aria-label=\"MathSciNet reference 64\">MathSciNet</a>\u00a0", "    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?0524.65006\" aria-label=\"MATH reference 64\">MATH</a>\u00a0", "    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 64\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Cubic%20convolution%20interpolation%20for%20digital%20image%20processing&amp;journal=IEEE%20Trans%20Acoust%20Speech%20Signal%20Process&amp;volume=29&amp;issue=6&amp;pages=1153-1160&amp;publication_year=1981&amp;author=Keys%2CR\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR65\">Kim J, Lee JK, Lee KM, 2016. Accurate image superresolution using very deep convolutional networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1646\u20131654. <a href=\"https://doi.org/10.1109/CVPR.2016.182\">https://doi.org/10.1109/CVPR.2016.182</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR66\">Kingma DP, Welling M, 2013. Auto-encoding variational Bayes. https://arxiv.org/abs/1312.6114</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR67\">Kong S, Shen XH, Lin Z, et al., 2016. Photo aesthetics ranking network with attributes and content adaptation. Proc 14<sup>th</sup> European Conf on Computer Vision, p.662\u2013679. <a href=\"https://doi.org/10.1007/978-3-319-46448-0_40\">https://doi.org/10.1007/978-3-319-46448-0_40</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 67\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Photo%20Aesthetics%20Ranking%20Network%20with%20Attributes%20and%20Content%20Adaptation&amp;pages=662-679&amp;publication_year=2016&amp;author=Kong%2CShu&amp;author=Shen%2CXiaohui&amp;author=Lin%2CZhe&amp;author=Mech%2CRadomir&amp;author=Fowlkes%2CCharless\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR68\">Krizhevsky A, Hinton G, 2009. Learning Multiple Layers of Features from Tiny Images. Technical Report, University of Toronto, Toronto.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 68\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Learning%20Multiple%20Layers%20of%20Features%20from%20Tiny%20Images&amp;publication_year=2009&amp;author=Krizhevsky%2CA&amp;author=Hinton%2CG\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR69\">Kwak H, An J, Jansen BJ, 2017. Automatic generation of personas using YouTube social media data. Proc 50<sup>th</sup> Hawaii Int Conf on System Sciences, p.833\u2013842.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR70\">Larsen ABL, S\u00f8nderby SK, Larochelle H, et al., 2016. Autoencoding beyond pixels using a learned similarity metric. Proc 33<sup>rd</sup> Int Conf on Machine Learning, p.1558\u20131566.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR71\">LeCun Y, Bottou L, Bengio Y, et al., 1998. Gradient-based learning applied to document recognition. <i>Proc IEEE</i>, 86(11):2278\u20132323. <a href=\"https://doi.org/10.1109/5.726791\">https://doi.org/10.1109/5.726791</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 71\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Gradient-based%20learning%20applied%20to%20document%20recognition&amp;journal=Proc%20IEEE&amp;volume=86&amp;issue=11&amp;pages=2278-2323&amp;publication_year=1998&amp;author=LeCun%2CY&amp;author=Bottou%2CL&amp;author=Bengio%2CY\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR72\">Ledig C, Theis L, Husz\u00e1r F, et al., 2017. Photo-realistic single image super-resolution using a generative adversarial network. IEEE Conf on Computer Vision and Pattern Recognition, p.105\u2013114. <a href=\"https://doi.org/10.1109/CVPR.2017.19\">https://doi.org/10.1109/CVPR.2017.19</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR73\">Li C, Wand M, 2016. Precomputed real-time texture synthesis with Markovian generative adversarial networks. Proc 14<sup>th</sup> European Conf on Computer Vision, p.702\u2013716. <a href=\"https://doi.org/10.1007/978-3-319-46487-9_43\">https://doi.org/10.1007/978-3-319-46487-9_43</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 73\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Precomputed%20Real-Time%20Texture%20Synthesis%20with%20Markovian%20Generative%20Adversarial%20Networks&amp;pages=702-716&amp;publication_year=2016&amp;author=Li%2CChuan&amp;author=Wand%2CMichael\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR74\">Li CC, Chen T, 2009. Aesthetic visual quality assessment of paintings. <i>IEEE J Sel Top Signal Process</i>, 3(2):236\u2013252. <a href=\"https://doi.org/10.1109/JSTSP.2009.2015077\">https://doi.org/10.1109/JSTSP.2009.2015077</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 74\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Aesthetic%20visual%20quality%20assessment%20of%20paintings&amp;journal=IEEE%20J%20Sel%20Top%20Signal%20Process&amp;volume=3&amp;issue=2&amp;pages=236-252&amp;publication_year=2009&amp;author=Li%2CCC&amp;author=Chen%2CT\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR75\">Li HH, Wang JG, Tang MM, et al., 2017. Polarization-dependent effects of an Airy beam due to the spin-orbit coupling. <i>J Opt Soc Am A</i>, 34(7):1114\u20131118. <a href=\"https://doi.org/10.1364/JOSAA.34.001114\">https://doi.org/10.1364/JOSAA.34.001114</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 75\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Polarization-dependent%20effects%20of%20an%20Airy%20beam%20due%20to%20the%20spin-orbit%20coupling&amp;journal=J%20Opt%20Soc%20Am%20A&amp;volume=34&amp;issue=7&amp;pages=1114-1118&amp;publication_year=2017&amp;author=Li%2CHH&amp;author=Wang%2CJG&amp;author=Tang%2CMM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR76\">Li XT, Liu SF, Kautz J, et al., 2019. Learning linear transformations for fast arbitrary style transfer. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.3809\u20133817.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR77\">Li YJ, Fang C, Yang JM, et al., 2017. Universal style transfer via feature transforms. Proc 31<sup>st</sup> Conf on Neural Information Processing Systems, p.386\u2013396.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR78\">Liu GL, Reda FA, Shih KJ, et al., 2018. Image inpainting for irregular holes using partial convolutions. Proc 15<sup>th</sup> European Conf on Computer Vision, p.85\u2013105. <a href=\"https://doi.org/10.1007/978-3-030-01252-6_6\">https://doi.org/10.1007/978-3-030-01252-6_6</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 78\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Image%20Inpainting%20for%20Irregular%20Holes%20Using%20Partial%20Convolutions&amp;pages=89-105&amp;publication_year=2018&amp;author=Liu%2CGuilin&amp;author=Reda%2CFitsum%20A.&amp;author=Shih%2CKevin%20J.&amp;author=Wang%2CTing-Chun&amp;author=Tao%2CAndrew&amp;author=Catanzaro%2CBryan\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR79\">Liu H, Singh P, 2004. ConceptNet\u2014a practical commonsense reasoning tool-kit. <i>BT Technol</i> J, 22(4):211\u2013226. <a href=\"https://doi.org/10.1023/B:BTTJ.0000047600.45421.6d\">https://doi.org/10.1023/B:BTTJ.0000047600.45421.6d</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 79\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=ConceptNet%E2%80%94a%20practical%20commonsense%20reasoning%20tool-kit&amp;journal=BT%20Technol&amp;volume=22&amp;issue=4&amp;pages=211-226&amp;publication_year=2004&amp;author=Liu%2CH&amp;author=Singh%2CP\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR80\">Liu MY, Huang X, Mallya A, et al., 2019. Few-shot unsupervised image-to-image translation. <a href=\"https://arxiv.org/abs/1905.01723\">https://doi.org/1905.01723</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR81\">Liu ZW, Luo P, Wang XG, et al., 2015. Deep learning face attributes in the wild. Proc IEEE Int Conf on Computer Vision, p.3730\u20133738. <a href=\"https://doi.org/10.1109/ICCV.2015.425\">https://doi.org/10.1109/ICCV.2015.425</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR82\">Lowdermilk T, 2013. User-Centered Design: a Developer\u2019s Guide to Building User-Friendly Applications. O\u2019Reilly, Beijing, China.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 82\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=User-Centered%20Design%3A%20a%20Developer%E2%80%99s%20Guide%20to%20Building%20User-Friendly%20Applications&amp;publication_year=2013&amp;author=Lowdermilk%2CT\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR83\">Lu X, Lin Z, Shen XH, et al., 2015. Deep multi-patch aggregation network for image style, aesthetics, and quality estimation. Proc IEEE Int Conf on Computer Vision, p.990\u2013998. <a href=\"https://doi.org/10.1109/ICCV.2015.119\">https://doi.org/10.1109/ICCV.2015.119</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR84\">Luo YW, Tang XO, 2008. Photo and video quality evaluation: focusing on the subject. Proc 10<sup>th</sup> European Conf on Computer Vision, p.386\u2013399.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 84\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Photo%20and%20Video%20Quality%20Evaluation%3A%20Focusing%20on%20the%20Subject&amp;pages=386-399&amp;publication_year=2008&amp;author=Luo%2CYiwen&amp;author=Tang%2CXiaoou\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR85\">Ma S, Liu J, Chen WC, 2017. A-lamp: adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. Proc 30<sup>th</sup> IEEE Conf on Computer Vision and Pattern Recognition, p.722\u2013731. <a href=\"https://doi.org/10.1109/CVPR.2017.84\">https://doi.org/10.1109/CVPR.2017.84</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR86\">Maguire M, Bevan N, 2002. User requirements analysis. In: Hammond J, Gross T, Wesson J (Eds.), Usability: Gaining a Competitive Edge. Springer, Boston, USA, p.133\u2013148. <a href=\"https://doi.org/10.1007/978-0-387-35610-5_9\">https://doi.org/10.1007/978-0-387-35610-5_9</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 86\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=User%20requirements%20analysis&amp;pages=133-148&amp;publication_year=2002&amp;author=Maguire%2CM&amp;author=Bevan%2CN\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR87\">Mai L, Jin HL, Liu F, 2016. Composition-preserving deep photo aesthetics assessment. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.497\u2013506. <a href=\"https://doi.org/10.1109/CVPR.2016.60\">https://doi.org/10.1109/CVPR.2016.60</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR88\">Matthews T, Judge T, Whittaker S, 2012. How do designers and user experience professionals actually perceive and use personas? Proc Conf on Human Factors in Computing Systems, p.1219\u20131228. <a href=\"https://doi.org/10.1145/2207676.2208573\">https://doi.org/10.1145/2207676.2208573</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR89\">McCaffrey T, Krishnamurty S, 2015. The obscure features hypothesis in design innovation. <i>Int J Des Creat Innov</i>, 3(1):1\u201328. <a href=\"https://doi.org/10.1080/21650349.2014.893840\">https://doi.org/10.1080/21650349.2014.893840</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 89\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20obscure%20features%20hypothesis%20in%20design%20innovation&amp;journal=Int%20J%20Des%20Creat%20Innov&amp;volume=3&amp;issue=1&amp;pages=1-28&amp;publication_year=2015&amp;author=McCaffrey%2CT&amp;author=Krishnamurty%2CS\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR90\">McGinn J, Kotamraju N, 2008. Data-driven persona development. Proc Conf on Human Factors in Computing Systems, p.1521\u20131524. <a href=\"https://doi.org/10.1145/1357054.1357292\">https://doi.org/10.1145/1357054.1357292</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR91\">Miaskiewicz T, Kozar KA, 2011. Personas and user-centered design: how can personas benefit product design processes? <i>Des Stud</i>, 32(5):417\u2013430. <a href=\"https://doi.org/10.1016/j.destud.2011.03.003\">https://doi.org/10.1016/j.destud.2011.03.003</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 91\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Personas%20and%20user-centered%20design%3A%20how%20can%20personas%20benefit%20product%20design%20processes%3F&amp;journal=Des%20Stud&amp;volume=32&amp;issue=5&amp;pages=417-430&amp;publication_year=2011&amp;author=Miaskiewicz%2CT&amp;author=Kozar%2CKA\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR92\">Mikolov T, Chen K, Corrado G, et al., 2013. Efficient estimation of word representations in vector space. <a href=\"https://arxiv.org/abs/1301.3781\">https://doi.org/1301.3781</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR93\">Miller GA, 1995. Wordnet: a lexical database for English. <i>Commun ACM</i>, 38(11):39\u201341. <a href=\"https://doi.org/10.1145/219717.219748\">https://doi.org/10.1145/219717.219748</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 93\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Wordnet%3A%20a%20lexical%20database%20for%20English&amp;journal=Commun%20ACM&amp;volume=38&amp;issue=11&amp;pages=39-41&amp;publication_year=1995&amp;author=Miller%2CGA\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR94\">Mirza M, Osindero S, 2014. Conditional generative adversarial nets. <a href=\"https://arxiv.org/abs/1411.1784\">https://doi.org/1411.1784</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR95\">Miyato T, Kataoka T, Koyama M, et al., 2018. Spectral normalization for generative adversarial networks. Int Conf on Learning Representations.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR96\">Murray N, Marchesotti L, Perronnin F, 2012. AVA: a large-scale database for aesthetic visual analysis. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2408\u20132415. <a href=\"https://doi.org/10.1109/CVPR.2012.6247954\">https://doi.org/10.1109/CVPR.2012.6247954</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR97\">Nazeri K, Ng E, Joseph T, et al., 2019. Edgeconnect: generative image inpainting with adversarial edge learning. <a href=\"https://arxiv.org/abs/1901.00212\">https://doi.org/1901.00212</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR98\">Nelson BA, Wilson JO, Rosen D, et al., 2009. Refined metrics for measuring ideation effectiveness. <i>Des Stud</i>, 30(6):737\u2013743. <a href=\"https://doi.org/10.1016/j.destud.2009.07.002\">https://doi.org/10.1016/j.destud.2009.07.002</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 98\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Refined%20metrics%20for%20measuring%20ideation%20effectiveness&amp;journal=Des%20Stud&amp;volume=30&amp;issue=6&amp;pages=737-743&amp;publication_year=2009&amp;author=Nelson%2CBA&amp;author=Wilson%2CJO&amp;author=Rosen%2CD\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR99\">Nielsen L, Hansen KS, Stage J, et al., 2015. A template for design personas: analysis of 47 persona descriptions from Danish industries and organizations. <i>Int J Sociotechnol Knowl Dev</i>, 7(1):45\u201361. <a href=\"https://doi.org/10.4018/ijskd.2015010104\">https://doi.org/10.4018/ijskd.2015010104</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 99\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20template%20for%20design%20personas%3A%20analysis%20of%2047%20persona%20descriptions%20from%20Danish%20industries%20and%20organizations&amp;journal=Int%20J%20Sociotechnol%20Knowl%20Dev&amp;volume=7&amp;issue=1&amp;pages=45-61&amp;publication_year=2015&amp;author=Nielsen%2CL&amp;author=Hansen%2CKS&amp;author=Stage%2CJ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR100\">Niles I, Pease A, 2001. Towards a standard upper ontology. Proc Int Conf on Formal Ontology in Information Systems, p.2\u20139. <a href=\"https://doi.org/10.1145/505168.505170\">https://doi.org/10.1145/505168.505170</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR101\">Nilsback ME, Zisserman A, 2008. Automated flower classification over a large number of classes. Proc 6<sup>th</sup> Indian Conf on Computer Vision, Graphics &amp; Image Processing, p.722\u2013729. <a href=\"https://doi.org/10.1109/ICVGIP.2008.47\">https://doi.org/10.1109/ICVGIP.2008.47</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR102\">Odena A, Olah C, Shlens J, 2017. Conditional image synthesis with auxiliary classifier GANs. Proc 34<sup>th</sup> Int Conf on Machine Learning, p.4043\u20134055.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR103\">Pan YH, 2017. Special issue on artificial intelligence 2.0. <i>Front Inform Technol Electron Eng</i>, 18(1):1\u20132. <a href=\"https://doi.org/10.1631/FITEE.1710000\">https://doi.org/10.1631/FITEE.1710000</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 103\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Special%20issue%20on%20artificial%20intelligence%202.0&amp;journal=Front%20Inform%20Technol%20Electron%20Eng&amp;volume=18&amp;issue=1&amp;pages=1-2&amp;publication_year=2017&amp;author=Pan%2CYH\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR104\">Park T, Liu MY, Wang TC, et al., 2019. Semantic image synthesis with spatially-adaptive normalization. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2337\u20132346.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR105\">Pathak D, Kr\u00e4henb\u00fchl P, Donahue J, et al., 2016. Context encoders: feature learning by inpainting. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.2536\u20132544. <a href=\"https://doi.org/10.1109/CVPR.2016.278\">https://doi.org/10.1109/CVPR.2016.278</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR106\">Peeters JR, Verhaegen PA, Vandevenne D, et al., 2010. Refined metrics for measuring novelty in ideation. ID-MME Virtual Concept Research in Interaction Design, Article 4.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR107\">Perera D, Zimmermann R, 2019. CNGAN: generative adversarial networks for cross-network user preference generation for non-overlapped users. World Wide Web Conf, p.3144\u20133150. <a href=\"https://doi.org/10.1145/3308558.3313733\">https://doi.org/10.1145/3308558.3313733</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR108\">Pruitt J, Adlin T, 2005. The Persona Lifecycle: Keeping People in Mind Throughout Product Design. Elsevier, Amsterdam, p.724.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 108\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=The%20Persona%20Lifecycle%3A%20Keeping%20People%20in%20Mind%20Throughout%20Product%20Design&amp;publication_year=2005&amp;author=Pruitt%2CJ&amp;author=Adlin%2CT\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR109\">Radford A, Metz L, Chintala S, 2016. Unsupervised representation learning with deep convolutional generative adversarial networks. Proc 4<sup>th</sup> Int Conf on Learning Representations.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR110\">Reed SE, Akata Z, Yan XC, et al., 2016a. Generative adversarial text to image synthesis. Proc 33<sup>rd</sup> Int Conf on Machine Learning, p.1681\u20131690.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR111\">Reed SE, Akata Z, Mohan S, et al., 2016b. Learning what and where to draw. Advances in Neural Information Processing Systems, p.217\u2013225.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR112\">Rigau J, Feixas M, Sbert M, 2008. Informational aesthetics measures. <i>IEEE Comput Graph Appl</i>, 28(2):24\u201334. <a href=\"https://doi.org/10.1109/MCG.2008.34\">https://doi.org/10.1109/MCG.2008.34</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 112\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Informational%20aesthetics%20measures&amp;journal=IEEE%20Comput%20Graph%20Appl&amp;volume=28&amp;issue=2&amp;pages=24-34&amp;publication_year=2008&amp;author=Rigau%2CJ&amp;author=Feixas%2CM&amp;author=Sbert%2CM\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR113\">Russell SJ, Norvig P, 2016. Artificial Intelligence: a Modern Approach. Pearson Education Limited, Harlow, Essex.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" href=\"http://www.emis.de/MATH-item?0835.68093\" aria-label=\"MATH reference 113\">MATH</a>\u00a0", "    <a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 113\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Artificial%20Intelligence%3A%20a%20Modern%20Approach&amp;publication_year=2016&amp;author=Russell%2CSJ&amp;author=Norvig%2CP\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR114\">Saleh B, Elgammal A, 2015. Large-scale classification of fine-art paintings: learning the right metric on the right feature. <a href=\"https://arxiv.org/abs/1505.00855\">https://doi.org/1505.00855</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR115\">Salimans T, Goodfellow IJ, Zaremba W, et al., 2016. Improved techniques for training GANs. Advances in Neural Information Processing Systems, p.2226\u20132234.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR116\">Salminen J, Seng\u00fcn S, Kwak H, et al., 2017. Generating cultural personas from social data: a perspective of middle eastern users. Proc 5<sup>th</sup> Int Conf on Future Internet of Things and Cloud Workshops, p.120\u2013125. <a href=\"https://doi.org/10.1109/FiCloudW.2017.97\">https://doi.org/10.1109/FiCloudW.2017.97</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR117\">Salminen J, Jansen BJ, An J, et al., 2018a. Are personas done? Evaluating their usefulness in the age of digital analytics. <i>Persona Stud</i>, 4(2):47\u201365. <a href=\"https://doi.org/10.21153/psj2018vol4no2art737\">https://doi.org/10.21153/psj2018vol4no2art737</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 117\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Are%20personas%20done%3F%20Evaluating%20their%20usefulness%20in%20the%20age%20of%20digital%20analytics&amp;journal=Persona%20Stud&amp;volume=4&amp;issue=2&amp;pages=47-65&amp;publication_year=2018&amp;author=Salminen%2CJ&amp;author=Jansen%2CBJ&amp;author=An%2CJ\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR118\">Salminen J, Jung SG, An J, et al., 2018b. Findings of a user study of automatically generated personas. Proc Conf on Human Factors in Computing Systems, p.LBW097. <a href=\"https://doi.org/10.1145/3170427.3188470\">https://doi.org/10.1145/3170427.3188470</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR119\">Salminen J, Eng\u00fcn S, Jung SG, et al., 2019. Design issues in automatically generated persona profiles: a qualitative analysis from 38 think-aloud transcripts. Proc Conf on Human Information Interaction and Retrieval, p.225\u2013229. <a href=\"https://doi.org/10.1145/3295750.3298942\">https://doi.org/10.1145/3295750.3298942</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR120\">Schwarz K, Wieschollek P, Lensch HPA, 2018. Will people like your image? Learning the aesthetic space. Proc IEEE Winter Conf on Applications of Computer Vision, p.2048\u20132057. <a href=\"https://doi.org/10.1109/WACV.2018.00226\">https://doi.org/10.1109/WACV.2018.00226</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR121\">Shah JJ, Kulkarni SV, Vargas-Hernandez N, 2000. Evaluation of idea generation methods for conceptual design: effectiveness metrics and design of experiments. <i>J Mech Des</i>, 122(4):377\u2013384. <a href=\"https://doi.org/10.1115/1.1315592\">https://doi.org/10.1115/1.1315592</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 121\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Evaluation%20of%20idea%20generation%20methods%20for%20conceptual%20design%3A%20effectiveness%20metrics%20and%20design%20of%20experiments&amp;journal=J%20Mech%20Des&amp;volume=122&amp;issue=4&amp;pages=377-384&amp;publication_year=2000&amp;author=Shah%2CJJ&amp;author=Kulkarni%2CSV&amp;author=Vargas-Hernandez%2CN\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR122\">Simonyan K, Zisserman A, 2014. Very deep convolutional networks for large-scale image recognition. <a href=\"https://arxiv.org/abs/1409.1556\">https://doi.org/1409.1556</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR123\">Strohmann T, Siemon D, Robra-Bissantz S, 2017. brAInstorm: intelligent assistance in group idea generation. Proc 12<sup>th</sup> Int Conf on Design Science Research in Information System and Technology, p.457\u2013461. <a href=\"https://doi.org/10.1007/978-3-319-59144-5_31\">https://doi.org/10.1007/978-3-319-59144-5_31</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 123\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=brAInstorm%3A%20Intelligent%20Assistance%20in%20Group%20Idea%20Generation&amp;pages=457-461&amp;publication_year=2017&amp;author=Strohmann%2CTimo&amp;author=Siemon%2CDominik&amp;author=Robra-Bissantz%2CSusanne\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR124\">Strothotte T, Schlechtweg S, 2002. Non-photorealistic Computer Graphics: Modeling, Rendering, and Animation. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 124\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Non-photorealistic%20Computer%20Graphics%3A%20Modeling%2C%20Rendering%2C%20and%20Animation&amp;publication_year=2002&amp;author=Strothotte%2CT&amp;author=Schlechtweg%2CS\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR125\">Tang X, Wang ZW, Luo WX, et al., 2018. Face aging with identity-preserved conditional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.7939\u20137947. <a href=\"https://doi.org/10.1109/CVPR.2018.00828\">https://doi.org/10.1109/CVPR.2018.00828</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR126\">Tang XO, Luo W, Wang XG, 2013. Content-based photo quality assessment. <i>IEEE Trans Multim</i>, 15(8):1930\u20131943. <a href=\"https://doi.org/10.1109/TMM.2013.2269899\">https://doi.org/10.1109/TMM.2013.2269899</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 126\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Content-based%20photo%20quality%20assessment&amp;journal=IEEE%20Trans%20Multim&amp;volume=15&amp;issue=8&amp;pages=1930-1943&amp;publication_year=2013&amp;author=Tang%2CXO&amp;author=Luo%2CW&amp;author=Wang%2CXG\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR127\">Vandevenne D, Verhaegen PA, Dewulf S, et al., 2015. A scalable approach for ideation in biologically inspired design. <i>Artif Intell Eng Des Anal Manuf</i>, 29(1):19\u201331. <a href=\"https://doi.org/10.1017/S0890060414000122\">https://doi.org/10.1017/S0890060414000122</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 127\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20scalable%20approach%20for%20ideation%20in%20biologically%20inspired%20design&amp;journal=Artif%20Intell%20Eng%20Des%20Anal%20Manuf&amp;volume=29&amp;issue=1&amp;pages=19-31&amp;publication_year=2015&amp;author=Vandevenne%2CD&amp;author=Verhaegen%2CPA&amp;author=Dewulf%2CS\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR128\">Varshney LR, Pinel F, Varshney KR, et al., 2019. A big data approach to computational creativity: the curious case of Chef Watson. <i>IBM J Res Dev</i>, 63(1):7:1\u20137:18. <a href=\"https://doi.org/10.1147/JRD.2019.2893905\">https://doi.org/10.1147/JRD.2019.2893905</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 128\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=A%20big%20data%20approach%20to%20computational%20creativity%3A%20the%20curious%20case%20of%20Chef%20Watson&amp;journal=IBM%20J%20Res%20Dev&amp;volume=63&amp;issue=1&amp;pages=7%3A1-7%3A18&amp;publication_year=2019&amp;author=Varshney%2CLR&amp;author=Pinel%2CF&amp;author=Varshney%2CKR\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR129\">Verma P, Smith JO, 2018. Neural style transfer for audio spectograms. <a href=\"https://arxiv.org/abs/1801.01589\">https://doi.org/1801.01589</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR130\">Wang J, Yu LT, Zhang WN, et al., 2017. IRGAN: a minimax game for unifying generative and discriminative information retrieval models. Proc 40<sup>th</sup> Int ACM SI-GIR Conf on Research and Development in Information Retrieval, p.515\u2013524. <a href=\"https://doi.org/10.1145/3077136.3080786\">https://doi.org/10.1145/3077136.3080786</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR131\">Wang TC, Liu MY, Zhu JY, et al., 2018. Video-to-video synthesis. <a href=\"https://arxiv.org/abs/1808.06601\">https://doi.org/1808.06601</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR132\">Wang WG, Shen JB, 2017. Deep cropping via attention box prediction and aesthetics assessment. Proc IEEE Int Conf on Computer Vision, p.2205\u20132213. <a href=\"https://doi.org/10.1109/ICCV.2017.240\">https://doi.org/10.1109/ICCV.2017.240</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR133\">Wang WN, Cai D, Wang L, et al., 2016. Synthesized computational aesthetic evaluation of photos. <i>Neurocomputing</i>, 172:244\u2013252. <a href=\"https://doi.org/10.1016/j.neucom.2014.12.106\">https://doi.org/10.1016/j.neucom.2014.12.106</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 133\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Synthesized%20computational%20aesthetic%20evaluation%20of%20photos&amp;journal=Neurocomputing&amp;volume=172&amp;pages=244-252&amp;publication_year=2016&amp;author=Wang%2CWN&amp;author=Cai%2CD&amp;author=Wang%2CL\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR134\">Wang WS, Yang S, Zhang WS, et al., 2018. Neural aesthetic image reviewer. <a href=\"https://arxiv.org/abs/1802.10240\">https://doi.org/1802.10240</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR135\">Wang XT, Yu K, Wu SX, et al., 2018. ESRGAN: enhanced super-resolution generative adversarial networks. European Conf on Computer Vision, p.63\u201379. <a href=\"https://doi.org/10.1007/978-3-030-11021-5_5\">https://doi.org/10.1007/978-3-030-11021-5_5</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 135\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=ESRGAN%3A%20Enhanced%20Super-Resolution%20Generative%20Adversarial%20Networks&amp;pages=63-79&amp;publication_year=2019&amp;author=Wang%2CXintao&amp;author=Yu%2CKe&amp;author=Wu%2CShixiang&amp;author=Gu%2CJinjin&amp;author=Liu%2CYihao&amp;author=Dong%2CChao&amp;author=Qiao%2CYu&amp;author=Loy%2CChen%20Change\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR136\">Wu JJ, Zhang CK, Xue TF, et al., 2016. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. Advances in Neural Information Processing Systems, p.82\u201390.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR137\">Xu T, Zhang PC, Huang QY, et al., 2018. AttnGAN: fine-grained text to image generation with attentional generative adversarial networks. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.1316\u20131324. <a href=\"https://doi.org/10.1109/CVPR.2018.00143\">https://doi.org/10.1109/CVPR.2018.00143</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR138\">Yan Y, Wang JR, Tang C, et al., 2019. Research on the development of contemporary design intelligence driven by neural network technology. In: Marcus A, Wang WT (Eds.), Design, User Experience, and Usability. Design Philosophy and Theory. Springer, Cham, p.368\u2013381. <a href=\"https://doi.org/10.1007/978-3-030-23570-3_27\">https://doi.org/10.1007/978-3-030-23570-3_27</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 138\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Research%20on%20the%20development%20of%20contemporary%20design%20intelligence%20driven%20by%20neural%20network%20technology&amp;pages=368-381&amp;publication_year=2019&amp;author=Yan%2CY&amp;author=Wang%2CJR&amp;author=Tang%2CC\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR139\">Yang HY, Huang D, Wang YH, et al., 2018. Learning face age progression: a pyramid architecture of GANs. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.31\u201339. <a href=\"https://doi.org/10.1109/CVPR.2018.00011\">https://doi.org/10.1109/CVPR.2018.00011</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR140\">Yang WM, Zhang XC, Tian YP, et al., 2019. Deep learning for single image super-resolution: a brief review. <i>IEEE Trans Multim</i>, 21(12):3106\u20133121. <a href=\"https://doi.org/10.1109/tmm.2019.2919431\">https://doi.org/10.1109/tmm.2019.2919431</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 140\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Deep%20learning%20for%20single%20image%20super-resolution%3A%20a%20brief%20review&amp;journal=IEEE%20Trans%20Multim&amp;volume=21&amp;issue=12&amp;pages=3106-3121&amp;publication_year=2019&amp;author=Yang%2CWM&amp;author=Zhang%2CXC&amp;author=Tian%2CYP\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR141\">Yang Y, Zhuang YT, Wu F, et al., 2008. Harmonizing hierarchical manifolds for multimedia document semantics understanding and cross-media retrieval. <i>IEEE Trans Multim</i>, 10(3):437\u2013446. <a href=\"https://doi.org/10.1109/TMM.2008.917359\">https://doi.org/10.1109/TMM.2008.917359</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 141\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Harmonizing%20hierarchical%20manifolds%20for%20multimedia%20document%20semantics%20understanding%20and%20cross-media%20retrieval&amp;journal=IEEE%20Trans%20Multim&amp;volume=10&amp;issue=3&amp;pages=437-446&amp;publication_year=2008&amp;author=Yang%2CY&amp;author=Zhuang%2CYT&amp;author=Wu%2CF\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR142\">Yi ZL, Zhang H, Tan P, et al., 2017. DualGAN: unsupervised dual learning for image-to-image translation. Proc IEEE Int Conf on Computer Vision, p.2868\u20132876. <a href=\"https://doi.org/10.1109/ICCV.2017.310\">https://doi.org/10.1109/ICCV.2017.310</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR143\">Yoon Y, Jeon HG, Yoo D, et al., 2015. Learning a deep convolutional network for light-field image super-resolution. Proc IEEE Int Conf on Computer Vision, p.57\u201365. https://doi.org/10.1109/ICCVW.2015.17</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR144\">You S, You N, Pan MX, 2019. PI-REC: progressive image reconstruction network with edge and color domain. <a href=\"https://arxiv.org/abs/1903.10146\">https://doi.org/1903.10146</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR145\">Yu F, Zhang YD, Song SR, et al., 2015. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. <a href=\"https://arxiv.org/abs/1506.03365\">https://doi.org/1506.03365</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR146\">Yu JH, Lin Z, Yang JM, et al., 2018a. Free-form image inpainting with gated convolution. <a href=\"https://arxiv.org/abs/1806.03589\">https://doi.org/1806.03589</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR147\">Yu JH, Lin Z, Yang JM, et al., 2018b. Generative image inpainting with contextual attention. Proc IEEE Conf on Computer Vision and Pattern Recognition, p.5505\u20135514. <a href=\"https://doi.org/10.1109/CVPR.2018.00577\">https://doi.org/10.1109/CVPR.2018.00577</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR148\">Zakharov E, Shysheya A, Burkov E, et al., 2019. Fewshot adversarial learning of realistic neural talking head models. <a href=\"https://arxiv.org/abs/1905.08233\">https://doi.org/1905.08233</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR149\">Zeiler MD, Taylor GW, Fergus R, 2011. Adaptive deconvolutional networks for mid and high level feature learning. Proc IEEE Int Conf on Computer Vision, p.2018\u20132025. <a href=\"https://doi.org/10.1109/ICCV.2011.6126474\">https://doi.org/10.1109/ICCV.2011.6126474</a>", "</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR150\">Zhang H, Xu T, Li H, et al., 2017. StackGAN: text to photo-realistic image synthesis with stacked generative adversarial networks. Proc IEEE Int Conf on Computer Vision, p.5907\u20135915.</p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR151\">Zhang H, Xu T, Li H, et al., 2019. StackGAN++: realistic image synthesis with stacked generative adversarial networks. <i>IEEE Trans Patt Anal Mach Intell</i>, 41(8):1947\u20131962. <a href=\"https://doi.org/10.1109/TPAMI.2018.2856256\">https://doi.org/10.1109/TPAMI.2018.2856256</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 151\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=StackGAN%2B%2B%3A%20realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks&amp;journal=IEEE%20Trans%20Patt%20Anal%20Mach%20Intell&amp;volume=41&amp;issue=8&amp;pages=1947-1962&amp;publication_year=2019&amp;author=Zhang%2CH&amp;author=Xu%2CT&amp;author=Li%2CH\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR152\">Zhang JJ, Yu JH, Zhang K, et al., 2017. Computational aesthetic evaluation of logos. <i>ACM Trans Appl Perc</i>, 14(3), Article 20. <a href=\"https://doi.org/10.1145/3058982\">https://doi.org/10.1145/3058982</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 152\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Computational%20Aesthetic%20Evaluation%20of%20Logos&amp;journal=ACM%20Transactions%20on%20Applied%20Perception&amp;volume=14&amp;issue=3&amp;pages=1-21&amp;publication_year=2017&amp;author=Zhang%2CJiajing&amp;author=Yu%2CJinhui&amp;author=Zhang%2CKang&amp;author=Zheng%2CXianjun%20Sam&amp;author=Zhang%2CJunsong\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR153\">Zhang R, Isola P, Efros AA, 2016. Colorful image colorization. Proc 14<sup>th</sup> European Conf on Computer Vision, p.649\u2013666. <a href=\"https://doi.org/10.1007/978-3-319-46487-9_40\">https://doi.org/10.1007/978-3-319-46487-9_40</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 153\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Colorful%20Image%20Colorization&amp;pages=649-666&amp;publication_year=2016&amp;author=Zhang%2CRichard&amp;author=Isola%2CPhillip&amp;author=Efros%2CAlexei%20A.\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR154\">Zhao H, Gallo O, Frosio I, et al., 2016. Loss functions for image restoration with neural networks. <i>IEEE Trans Comput Imag</i>, 3(1):47\u201357. <a href=\"https://doi.org/10.1109/tci.2016.2644865\">https://doi.org/10.1109/tci.2016.2644865</a>", "</p><p class=\"c-article-references__links u-hide-print\"><a data-track=\"click\" data-track-action=\"outbound reference\" data-track-label=\"link\" aria-label=\"Google Scholar reference 154\" href=\"http://scholar.google.com/scholar_lookup?&amp;title=Loss%20functions%20for%20image%20restoration%20with%20neural%20networks&amp;journal=IEEE%20Trans%20Comput%20Imag&amp;volume=3&amp;issue=1&amp;pages=47-57&amp;publication_year=2016&amp;author=Zhao%2CH&amp;author=Gallo%2CO&amp;author=Frosio%2CI\">", "                    Google Scholar</a>\u00a0", "                </p></li><li class=\"c-article-references__item js-c-reading-companion-references-item\"><p class=\"c-article-references__text\" id=\"ref-CR155\">Zhu JY, Park T, Isola P, et al., 2017. Unpaired image-to-image translation using cycle-consistent adversarial networks. Proc IEEE Int Conf on Computer Vision, p.2242\u20132251. <a href=\"https://doi.org/10.1109/ICCV.2017.244\">https://doi.org/10.1109/ICCV.2017.244</a>", "</p></li></ol><p class=\"c-article-references__download u-hide-print\"><a data-track=\"click\" data-track-action=\"download citation references\" data-track-label=\"link\" href=\"https://citation-needed.springer.com/v2/references/10.1631/FITEE.1900398?format=refman&amp;flavour=references\">Download references<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#global-icon-download\"></use></svg></a></p></div></div></div></section></div><section data-title=\"Acknowledgements\"><div class=\"c-article-section\" id=\"Ack1-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"Ack1\">Acknowledgements</h2><div class=\"c-article-section__content\" id=\"Ack1-content\"><p>Figs. 5c, 5d, and 8 in this study were generated by the pre-trained models of Runway toolkit (https://runwayml.com).</p></div></div></section><section aria-labelledby=\"author-information\" data-title=\"Author information\"><div class=\"c-article-section\" id=\"author-information-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"author-information\">Author information</h2><div class=\"c-article-section__content\" id=\"author-information-content\"><h3 class=\"c-article__sub-heading\" id=\"affiliations\">Affiliations</h3><ol class=\"c-article-author-affiliation__list\"><li id=\"Aff1\"><p class=\"c-article-author-affiliation__address\">College of Computer Science and Technology, Zhejiang University, Hangzhou, 310027, China</p><p class=\"c-article-author-affiliation__authors-list\">Yong-chuan Tang,\u00a0Jiang-jie Huang,\u00a0Meng-ting Yao,\u00a0Jia Wei,\u00a0Wei Li,\u00a0Yong-xing He\u00a0&amp;\u00a0Ze-jian Li</p></li><li id=\"Aff2\"><p class=\"c-article-author-affiliation__address\">Key Laboratory of Design Intelligence and Digital Creativity of Zhejiang Province, Hangzhou, 310027, China</p><p class=\"c-article-author-affiliation__authors-list\">Yong-chuan Tang,\u00a0Jiang-jie Huang,\u00a0Meng-ting Yao,\u00a0Jia Wei,\u00a0Wei Li\u00a0&amp;\u00a0Yong-xing He</p></li><li id=\"Aff3\"><p class=\"c-article-author-affiliation__address\">Zhejiang Lab, Hangzhou, 310012, China</p><p class=\"c-article-author-affiliation__authors-list\">Yong-chuan Tang,\u00a0Wei Li,\u00a0Yong-xing He\u00a0&amp;\u00a0Ze-jian Li</p></li><li id=\"Aff4\"><p class=\"c-article-author-affiliation__address\">Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Hangzhou, 310027, China</p><p class=\"c-article-author-affiliation__authors-list\">Ze-jian Li</p></li></ol><div class=\"u-js-hide u-hide-print\" data-test=\"author-info\"><span class=\"c-article__sub-heading\">Authors</span><ol class=\"c-article-authors-search u-list-reset\"><li id=\"auth-Yong_chuan-Tang\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Yong-chuan Tang</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Yong-chuan+Tang&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in", "                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yong-chuan+Tang\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yong-chuan+Tang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Jiang_jie-Huang\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Jiang-jie Huang</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Jiang-jie+Huang&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in", "                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jiang-jie+Huang\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jiang-jie+Huang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Meng_ting-Yao\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Meng-ting Yao</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Meng-ting+Yao&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in", "                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Meng-ting+Yao\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Meng-ting+Yao%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Jia-Wei\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Jia Wei</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Jia+Wei&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in", "                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Jia+Wei\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Jia+Wei%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Wei-Li\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Wei Li</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Wei+Li&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in", "                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Wei+Li\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Wei+Li%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Yong_xing-He\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Yong-xing He</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Yong-xing+He&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in", "                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Yong-xing+He\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Yong-xing+He%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li><li id=\"auth-Ze_jian-Li\"><span class=\"c-article-authors-search__title u-h3 js-search-name\">Ze-jian Li</span><div class=\"c-article-authors-search__list\"><div class=\"c-article-authors-search__item c-article-authors-search__list-item--left\"><a href=\"/search?dc.creator=&#34;Ze-jian+Li&#34;\" class=\"c-article-button\" data-track=\"click\" data-track-action=\"author link - publication\" data-track-label=\"link\">View author publications</a></div><div class=\"c-article-authors-search__item c-article-authors-search__list-item--right\"><p class=\"search-in-title-js c-article-authors-search__text\">You can also search for this author in", "                        <span class=\"c-article-identifiers\"><a class=\"c-article-identifiers__item\" href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Ze-jian+Li\" data-track=\"click\" data-track-action=\"author link - pubmed\" data-track-label=\"link\">PubMed</a><span class=\"u-hide\">\u00a0</span><a class=\"c-article-identifiers__item\" href=\"http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Ze-jian+Li%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en\" data-track=\"click\" data-track-action=\"author link - scholar\" data-track-label=\"link\">Google Scholar</a></span></p></div></div></li></ol></div><h3 class=\"c-article__sub-heading\" id=\"corresponding-author\">Corresponding author</h3><p id=\"corresponding-author-list\">Correspondence to", "                <a id=\"corresp-c1\" href=\"mailto:yctang@zju.edu.cn\">Yong-chuan Tang</a>.</p></div></div></section><section data-title=\"Additional information\"><div class=\"c-article-section\" id=\"additional-information-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"additional-information\">Additional information</h2><div class=\"c-article-section__content\" id=\"additional-information-content\"><h3 class=\"c-article__sub-heading\">Compliance with ethics guidelines</h3><p>Yong-chuan TANG, Jiang-jie HUANG, Meng-ting YAO, Jia WEI, Wei LI, Yong-xing HE, and Ze-jian LI declare that they have no conflict of interest.</p><p>Project supported by the National Science and Technology Innovation 2030 Major Project of the Ministry of Science and Technology of China (No. 2018AAA0100703), the National Natural Science Foundation of China (Nos. 61773336 and 91748127), the Chinese Academy of Engineering Consulting Project (No. 2018-ZD-12-06), the Provincial Key Research and Development Plan of Zhejiang Province, China (No. 2019C03137), and the Ng Teng Fong Charitable Foundation in the form of ZJU-SUTD IDEA Grant</p></div></div></section><section data-title=\"Rights and permissions\"><div class=\"c-article-section\" id=\"rightslink-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"rightslink\">Rights and permissions</h2><div class=\"c-article-section__content\" id=\"rightslink-content\"><p class=\"c-article-rights\"><a data-track=\"click\" data-track-action=\"view rights and permissions\" data-track-label=\"link\" href=\"https://s100.copyright.com/AppDispatchServlet?title=A%20review%20of%20design%20intelligence%3A%20progress%2C%20problems%2C%20and%20challenges&amp;author=Yong-chuan%20Tang%20et%20al&amp;contentID=10.1631%2FFITEE.1900398&amp;copyright=Zhejiang%20University%20and%20Springer-Verlag%20GmbH%20Germany%2C%20part%20of%20Springer%20Nature&amp;publication=2095-9184&amp;publicationDate=2020-02-13&amp;publisherName=SpringerNature&amp;orderBeanReset=true\">Reprints and Permissions</a></p></div></div></section><section aria-labelledby=\"article-info\" data-title=\"About this article\"><div class=\"c-article-section\" id=\"article-info-section\"><h2 class=\"c-article-section__title js-section-title js-c-reading-companion-sections-item\" id=\"article-info\">About this article</h2><div class=\"c-article-section__content\" id=\"article-info-content\"><div class=\"c-bibliographic-information\"><div class=\"u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border\"><a data-crossmark=\"10.1631/FITEE.1900398\" target=\"_blank\" rel=\"noopener\" href=\"https://crossmark.crossref.org/dialog/?doi=10.1631/FITEE.1900398\" data-track=\"click\" data-track-action=\"Click Crossmark\" data-track-label=\"link\" data-test=\"crossmark\"><img width=\"57\" height=\"81\" alt=\"Verify currency and authenticity via CrossMark\" src=\"data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>\" /></a></div><div class=\"c-bibliographic-information__column\"><h3 class=\"c-article__sub-heading\" id=\"citeas\">Cite this article</h3><p class=\"c-bibliographic-information__citation\">Tang, Yc., Huang, Jj., Yao, Mt. <i>et al.</i> A review of design intelligence: progress, problems, and challenges.", "                    <i>Front Inform Technol Electron Eng</i> <b>20, </b>1595\u20131617 (2019). https://doi.org/10.1631/FITEE.1900398</p><p class=\"c-bibliographic-information__download-citation u-hide-print\"><a data-test=\"citation-link\" data-track=\"click\" data-track-action=\"download article citation\" data-track-label=\"link\" data-track-external=\"\" href=\"https://citation-needed.springer.com/v2/references/10.1631/FITEE.1900398?format=refman&amp;flavour=citation\">Download citation<svg width=\"16\" height=\"16\" focusable=\"false\" role=\"img\" aria-hidden=\"true\" class=\"u-icon\"><use xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"#global-icon-download\"></use></svg></a></p><ul class=\"c-bibliographic-information__list\" data-test=\"publication-history\"><li class=\"c-bibliographic-information__list-item\"><p>Received<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2019-08-06\">06 August 2019</time></span></p></li><li class=\"c-bibliographic-information__list-item\"><p>Accepted<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2019-11-26\">26 November 2019</time></span></p></li><li class=\"c-bibliographic-information__list-item\"><p>Published<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2020-02-13\">13 February 2020</time></span></p></li><li class=\"c-bibliographic-information__list-item\"><p>Issue Date<span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><time datetime=\"2019-12\">December 2019</time></span></p></li><li class=\"c-bibliographic-information__list-item c-bibliographic-information__list-item--doi\"><p><abbr title=\"Digital Object Identifier\">DOI</abbr><span class=\"u-hide\">: </span><span class=\"c-bibliographic-information__value\"><a href=\"https://doi.org/10.1631/FITEE.1900398\" data-track=\"click\" data-track-action=\"view doi\" data-track-label=\"link\">https://doi.org/10.1631/FITEE.1900398</a></span></p></li></ul><div data-component=\"share-box\"><div class=\"c-article-share-box u-display-none\" hidden=\"\"><h3 class=\"c-article__sub-heading\">Share this article</h3><p class=\"c-article-share-box__description\">Anyone you share the following link with will be able to read this content:</p><button class=\"js-get-share-url c-article-share-box__button\" id=\"get-share-url\" data-track=\"click\" data-track-label=\"button\" data-track-external=\"\" data-track-action=\"get shareable link\">Get shareable link</button><div class=\"js-no-share-url-container u-display-none\" hidden=\"\"><p class=\"js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info\">Sorry, a shareable link is not currently available for this article.</p></div><div class=\"js-share-url-container u-display-none\" hidden=\"\"><p class=\"js-share-url c-article-share-box__only-read-input\" id=\"share-url\" data-track=\"click\" data-track-label=\"button\" data-track-action=\"select share url\"></p><button class=\"js-copy-share-url c-article-share-box__button--link-like\" id=\"copy-share-url\" data-track=\"click\" data-track-label=\"button\" data-track-action=\"copy share url\" data-track-external=\"\">Copy to clipboard</button></div><p class=\"js-c-article-share-box__additional-info c-article-share-box__additional-info\">", "                            Provided by the Springer Nature SharedIt content-sharing initiative", "                        </p></div></div><h3 class=\"c-article__sub-heading\">Key words</h3><ul class=\"c-article-subject-list\"><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Design intelligence</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Creativity</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Personas</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Ideation</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">AI-generated content</span></li><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">Computational aesthetics</span></li></ul><h3 class=\"c-article__sub-heading\">CLC number</h3><ul class=\"c-article-subject-list\"><li class=\"c-article-subject-list__subject\"><span itemprop=\"about\">TP183</span></li></ul><div data-component=\"article-info-list\"></div></div></div></div></div></section>", "                </div>", "            </article>", "        </main>", "        <div class=\"c-article-extras u-text-sm u-hide-print\" id=\"sidebar\" data-container-type=\"reading-companion\" data-track-component=\"reading companion\">", "            <aside>", "                <div data-test=\"download-article-link-wrapper\">", "                    ", "                </div>", "                <div data-test=\"collections\">", "                    ", "    ", "                </div>", "                <div data-test=\"editorial-summary\">", "                    ", "                </div>", "                <div class=\"c-reading-companion\">", "                    <div class=\"c-reading-companion__sticky\" data-component=\"reading-companion-sticky\" data-test=\"reading-companion-sticky\">", "                        ", "                            <div class=\"c-article-buy-box\">", "                                <div class=\"sprcom-buybox-articleSidebar\" id=\"sprcom-buybox-articleSidebar\">", " <h2 class=\"c-box__heading\">Access options</h2>", " <article class=\"c-box\" data-test-id=\"buy-article\">", "  <h3 class=\"c-box__heading\">Buy single article</h3>", "  <div class=\"c-box__body\">", "   <div class=\"buybox__info\">", "    <p>Instant access to the full article PDF.</p>", "   </div>", "   <div class=\"buybox__buy\">", "    <p class=\"buybox__price\">USD 39.95</p>", "    <p class=\"buybox__price-info\">Price includes VAT (Brazil)<br>Tax calculation will be finalised during checkout.</p>", "    <form action=\"https://order.springer.com/public/checkout?abtest=v2\" method=\"post\">", "     <input type=\"hidden\" name=\"type\" value=\"article\">", "     <input type=\"hidden\" name=\"doi\" value=\"10.1631/FITEE.1900398\">", "     <input type=\"hidden\" name=\"isxn\" value=\"2095-9230\">", "     <input type=\"hidden\" name=\"contenttitle\" value=\"A review of design intelligence: progress, problems, and challenges\">", "     <input type=\"hidden\" name=\"copyrightyear\" value=\"2019\">", "     <input type=\"hidden\" name=\"year\" value=\"2020\">", "     <input type=\"hidden\" name=\"authors\" value=\"Yong-chuan Tang, et al.\">", "     <input type=\"hidden\" name=\"title\" value=\"Frontiers of Information Technology &amp; Electronic Engineering\">", "     <input type=\"hidden\" name=\"mac\" value=\"82FB53023BBB3109B3CB91533294E4D6\">", "     <input type=\"submit\" class=\"c-box__button\" onclick=\"dataLayer.push({&quot;event&quot;:&quot;addToCart&quot;,&quot;ecommerce&quot;:{&quot;currencyCode&quot;:&quot;USD&quot;,&quot;add&quot;:{&quot;products&quot;:[{&quot;name&quot;:&quot;A review of design intelligence: progress, problems, and challenges&quot;,&quot;id&quot;:&quot;2095-9230&quot;,&quot;price&quot;:39.95,&quot;brand&quot;:&quot;Zhejiang University Press&quot;,&quot;category&quot;:&quot;Computer Science&quot;,&quot;variant&quot;:&quot;ppv-article&quot;,&quot;quantity&quot;:1}]}}});\" value=\"Buy article PDF\">", "    </form>", "   </div>", "  </div>", "  <script>dataLayer.push({\"ecommerce\":{\"currency\":\"USD\",\"impressions\":[{\"name\":\"A review of design intelligence: progress, problems, and challenges\",\"id\":\"2095-9230\",\"price\":39.95,\"brand\":\"Zhejiang University Press\",\"category\":\"Computer Science\",\"variant\":\"ppv-article\",\"quantity\":1}]}});</script>", " </article>", " <article class=\"c-box buybox__rent-article\" id=\"deepdyve\" style=\"display: none\" data-test-id=\"journal-subscription\">", "  <div class=\"c-box__body\">", "   <div class=\"buybox__info\">", "    <p><a class=\"deepdyve-link\" target=\"deepdyve\" rel=\"nofollow\" data-track=\"click\" data-track-action=\"rent article\" data-track-label=\"rent action, new buybox\">Rent this article via DeepDyve.</a></p>", "   </div>", "  </div>", "  <script>", "            function deepDyveResponse(data) {", "                if (data.status === 'ok') {", "                    [].slice.call(document.querySelectorAll('.c-box.buybox__rent-article')).forEach(function (article) {", "                        article.style.display = 'flex'", "                        var link = article.querySelector('.deepdyve-link')", "                        if (link) {", "                          link.setAttribute('href', data.url)", "                        }", "                    })", "                }", "            }", "            var script = document.createElement('script')", "            script.src = '//www.deepdyve.com/rental-link?docId=10.1631/FITEE.1900398&journal=2095-9230&fieldName=journal_doi&affiliateId=springer&format=jsonp&callback=deepDyveResponse'", "            document.body.appendChild(script)", "          </script>", " </article>", " <aside class=\"buybox__institutional-sub\">", "  <div class=\"c-box__body\">", "   <div class=\"buybox__info\">", "    <p><a href=\"https://www.springernature.com/gp/librarians/licensing/license-options?&amp;abtest=v2\" data-track=\"click\" data-track-action=\"institutional link\" data-track-label=\"institutional subscriptions, new buybox\">Learn more about Institutional subscriptions</a></p>", "   </div>", "  </div>", " </aside>", " <style>.sprcom-buybox-articleSidebar{", "  box-shadow: 0px 0px 5px rgba(51,51,51,0.101);", "  display: flex;", "  flex-wrap: wrap;", "  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;", "  text-align: center;", "}", ".sprcom-buybox-articleSidebar *{", "  box-sizing: border-box;", "  line-height: calc(100% + 4px);", "  margin: 0px;", "}", ".sprcom-buybox-articleSidebar > *{", "  display: flex;", "  flex-basis: 240px;", "  flex-direction: column;", "  flex-grow: 1;", "  flex-shrink: 1;", "  margin: 0.5px;", "}", ".sprcom-buybox-articleSidebar > *{", "  box-shadow: 0 0 0 1px rgba(204,204,204,0.494);", "}", ".sprcom-buybox-articleSidebar .c-box__body{", "  display: flex;", "  flex-direction: column-reverse;", "  flex-grow: 1;", "  justify-content: space-between;", "  padding: 6%;", "}", ".sprcom-buybox-articleSidebar .c-box__body .buybox__buy{", "  display: flex;", "  flex-direction: column-reverse;", "}", ".sprcom-buybox-articleSidebar p{", "  color: #333;", "  font-size: 15px;", "}", ".sprcom-buybox-articleSidebar .buybox__price{", "  font-size: 24px;", "  font-weight: 500;", "  line-height: calc(100% + 8px);", "  margin: 20px 0;", "  order: 1;", "}", ".sprcom-buybox-articleSidebar form{", "  order: 1;", "}", ".sprcom-buybox-articleSidebar .buybox__price-info{", "  margin-bottom: 20px;", "}", ".sprcom-buybox-articleSidebar .c-box__heading{", "  background-color: #f0f0f0;", "  color: #333;", "  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;", "  font-size: 16px;", "  margin: 0px;", "  padding: 10px 12px;", "  text-align: center;", "}", ".sprcom-buybox-articleSidebar .c-box__button{", "  background-color: #3365A4;", "  border: 1px solid transparent;", "  border-radius: 2px;", "  color: #fff;", "  cursor: pointer;", "  display: inline-block;", "  font-family: inherit;", "  font-size: 16px;", "  max-width: 222px;", "  padding: 10px 12px;", "  text-decoration: none;", "  width: 100%;", "}", ".sprcom-buybox-articleSidebar h3{", "  clip: rect(1px, 1px, 1px, 1px);", "  height: 1px;", "  overflow: hidden;", "  position: absolute;", "  width: 1px;", "}", ".sprcom-buybox-articleSidebar h2{", "  flex-basis: 100%;", "  margin-bottom: 16px;", "  text-align: left;", "}", ".sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .c-box__body{", "  flex-direction: row;", "}", ".sprcom-buybox-articleSidebar .buybox__institutional-sub, .buybox__rent-article .buybox__info{", "  text-align: left;", "}", ".sprcom-buybox-articleSidebar .buybox__institutional-sub{", "  background-color: #f0f0f0;", "}", ".sprcom-buybox-articleSidebar .visually-hidden{", "  clip: rect(1px, 1px, 1px, 1px);", "  height: 1px;", "  overflow: hidden;", "  position: absolute;", "  width: 1px;", "}", ".sprcom-buybox-articleSidebar style{", "  display: none;", "}", "</style>", "</div>", "                            </div>", "                        ", "                        <div class=\"c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active\" id=\"tabpanel-sections\">", "                            <div class=\"u-mt-16\" data-component-mpu>", "    <aside class=\"c-ad c-ad--300x250\">", "        <div class=\"c-ad__inner\">", "            <p class=\"c-ad__label\">Advertisement</p>", "            <div id=\"div-gpt-ad-MPU1\" data-pa11y-ignore data-gpt data-gpt-unitpath=\"/270604982/springerlink/11714/article\" data-gpt-sizes=\"300x250\" data-gpt-targeting=\"pos=MPU1;articleid=FITEE.1900398;\"></div>", "        </div>", "    </aside>", "</div>", "                        </div>", "                        <div class=\"c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width\" id=\"tabpanel-figures\"></div>", "                        <div class=\"c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width\" id=\"tabpanel-references\"></div>", "                    </div>", "                </div>", "            </aside>", "        </div>", "    </div>", "    ", "    <script src=\"/.cdn-rum/performance.js\" async></script>", "    ", "        <script>", "            ", "        </script>", "    ", "        ", "    <footer class=\"app-footer\" role=\"contentinfo\" data-test=\"springerlink-footer\">", "        <div class=\"app-footer__aside-wrapper u-hide-print\">", "            <div class=\"app-footer__container\">", "                <p class=\"app-footer__strapline\">Over 10 million scientific documents at your fingertips</p>", "                ", "                    <div class=\"app-footer__edition\" data-component=\"SV.EditionSwitcher\">", "                        <span class=\"u-visually-hidden\" data-role=\"button-dropdown__title\" data-btn-text=\"Switch between Academic & Corporate Edition\">Switch Edition</span>", "                        <ul class=\"app-footer-edition-list\" data-role=\"button-dropdown__content\" data-test=\"footer-edition-switcher-list\">", "                            <li class=\"selected\">", "                                <a data-test=\"footer-academic-link\"", "                                   href=\"/siteEdition/link\"", "                                   id=\"siteedition-academic-link\">Academic Edition</a>", "                            </li>", "                            <li>", "                                <a data-test=\"footer-corporate-link\"", "                                   href=\"/siteEdition/rd\"", "                                   id=\"siteedition-corporate-link\">Corporate Edition</a>", "                            </li>", "                        </ul>", "                    </div>", "                ", "            </div>", "        </div>", "        <div class=\"app-footer__container\">", "            <ul class=\"app-footer__nav u-hide-print\">", "                <li><a href=\"/\">Home</a></li>", "                <li><a href=\"/impressum\">Impressum</a></li>", "                <li><a href=\"/termsandconditions\">Legal information</a></li>", "                <li><a href=\"/privacystatement\">Privacy statement</a></li>", "                <li><a href=\"https://www.springernature.com/ccpa\">California Privacy Statement</a></li>", "                <li><a href=\"/cookiepolicy\">How we use cookies</a></li>", "                ", "                <li><a class=\"optanon-toggle-display\" data-cc-action=\"preferences\" href=\"javascript:void(0);\">Manage cookies/Do not sell my data</a></li>", "                ", "                <li><a href=\"/accessibility\">Accessibility</a></li>", "                <li><a href=\"https://support.springer.com/en/support/home\">FAQ</a></li>", "                <li><a id=\"contactus-footer-link\" href=\"https://support.springer.com/en/support/solutions/articles/6000206179-contacting-us\">Contact us</a></li>", "                <li><a href=\"https://www.springer.com/gp/shop/promo/affiliate/springer-nature\">Affiliate program</a></li>", "            </ul>", "            <div class=\"c-user-metadata\">", "    ", "        <p class=\"c-user-metadata__item\">", "            <span data-test=\"footer-user-login-status\">Not logged in</span>", "            <span data-test=\"footer-user-ip\"> - 201.35.134.227</span>", "        </p>", "        <p class=\"c-user-metadata__item\" data-test=\"footer-business-partners\">", "            Not affiliated", "        </p>", "    ", "</div>", "            <a class=\"app-footer__parent-logo\" target=\"_blank\" rel=\"noopener\" href=\"//www.springernature.com\"  title=\"Go to Springer Nature\">", "                <span class=\"u-visually-hidden\">Springer Nature</span>", "                <svg width=\"125\" height=\"12\" focusable=\"false\" aria-hidden=\"true\">", "                    <image width=\"125\" height=\"12\" alt=\"Springer Nature logo\"", "                           src=/oscar-static/images/springerlink/png/springernature-60a72a849b.png", "                           xmlns:xlink=\"http://www.w3.org/1999/xlink\"", "                           xlink:href=/oscar-static/images/springerlink/svg/springernature-b88bf25ad4.svg>", "                    </image>", "                </svg>", "            </a>", "            <p class=\"app-footer__copyright\">&copy; 2021 Springer Nature Switzerland AG. Part of <a target=\"_blank\" rel=\"noopener\" href=\"//www.springernature.com\">Springer Nature</a>.</p>", "            ", "        </div>", "        ", "    <svg class=\"u-hide hide\">", "        <symbol id=\"global-icon-chevron-right\" viewBox=\"0 0 16 16\">", "            <path d=\"M7.782 7L5.3 4.518c-.393-.392-.4-1.022-.02-1.403a1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L7.782 9l1.013-.998z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"global-icon-download\" viewBox=\"0 0 16 16\">", "            <path d=\"M2 14c0-.556.449-1 1.002-1h9.996a.999.999 0 110 2H3.002A1.006 1.006 0 012 14zM9 2v6.8l2.482-2.482c.392-.392 1.022-.4 1.403-.02a1.001 1.001 0 010 1.417l-4.177 4.177a1.001 1.001 0 01-1.416 0L3.115 7.715a.991.991 0 01-.016-1.4 1 1 0 011.42.003L7 8.8V2c0-.55.444-.996 1-.996.552 0 1 .445 1 .996z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"global-icon-email\" viewBox=\"0 0 18 18\">", "            <path d=\"M1.995 2h14.01A2 2 0 0118 4.006v9.988A2 2 0 0116.005 16H1.995A2 2 0 010 13.994V4.006A2 2 0 011.995 2zM1 13.994A1 1 0 001.995 15h14.01A1 1 0 0017 13.994V4.006A1 1 0 0016.005 3H1.995A1 1 0 001 4.006zM9 11L2 7V5.557l7 4 7-4V7z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"global-icon-institution\" viewBox=\"0 0 18 18\">", "            <path d=\"M14 8a1 1 0 011 1v6h1.5a.5.5 0 01.5.5v.5h.5a.5.5 0 01.5.5V18H0v-1.5a.5.5 0 01.5-.5H1v-.5a.5.5 0 01.5-.5H3V9a1 1 0 112 0v6h8V9a1 1 0 011-1zM6 8l2 1v4l-2 1zm6 0v6l-2-1V9zM9.573.401l7.036 4.925A.92.92 0 0116.081 7H1.92a.92.92 0 01-.528-1.674L8.427.401a1 1 0 011.146 0zM9 2.441L5.345 5h7.31z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"global-icon-search\" viewBox=\"0 0 22 22\">", "            <path fill-rule=\"evenodd\" d=\"M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z\"/>", "        </symbol>", "        <symbol id=\"icon-info\" viewBox=\"0 0 18 18\">", "            <path d=\"m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"icon-success\" viewBox=\"0 0 18 18\">", "            <path d=\"M9 0a9 9 0 110 18A9 9 0 019 0zm3.486 4.982l-4.718 5.506L5.14 8.465a.991.991 0 00-1.423.133 1.06 1.06 0 00.13 1.463l3.407 2.733a1 1 0 001.387-.133l5.385-6.334a1.06 1.06 0 00-.116-1.464.991.991 0 00-1.424.119z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"icon-chevron-down\" viewBox=\"0 0 16 16\">", "            <path d=\"m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z\" fill-rule=\"evenodd\" transform=\"matrix(0 1 -1 0 11 1)\"/>", "        </symbol>", "        <symbol id=\"icon-warning\" viewBox=\"0 0 18 18\">", "            <path d=\"m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"icon-plus\" viewBox=\"0 0 16 16\">", "            <path d=\"m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z\" fill-rule=\"evenodd\"/>", "        </symbol>", "        <symbol id=\"icon-minus\" viewBox=\"0 0 16 16\">", "            <path d=\"m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z\" fill-rule=\"evenodd\"/>", "        </symbol>", "    </svg>", "    </footer>", "    </div>", "    ", "    ", "</body>", "</html>"]}