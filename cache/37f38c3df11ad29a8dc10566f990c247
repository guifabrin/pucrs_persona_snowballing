{"content": "<!DOCTYPE html>\n<html lang=\"en\" class=\"pb-page\" data-request-id=\"88e9bb19-6c8c-4dd1-aa13-adf1de87c691\"><head data-pb-dropzone=\"head\"><meta name=\"pbContext\" content=\";requestedJournal:journal:utis20;issue:issue:10.1080/utis20.v031.i03;csubtype:string:Special;page:string:Article/Chapter View;ctype:string:Journal Content;article:article:10.1080/01972243.2015.1020212;journal:journal:utis20;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;subPage:string:Full Text\" />\n<link rel=\"schema.DC\" href=\"http://purl.org/DC/elements/1.0/\" /><meta name=\"citation_journal_title\" content=\"The Information Society\" /><meta name=\"dc.Title\" content=\"Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping\" /><meta name=\"dc.Creator\" content=\"Dag Sverre  Syrdal\" /><meta name=\"dc.Creator\" content=\"Kerstin  Dautenhahn\" /><meta name=\"dc.Creator\" content=\"Kheng Lee  Koay\" /><meta name=\"dc.Creator\" content=\"Wan Ching  Ho\" /><meta name=\"dc.Subject\" content=\"assistive robotics; domestic robots; human\u2013robot interaction; prototyping\" /><meta name=\"dc.Description\" content=\"In order to investigate how the use of robots may impact everyday tasks, twelve participants in our study interacted with a University of Hertfordshire Sunflower robot over a period of 8 weeks in t...\" /><meta name=\"Description\" content=\"In order to investigate how the use of robots may impact everyday tasks, twelve participants in our study interacted with a University of Hertfordshire Sunflower robot over a period of 8 weeks in t...\" /><meta name=\"dc.Publisher\" content=\"Routledge\" /><meta name=\"dc.Date\" scheme=\"WTN8601\" content=\"13 May 2015\" /><meta name=\"dc.Type\" content=\"research-article\" /><meta name=\"dc.Format\" content=\"text/HTML\" /><meta name=\"dc.Identifier\" scheme=\"publisher-id\" content=\"1020212\" /><meta name=\"dc.Identifier\" scheme=\"doi\" content=\"10.1080/01972243.2015.1020212\" /><meta name=\"dc.Source\" content=\"https://doi.org/10.1080/01972243.2015.1020212\" /><meta name=\"dc.Language\" content=\"en\" /><meta name=\"dc.Coverage\" content=\"world\" /><meta name=\"dc.Rights\" content=\"\u00a9 2015 The Author(s). Published with license by Taylor &amp; Francis\u00a9 Dag Sverre Syrdal, Kerstin Dautenhahn, Kheng Lee Koay, and Wan Ching Ho.\" /><meta name=\"keywords\" content=\"assistive robotics,domestic robots,human\u2013robot interaction,prototyping\" /><meta name=\"citation_fulltext_world_readable\" content=\"\" />\n<link rel=\"meta\" type=\"application/atom+xml\" href=\"https://doi.org/10.1080%2F01972243.2015.1020212\" />\n<link rel=\"meta\" type=\"application/rdf+json\" href=\"https://doi.org/10.1080%2F01972243.2015.1020212\" />\n<link rel=\"meta\" type=\"application/unixref+xml\" href=\"https://doi.org/10.1080%2F01972243.2015.1020212\" />\n<title>Full article: Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</title>\n<meta charset=\"UTF-8\">\n<meta name=\"robots\" content=\"noarchive\" />\n\n<meta property=\"og:title\" content=\"Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"og:url\" content=\"https://www.tandfonline.com/doi/abs/10.1080/01972243.2015.1020212\" />\n<meta property=\"og:image\" content=\"https://www.tandfonline.com/doi/cover-img/10.1080/utis20.v031.i03\" />\n<meta property=\"og:site_name\" content=\"Taylor & Francis\" />\n<meta property=\"og:description\" content=\"(2015). Integrating Constrained Experiments in Long-Term Human&#x2013;Robot Interaction Using Task- and Scenario-Based Prototyping. The Information Society: Vol. 31, Beyond Industrial Robotics: Social Robots Entering Public and Domestic Spheres, pp. 265-283.\" />\n<meta name=\"twitter:card\" content=\"summary_large_image\">\n<meta name=\"twitter:site\" content=\"@tandfonline\">\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n<link href=\"//connect.facebook.net\" rel=\"preconnect\" />\n<link href=\"//go.taylorandfrancis.com\" rel=\"preconnect\" />\n<link href=\"//pi.pardot.com\" rel=\"preconnect\" />\n<link href=\"//static.hotjar.com\" rel=\"preconnect\" />\n<link href=\"//cdn.pbgrd.com\" rel=\"preconnect\" />\n<link href=\"//f1-eu.readspeaker.com\" rel=\"preconnect\" />\n<link href=\"//www.googleadservices.com\" rel=\"preconnect\" />\n<link href=\"https://m.addthis.com\" rel=\"preconnect\" />\n<link href=\"https://wl.figshare.com\" rel=\"preconnect\" />\n<link href=\"https://pagead2.googlesyndication.com\" rel=\"preconnect\" />\n<link href=\"https://www.googletagmanager.com\" rel=\"preconnect\" />\n<link href=\"https://www.google-analytics.com\" rel=\"preconnect\" />\n<link href=\"https://fonts.googleapis.com\" rel=\"preconnect\" />\n<link href=\"https://fonts.gstatic.com\" rel=\"preconnect\" />\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/css/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/icomoon/icomoon.woff?g276mb\" type=\"font/woff\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-regular.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link type=\"text/css\" rel=\"stylesheet\" href=\"/wro/kriw~product.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/pb/css/t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" id=\"pb-css\" data-pb-css-id=\"t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" />\n<script type=\"text/javascript\" src=\"//cdn.pbgrd.com/core-tandf.js\" async></script>\n<script data-ad-client=\"ca-pub-5143040550582507\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\" async></script>\n\n<script>\n    (function(h,o,t,j,a,r){\n        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};\n        h._hjSettings={hjid:864760,hjsv:6};\n        a=o.getElementsByTagName('head')[0];\n        r=o.createElement('script');r.async=1;\n        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;\n        a.appendChild(r);\n    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');\n</script>\n<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>\n<script type=\"text/javascript\">\n        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};\n    </script>\n<script type=\"application/javascript\" src=\"//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr\" id=\"read-speaker\" async></script>\n<script>var tandfData={\"search\":{\"cbRec\":1},\"seamlessAccess\":{\"apiUrl\":\"https://service.seamlessaccess.org/ps/\",\"context\":\"seamlessaccess.org\"},\"identity\":{\"isSpv\":false,\"isAuthenticated\":false},\"pubCount\":{\"citedCount\":7},\"actionLog\":{\"eventGroupKey\":\"34a360bb-b0c7-4bd7-9698-5d10d67fc9b7\"}};</script>\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n<link rel=\"canonical\" href=\"https://www.tandfonline.com/doi/full/10.1080/01972243.2015.1020212\" />\n</head>\n<body class=\"pb-ui\">\n\n<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>\n\n<noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\n\n\n<script type=\"text/javascript\" src=\"/wro/kriw~jquery-3.5.0.js\"></script>\n<div class=\"skipContent off-screen\"><a href=\"#top-content-scroll\" class=\"skipToContent\" title=\"Skip to Main Content\" tabIndex=\"0\">Skip to Main Content</a></div>\n<script type=\"text/javascript\">(function(e){var t=e.getElementsByClassName(\"skipToContent\");t.length>0&&(t[0].onclick=function(){var t=e.getElementById(\"top-content-scroll\");null==t&&(t=e.getElementsByClassName(\"top-content-scroll\").item(0)),t.setAttribute(\"tabindex\",\"0\"),t.focus()})})(document);</script>\n<div id=\"pb-page-content\" data-ng-non-bindable>\n<div data-pb-dropzone=\"main\" data-pb-dropzone-name=\"Main\">\n<div class=\"widget pageHeader none  widget-none  widget-compact-all\" id=\"a4d4fdd3-c594-4d68-9f06-b69b8b37ed56\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><header class=\"page-header\" aria-label=\"Main Banner\">\n<div data-pb-dropzone=\"main\">\n<div class=\"widget responsive-layout none header-top widget-none  widget-compact-all\" id=\"036fa949-dc25-4ffe-9df0-d7daefee281b\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-xs  \">\n<div class=\"col-xs-1-6 header-index\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal\" id=\"e817489e-2520-418b-a731-b62e247e74df\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\" title=\"Taylor and Francis Online\">\n<img src=\"/pb-assets/Global/tfo_logo-1444989687640.png\" alt=\"Taylor and Francis Online\" />\n</a></div>\n</div>\n</div>\n<div class=\"widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal\" id=\"b3fe8380-8b88-4558-b004-6485d3aea155\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\">\n<img src=\"/pb-assets/Global/tfo_logo_sm-1459688573210.png\" />\n</a></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-xs-5-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget layout-inline-content alignRight  widget-none  widget-compact-all\" id=\"a8a37801-55c7-4566-bdef-e4e738967e38\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">\n<div class=\"widget layout-inline-content none customLoginBar widget-none\" id=\"fbe90803-b9c8-4bef-9365-cb53cc4bfa0e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">\n\n<div class=\"widget literatumInstitutionBanner none bannerWidth widget-none\" id=\"3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"institution-image hidden-xs logout-institution-image\">\n</div></div>\n</div>\n</div>\n<div class=\"widget literatumNavigationLoginBar none  widget-none  widget-compact-all\" id=\"1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"loginBar not-logged-in\">\n<span class=\"icon-user\"></span>\n<a href=\"/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F01972243.2015.1020212\" class=\"sign-in-link\">\nLog in\n</a>\n<span class=\"loginSeprator\">&nbsp;|&nbsp;</span>\n<a href=\"/action/registration?redirectUri=%2F\" class=\"register-link\">\nRegister\n</a>\n</div></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n\n<div class=\"widget eCommerceCartIndicatorWidget none literatumCartLink widget-none\" id=\"9de10bb5-08af-48bc-b9f6-3f6433229f3e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><a href=\"/action/showCart?FlowID=1\" class=\"cartLabel\">\n<span class=\"hidden-xs hidden-sm visible-tl-inline-block\">Cart</span>\n<span class=\"cartItems\" data-id=\"cart-size\" role=\"status\">\n</span>\n</a></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none breadcrumbs-container widget-none  widget-compact-all\" id=\"64c16283-4b04-4d90-ac0f-4db85fcd0cf5\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget literatumBreadcrumbs none breadcrumbs-widget widget-none  widget-compact-all\" id=\"b1c121c1-cbbd-4241-8774-7120f3a783e8\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\">\n<nav aria-label=\"Breadcrumb\">\n<ol class=\"breadcrumbs\">\n<li class=\"\">\n<a href=\"/\" class=\"bc-click\">\nHome\n</a>\n</li>\n<li class=\"\">\n<a href=\"/action/showPublications?pubType=journal\" class=\"bc-click\">\nAll Journals\n</a>\n</li>\n<li class=\"\">\n<a href=\"/toc/utis20/current\" class=\"bc-click\">\nThe Information Society\n</a>\n</li>\n<li class=\"\">\n<a href=\"/loi/utis20\" class=\"bc-click\">\n List of Issues\n</a>\n</li>\n<li class=\"\">\n<a href=\"/toc/utis20/31/3\" class=\"bc-click\">\nVolume 31, Issue 3\n</a>\n</li>\n<li class=\"\">\n<a href=\"#\" class=\"bc-click\" aria-current=\"page\">\nIntegrating Constrained Experiments in L ....\n</a>\n</li>\n</ol>\n</nav>\n<script type=\"application/ld+json\">\n    {\n        \"@context\": \"https://schema.org\",\n        \"@type\": \"BreadcrumbList\",\n        \"itemListElement\":\n        [{\n            \"@type\": \"ListItem\",\n            \"position\": \"1\",\n            \"name\": \"Home\"\n            ,\"item\": \"https://www.tandfonline.com/\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"2\",\n            \"name\": \"All Journals\"\n            ,\"item\": \"https://www.tandfonline.com/action/showPublications?pubType=journal\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"3\",\n            \"name\": \"The Information Society\"\n            ,\"item\": \"https://www.tandfonline.com/toc/utis20/current\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"4\",\n            \"name\": \"List of Issues\"\n            ,\"item\": \"https://www.tandfonline.com/loi/utis20\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"5\",\n            \"name\": \"Volume 31, Issue 3\"\n            ,\"item\": \"https://www.tandfonline.com/toc/utis20/31/3\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"6\",\n            \"name\": \"Integrating Constrained Experiments in L .... \"\n            \n        }\n        ]\n    }\n</script></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</header></div>\n</div>\n</div>\n<div data-widget-def=\"pageBody\" data-widget-id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\" role=\"main\">\n<div class=\"widget pageBody none  widget-none  widget-compact-all\" id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\">\n<div class=\"page-body pagefulltext\">\n<div data-pb-dropzone=\"main\">\n<div class=\"widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all\" id=\"1728e801-36cd-4288-9f53-392bad29506a\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md gutterless \">\n<div class=\"col-md-5-12 search_container \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget quickSearchWidget none search-customize-width widget-none  widget-compact-all\" id=\"d46e3260-1f5c-4802-821a-28a03a699c82\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"quickSearchFormContainer \">\n<form action=\"/action/doSearch\" name=\"quickSearch\" class=\"quickSearchForm \" title=\"Quick Search\" role=\"search\" method=\"get\" onsubmit=\"appendSearchFilters(this)\" aria-label=\"Quick Search\"><span class=\"simpleSearchBoxContainer\">\n<input name=\"AllField\" class=\"searchText main-search-field autocomplete\" value=\"\" type=\"search\" id=\"searchText\" title=\"Type search term here\" aria-label=\"Search\" placeholder=\"Enter keywords, authors, DOI, ORCID etc\" autocomplete=\"off\" data-history-items-conf=\"3\" data-publication-titles-conf=\"3\" data-publication-items-conf=\"3\" data-topics-conf=\"3\" data-contributors-conf=\"3\" data-fuzzy-suggester=\"false\" data-auto-complete-target=\"title-auto-complete\" />\n</span>\n<span class=\"searchDropDownDivRight\">\n<label for=\"searchInSelector\" class=\"visuallyhidden\">Search in:</label>\n<select id=\"searchInSelector\" name=\"SeriesKey\" class=\"js__searchInSelector\">\n<option value=\"utis20\" id=\"thisJournal\" data-search-in=\"thisJournal\">\nThis Journal\n</option>\n<option value=\"\" data-search-in=\"default\">\nAnywhere\n</option>\n</select>\n</span>\n<span class=\"quick-search-btn\">\n<input class=\"mainSearchButton searchButtons pointer\" title=\"Search\" role=\"button\" type=\"submit\" value=\"\" aria-label=\"Search\" />\n</span></form>\n</div>\n<div class=\"advancedSearchLinkDropZone\" data-pb-dropzone=\"advancedSearchLinkDropZone\">\n<div class=\"widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all\" id=\"323e2a31-1c81-4995-bd17-8e149458c214\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><a href=\"/search/advanced\" class=\"advSearchArticle\">Advanced search</a></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-7-12 serNav_container\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget literatumSeriesNavigation none  widget-none\" id=\"7730bfe1-9fca-4cf4-a6d6-2a0148105437\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"issueSerialNavigation journal\">\n<div class=\"cover\">\n<img src=\"/action/showCoverImage?doi=10.1080/utis20.v031.i03\" alt=\"Publication Cover\" width=\"90\" height=\"120\" />\n</div>\n<div class=\"info trim-spaces\">\n<div class=\"title-container\">\n<h1 class=\"journal-heading\">\n<a href=\"/toc/utis20/current\">\nThe Information Society\n</a>\n</h1>\n<div class=\"subtitle\">\n<span>\nAn International Journal\n</span>\n</div>\n<span class=\"issue-heading\">\nVolume 31, 2015 - <a href=\"/toc/utis20/31/3\" class=\"nav-toc-list\">Issue 3</a><span class=\"specialTitle\"><a href=\"/toc/utis20/31/3\">: Beyond Industrial Robotics: Social Robots Entering Public and Domestic Spheres</a>\n</span>\n</span>\n</div>\n<div class=\"seriesNavDropZone\" data-pb-dropzone=\"seriesNavDropZone\">\n<div class=\"widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal\" id=\"753455df-1eeb-47ca-bdc9-e19022075973\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"serial-action\">\n<a href=\"https://rp.tandfonline.com/submission/create?journalCode&#x3D;UTIS\" class=\"green submitAnArticle\"><span>Submit an article</span></a>\n<a href=\"/toc/utis20/current\" class=\"jHomepage\"><span>Journal homepage</span></a>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none  widget-none  widget-compact-vertical\" id=\"e42aea8f-434a-4d39-aaef-f56af3ff00dc\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all\" id=\"6aacf107-e82d-494d-a14c-0c00bba52560\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"accessLogo\">\n<div>\n<img class=\"accessIconLocation\" src=\"/pb-assets/3rdPartyLogos/accessOA-1452596421933.png\" alt=\"Open access\" />\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n\n<div class=\"widget responsive-layout none publicationContentHeader widget-none  widget-compact-all\" id=\"63f402e4-3498-4709-8d7d-ee8e69f93467\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical\" id=\"5afd8b6d-7e09-43ff-8ad6-afa3764e543c\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"articleMetricsContainer\">\n<div class=\"content compactView\">\n<div class=\"section\">\n<div class=\"value\">\n1,437\n</div>\n<div class=\"title\">\nViews\n</div>\n</div>\n<div class=\"section\">\n<div class=\"value\">\n7\n</div>\n<div class=\"title\">\nCrossRef citations to date\n</div>\n</div>\n<div class=\"section score\">\n<div class=\"altmetric-score true\">\n<div class=\"value\" data-doi=\"10.1080/01972243.2015.1020212\">\n<span class=\"metrics-score\">0</span>\n</div>\n<div class=\"title\">\nAltmetric\n</div>\n</div>\n</div>\n<script>tandfData.altmetric={key:'be0ef6915d1b2200a248b7195d01ef22'}</script>\n<script src=\"/wro/kriw~altmetric.js\" async></script>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-2-3 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n\n<div class=\"widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all\" id=\"fa57727f-b942-4eb8-9ed2-ecfe11ac03f5\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div id=\"read-speaker-container\" style=\"display: block; height: 40px\">\n<div id=\"readspeaker_button1\" class=\"rs_skip rsbtn rs_preserve\">\n<a href=\"//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F01972243.2015.1020212\" rel=\"nofollow\" class=\"rsbtn_play\" accesskey=\"L\" title=\"Listen to this page using ReadSpeaker webReader\" style=\"border-radius: 0 11.4px 11.4px 2px;\">\n<span class=\"rsbtn_left rsimg rspart\"><span class=\"rsbtn_text\"><span>Listen</span></span></span>\n<span class=\"rsbtn_right rsimg rsplay rspart\"></span>\n</a>\n</div>\n</div>\n<div class=\"toc-heading\">ARTICLES</div>\n<h1><span class=\"NLM_article-title hlFld-title\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span></h1><div class=\"literatumAuthors\"><div class=\"publicationContentAuthors\"><div class=\"hlFld-ContribAuthor\"><span class=\"NLM_contrib-group\"><span class=\"contribDegrees corresponding \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Syrdal%2C+Dag+Sverre\">Dag Sverre Syrdal<i class=\"fa fa-envelope\" aria-hidden=\"true\" style=\"margin-left: 0.5em;\"></i></a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom<span class=\"corr-sec\"><span class=\"heading\">Correspondence</span><span class=\"corr-email\"><i class=\"fa fa-envelope\" style=\"color: #10147E; padding-right: 7px\" aria-hidden=\"true\"></i><a href=\"mailto:d.s.syrdal@herts.ac.uk\">d.s.syrdal@herts.ac.uk</a></span><br /></span></span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Dautenhahn%2C+Kerstin\">Kerstin Dautenhahn</a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Koay%2C+Kheng+Lee\">Kheng Lee Koay</a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom</span></div> &amp; </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Ho%2C+Wan+Ching\">Wan Ching Ho</a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom</span></div></span></span></div></div></div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none  widget-none  widget-compact-all\" id=\"5f562208-b1d5-4e5a-81c7-356431240f04\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">\n<div class=\"row row-md gutterless \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget layout-inline-content none  widget-none  widget-compact-all\" id=\"87ac5840-18fa-4a14-8eca-065b90ede3d7\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">\n<div class=\"widget literatumContentItemPageRange none  widget-none  widget-compact-all\" id=\"45057865-d60c-414c-bc81-646debb621b0\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><span class=\"contentItemPageRange\">Pages 265-283\n</span></div>\n</div>\n</div>\n<div class=\"widget literatumContentItemHistory none  widget-none  widget-compact-all\" id=\"32bf868e-52ce-411a-9dc3-717743aad997\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div>Received 28 Dec 2013</div><div>Accepted 15 Aug 2014</div><div>Published online: 13 May 2015</div></div>\n</div>\n</div>\n<div class=\"widget literatumArticleToolsWidget none  widget-none  widget-compact-all\" id=\"ed673666-7b5d-470e-bd33-c5c679d996cb\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"articleTools\">\n<ul class=\"linkList\">\n<li class=\"downloadCitations\">\n<a href=\"/action/showCitFormats?doi=10.1080%2F01972243.2015.1020212\"><i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>Download citation</a>\n</li>\n\n<li class=\"dx-doi\">\n<a href=\"https://doi.org/10.1080/01972243.2015.1020212\"><i class=\"fa fa-external-link-square\" style=\"margin: 0 0.25rem 0 0\" aria-hidden=\"true\"></i>https://doi.org/10.1080/01972243.2015.1020212</a>\n</li>\n<script src=\"/wro/kriw~crossmark.js\" async></script>\n<li class=\"cross_mark\">\n<a class=\"cross_mark--link\" data-doi=\"10.1080/01972243.2015.1020212\" data-target=\"crossmark\" href=\"#\">\n<img src=\"/templates/jsp/images/CROSSMARK_Color_horizontal.svg\" alt=\"CrossMark Logo\" width=\"100\" height=\"22px\" />\n<span aria-describedby=\"crossMark-description\"><span class=\"off-screen\" id=\"crossMark-description\">CrossMark</span></span>\n</a>\n</li>\n</ul>\n</div></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents2\">\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none publicationContentBody widget-none\" id=\"f4a74f7a-9ba2-4605-86b1-8094cb1f01de\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget sectionsNavigation none  widget-none\" id=\"f15bd2de-bb18-4067-8ab9-03ea3be30bf7\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"sections-nav\" role=\"navigation\" aria-label=\"Article Navigation\"><span class=\"title\">In this article<a href=\"#\" class=\"close\" tabindex=\"-1\"><span aria-label=\"close button for mobile\"><span class=\"off-screen\" id=\"close-description\">Close</span></span></a></span><ul class=\"sections-list\"><li><span class=\"sub-art-heading\"><a href=\"#_i2\">PROTOTYPING OF HUMAN\u2013ROBOT INTERACTION</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i3\">UNIVERSITY OF HERTFORDSHIRE ROBOT HOUSE</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i4\">CONSTRUCTED PERSONAS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i5\">OPEN-ENDED SCENARIOS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i8\">CONSTRAINED EXPERIMENTS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i14\">RESEARCH QUESTIONS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i18\">METHODOLOGY</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i30\">RESULTS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i45\">DISCUSSION</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i51\">CONCLUSIONS</a></span><ul class=\"sub-art-titles\"></ul></li><li><a href=\"#ack\">Acknowledgements</a></li><li><a href=\"#references-Section\">References</a></li></ul></div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-7-12 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget responsive-layout none rs_readArea widget-none  widget-compact-all\" id=\"9751b4f9-64b9-44c0-955b-f75246902839\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all\" id=\"d29f04e9-776c-4996-a0d8-931023161e00\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n    MathJax.Hub.Config({\n        \"HTML-CSS\": {scale: 70, linebreaks: {automatic: true, width: \"container\"}},\n        SVG: {linebreaks: {automatic: true, width: \"25%\"}},\n        menuSettings: {zoom: \"Click\"},\n\n        /* This is necessary to lazy loading. */\n        skipStartupTypeset: true\n    });\n</script>\n<script type=\"application/javascript\" async src=\"/wro/kriw~mathjax.js\"></script>\n<div class=\"articleMeta ja\">\n<div class=\"tocHeading\">\n<h2>ARTICLES</h2>\n</div>\n<div class=\"hlFld-Title\">\n<div class=\"publicationContentTitle\">\n<h1 class=\"chaptertitle\">\nIntegrating Constrained Experiments in Long-Term Human&#x2013;Robot Interaction Using Task- and Scenario-Based Prototyping\n</h1>\n</div>\n</div>\n<div class=\"copyrightStatement\">\n</div>\n<div class=\"articleMetaDrop publicationContentDropZone\" data-pb-dropzone=\"articleMetaDropZone\">\n</div>\n<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone1\" data-pb-dropzone=\"articleMetaDropZone1\">\n</div>\n<div class=\"copyrightline\">\n</div>\n<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone2\" data-pb-dropzone=\"articleMetaDropZone2\">\n</div>\n</div>\n<div class=\"publication-tabs ja publication-tabs-dropdown\">\n<div class=\"tabs tabs-widget\">\n<ul class=\"tab-nav\" role=\"tablist\">\n<li class=\"active\" role=\"tab\">\n<a href=\"/doi/full/10.1080/01972243.2015.1020212?scroll=top&amp;needAccess=true\" class=\"show-full\">\n<i class=\"fa fa-file-text\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">\nFull Article\n</span>\n</a>\n</li>\n<li role=\"tab\">\n<a href=\"/doi/figure/10.1080/01972243.2015.1020212?scroll=top&amp;needAccess=true\" class=\"show-figure\">\n<i class=\"fa fa-image\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">Figures & data</span>\n</a>\n</li>\n<li role=\"tab\">\n<a href=\"/doi/ref/10.1080/01972243.2015.1020212?scroll=top\" class=\"show-references\">\n<i class=\"fa fa-book\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">References</span>\n</a>\n</li>\n<li class=\"citedbyTab \" role=\"tab\">\n<a href=\"/doi/citedby/10.1080/01972243.2015.1020212?scroll=top&amp;needAccess=true\">\n<i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">\nCitations\n</span>\n</a>\n</li>\n<li role=\"tab\" class=\"metrics-tab\">\n<a href=\"#metrics-content\" class=\"show-metrics\">\n<i class=\"fa fa-bar-chart\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">Metrics</span>\n</a>\n</li>\n<li role=\"tab\" class=\"licencing-tab \">\n<a href=\"/action/showCopyRight?scroll=top&amp;doi=10.1080%2F01972243.2015.1020212\" class=\"show-copyright\">\n<i class=\"fa fa-copyright\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">Licensing</span>\n</a>\n</li>\n<li role=\"tab\" class=\"permissions-tab \">\n<a href=\"/doi/abs/10.1080/01972243.2015.1020212?tab=permissions&amp;scroll=top\" class=\"show-permissions\">\n<i class=\"fa fa-print\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">\nReprints & Permissions</span></a>\n</li>\n<li class=\"pdf-tab \" role=\"tab\">\n<a href=\"/doi/pdf/10.1080/01972243.2015.1020212?needAccess=true\" class=\"show-pdf\" role=\"button\" target=\"_blank\">\n<span class=\"nav-data\">\nPDF\n</span>\n</a>\n</li>\n</ul>\n<div class=\"tab-content \">\n<a id=\"top-content-scroll\"></a>\n<div class=\"tab tab-pane active\">\n<article class=\"article\">\n<p class=\"fulltext\"></p><div class=\"hlFld-Abstract\"><p class=\"fulltext\"></p><div class=\"sectionInfo abstractSectionHeading\"><h2 id=\"abstract\" class=\"section-heading-2\">Abstract</h2></div><div class=\"abstractSection abstractInFull\"><p>In order to investigate how the use of robots may impact everyday tasks, twelve participants in our study interacted with a University of Hertfordshire Sunflower robot over a period of 8 weeks in the university's Robot House. Participants performed two constrained tasks, one physical and one cognitive, four times over this period. Participant responses were recorded using a variety of measures including the System Usability Scale and the NASA Task Load Index. The use of the robot had an impact on the experienced workload of the participants di\ufb00erently for the two tasks, and this e\ufb00ect changed over time. In the physical task, there was evidence of adaptation to the robot's behavior. For the cognitive task, the use of the robot was experienced as more frustrating in the later weeks.</p></div></div><div class=\"abstractKeywords\"><div class=\"hlFld-KeywordText\"><div><p class=\"kwd-title\" aria-label=\"Keywords\">Keywords: </p><a href=\"/keyword/Assistive+Robotics\" class=\"kwd-btn keyword-click\" role=\"button\">assistive robotics</a><a href=\"/keyword/Domestic+Robots\" class=\"kwd-btn keyword-click\" role=\"button\">domestic robots</a><a href=\"/keyword/Human%E2%80%93robot+Interaction\" class=\"kwd-btn keyword-click\" role=\"button\">human\u2013robot interaction</a><a href=\"/keyword/Prototyping\" class=\"kwd-btn keyword-click\" role=\"button\">prototyping</a></div></div></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneAbs\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div></div><div class=\"hlFld-Fulltext\"><div id=\"s0001\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><p>In the field of human\u2013robot interaction, domestic, human-centered environments present serious challenges for prototyping human\u2013machine interactions. In particular, when addressing future and emergent technologies, it is a challenge to enable interactions that are situated in such a way that they are meaningful to the user, and allow users to translate this experience to their everyday life. Moreover, the experience of such interactions is subjective, and the relationship between interactants, technologies, and situations can be complex and dynamic (Buchenau and Suri <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0006\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2000</a></span>). On the technical side, cutting-edge technologies often do not have the stability required to function autonomously in an e\ufb00ective and safe manner for sustained periods of time outside of highly constrained settings. However, such feedback is critical for guiding the development of these technologies. This necessitates a high degree of pragmatism and creativity when developing appropriate methodologies for examining how prospective users interact with these technologies, and how these interactions may benefit or hinder the user (Dautenhahn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0012\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>).</p><p>While there have been studies of actual robots acting autonomously in a domestic environment without continuous oversight by experimenters, either the robots employed have had limited movement capabilities, and served mainly as physically embodied conversational agents (not unlike those described in Bickmore and Cassell <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0003\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>) as in the KSERA project (Payr <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0037\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>), or the robots were market-ready products (Fernaeus et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0017\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>; Sung et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0044\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>) or at a late stage in the development cycle (Kidd and Breazeal <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0025\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>). Furthermore, due to the cost in time and resources to set up and run the experiments, live interactions with robotic technologies in complex usage scenarios usually involve only a relatively small number of participants (Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>; Huijnen et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0022\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>). While it is often desirable to run studies with the largest number of participants possible for greater generalizability, there is also the need for studies that allow for a wide range of interactions to capture data on human\u2013robot interaction in all its richness. This balance lies at the heart of our e\ufb00orts to develop, adapt, and use prototyping methodologies for domestic human\u2013robot interaction (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0050\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>).</p></div><div id=\"s0002\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i2\" class=\"section-heading-2\">PROTOTYPING OF HUMAN\u2013ROBOT INTERACTION</h2><p>Broadly, there are two different approaches to prototyping of human\u2013robot interaction. The first one is a holistic, scenario-based approach (Carroll <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0007\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2000</a></span>), which takes a high-level view of the situations and tries to capture the experience of the interaction through narratives. Here the participants\u2019 interactions with the robot are framed within a narrative that allows them to evaluate the potential impact of the prototype in everyday life situations. These scenarios can be presented to the participants as written stories (Blythe and Wright <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0005\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>), videos (Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>; Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0048\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>), theater performances (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0047\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>; Chatley et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0009\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>; Newell et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0035\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>), or live human\u2013robot interactions (Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0030\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>). The second approach is more reductionist and condenses and abstracts the salient features of the interaction into a controlled experimental setup. This approach has been used successfully for studying human\u2013robot proxemics (Tapus et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0051\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0027\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>; Dautenhahn et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0013\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>), specific robot behavior styles (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0046\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>; Fussell et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0018\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Bartneck et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0002\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>), and di\ufb00erent user groups.</p><p>These two approaches are not mutually exclusive. For instance, Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2011)</a></span> combined a high-level narrative with a highly constrained experimental manipulation in a video study. However, each has clear strengths and weaknesses when compared to the other. The narrative approach provides insights into how robotic technologies may impact on people's lives on a more conceptual level. It does not, however give the participant the clear ability to experience and di\ufb00erentiate between the ways that the particularities of a robot's behavior or characteristics impact specific interactions. Highly controlled, experimental studies, on the other hand, are often lacking in ecological validity, but allow for in-depth understanding of specific aspects of the interaction.</p><p>The study presented here fruitfully brought together both approaches: The controlled experiments were integrated with open-ended scenarios as part of a long-term study (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0045\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>). These studies were conducted in the University of Hertfordshire Robot House.</p></div><div id=\"s0003\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i3\" class=\"section-heading-2\">UNIVERSITY OF HERTFORDSHIRE ROBOT HOUSE</h2><p>The UH Robot House is a residential house, near the University of Hertfordshire campus, that has been adapted for human\u2013robot interaction studies. It has been augmented into a \u201csmart home\u201d with low-cost, resource-e\ufb03cient sensor systems that inform the robots about user activities and other events in the environment (Duque et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0016\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>). Moreover, it offers ecological validity because it is a real working house, with kitchen appliances, a TV, a doorbell, and so on. Throughout the studies presented here, participants primarily used the living room, dining area, and kitchen, sometimes responding to events (visitors, deliveries, etc.) at the front door, with an extra room used in the briefing for the open-ended scenario. In general, the Robot House serves as an e\ufb00ective test bed for prototyping domestic human\u2013robot interactions. Its infrastructure supports interactions with a range of robots such as the UH Sunflower robot (Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0026\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>), PeopleBots (Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>), and the IPA Care-O-Bot 3 (Parlitz et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0036\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0028\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>).</p></div><div id=\"s0004\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i4\" class=\"section-heading-2\">CONSTRUCTED PERSONAS</h2><p>Personas are understood in human\u2013computer interaction as fictional yet highly realized users of a given technology (Chang et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0008\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>). By creating and extrapolating behaviors, goals, histories, and characteristics of these, it is possible to tightly focus the technological development. The specific personas used to guide the scenario development in the Robot House were a couple in their mid-to-late sixties. The personas were given work, interests, and health issues, which are summarized next.</p><p><i>The Husband (David)</i> is recently retired from a white-collar profession. He is looking forward to spend some time focusing on his hobbies, which include reading, watching documentaries, and building military models. He has a heart condition, which requires him to take medication regularly. For some reason, he often forgets to take this medication and has to be reminded by his wife daily. He also has a condition (likely arthritis in the knees) that gives him some mobility issues.</p><p><i>The Wife (Judy)</i> works from home most days. Her husband's recent retirement and associated distractions are causing her some stress, and the couple some tension. She normally stays in her home o\ufb03ce almost exclusively during her working hours, interacting with David primarily at mealtimes. She is used to computing technology, relying on it to work e\ufb00ectively from her home o\ufb03ce. This has also enabled her and David to maintain close contact (using Skype and other social media) with their children and grandchildren.</p><p>Based on the lives of these personas, we created a \u201ctypical\u201d day comprised of episodes in which the robot was utilized to aid \u201cJudy\u201d and \u201cDavid\u201d in their daily activities. See <a href=\"#f0001 f0002\">Figures\u00a01 and 2</a> for episodes from a scenario based on a \u201ctypical\u201d day for the user personas. The evaluation scenarios were created by examining the possible roles that the robot could play in the di\ufb00erent episodes that comprised a \u201ctypical\u201d day for the two user personas. This was done both as high-level narrative-based interactions, which presented scenarios where the interaction with the robot was within a specific context for the participants, and through constrained and experimental examinations of the role of the robot within specific tasks.</p></div><div id=\"s0005\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i5\" class=\"section-heading-2\">OPEN-ENDED SCENARIOS</h2><p>The open-ended scenarios sought to convey the impact of the agent within a wider context to the participants in an evaluation study. To achieve this, two open-ended scenarios were created. They were inspired by the Persona Scenarios (as shown in <a href=\"#f0001\">Figure\u00a01</a>) but di\ufb00ered in that they were intended for a single user, and would be meaningful to an experimental participant within the context of a 1-hour duration interaction (for long-term studies a duration of 1\u00a0hour maximum for each session was considered appropriate in order to avoid fatiguing the participants). The scenarios were grounded in an imagined daily life, with the robot adopting an assistive role: allowing the participants to inform the robot about their preferences in terms of drinks, snacks, leisure activities, and TV programs that they preferred. These elements were used in individual episodes whereby each scenario was performed twice during the long-term studies, according to the schedule shown in <a class=\"ref showTableEventRef\" data-ID=\"t0001\">Table\u00a01</a>. In these episodes, the participants were asked to engage in a structured role-play-like scenario (Seland <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0041\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>) in order to investigate the role of the robot in a manner that could be directly related to the participants\u2019 everyday experience. Therefore, they could directly experience the impact of the robot. These scenarios also investigated particular issues that were of interest to our research, such as human and robot communication and \u201cagent migration\u201d (see explanation in the following). <div class=\"figure figureViewer\" id=\"f0001\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 1. </span> Episode from a \u201cNormal Day\u201d for the user personas (1).</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0001image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0001_oc.jpg\"}' width=\"383\" height=\"286\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0001\"><p><span class=\"captionLabel\">FIG. 1. </span> Episode from a \u201cNormal Day\u201d for the user personas (1).</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0001\"></div> <div class=\"figure figureViewer\" id=\"f0002\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 2. </span> Episode from a \u201cNormal Day\u201d for the user personas (2).</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0002image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0002_oc.jpg\"}' width=\"439\" height=\"500\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0002\"><p><span class=\"captionLabel\">FIG. 2. </span> Episode from a \u201cNormal Day\u201d for the user personas (2).</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0002\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 1 </span> Overview of sessions</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0001-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0001&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0001\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>These scenarios were based around episodes in two \u201cimaginary\u201d days and were intended to investigate interactions with and responses to the robot in an everyday setting. The first episode took place in the \u201cmorning\u201d and focused on the expressive capabilities of the Sunflower robot. The second episode was set during the \u201cafternoon\u201d and was focused on the participants\u2019 impression of agent migration\u2014the ability of an agent's \u201cmind\u201d to move between di\ufb00erent robot and virtual embodiments (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0046\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>; Du\ufb00y et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0015\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2003</a></span>). Here, the agent's \u201cmind\u201d comprises its memory, its interaction history, and a sense of context; for example, it can remember the user's preferences while moving between di\ufb00erent embodiments, and can continue tasks begun in one embodiment within another. This allows the agent to take advantage of features and functionalities of more than one embodiment while maintaining the persistent features that make it unique and recognizable from a user's perspective. These attributes include awareness of interaction history and context, as well as persistent customizable features. In the scenario, the migration took place between a Sunflower and a SONY Aibo robot. For both of these scenarios, participants were briefed as to the time of day and the particulars of the situation they were going to take part in (Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0029\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>).</p></div><div id=\"s0006\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i8\" class=\"section-heading-2\">CONSTRAINED EXPERIMENTS</h2><div id=\"s0006-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i9\">Cognitive Prosthetic</h3><p>The scenarios identified several instances in which the robot companion would be able to assist the user by providing information. This information could be provided in the form of reminders of appointments, mealtimes, and medicines. In the chosen scenario the robot's task was to remind \u201cDavid\u201d to take his heart medication.</p><p>Adherence to a prescribed regimen of medication can be difficult for many patients. Early approaches (as exemplified by Schwartz et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0040\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1962</a></span>) presented this as being caused by a shortfall in the ability of the patient, who was seen as making mistakes. More recent approaches consider a wider range of reasons for nonadherence to prescribed medicine regimens. In addition to the cognitive abilities of the patient, the new approaches also take into account other factors such as the complexity of the medication schedule, perceived e\ufb03cacy of the treatment, and perceived risk of side e\ufb00ects (Horne et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0021\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>).</p><p>While this particular scenario used the robot purely to remind the user of his schedule in a manner similar to that of cognitive prosthetics on hand-held platforms (Modayil et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0034\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>), this functionality can also be combined with more persuasive technologies that use relational and other strategies in order to encourage habits conducive to the health of the user (Bickmore et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0004\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>). However, this was not the focus of the current study, which focused purely on the cognitive prosthetic aspect of such technologies and its impact within the performance of a task.</p><p>The experimental instantiation of the Cognitive Prosthetic task involved participants putting Scrabble tiles into the correct spaces of a medicine dispenser on the living room table (see <a href=\"#f0005\">Figure\u00a05</a>, shown later), relying on a master list that had to remain on the kitchen bench. There were 28 spaces for the tiles, and both the position of the tiles in the dispenser and their position on the list in the kitchen were randomized. <div class=\"figure figureViewer\" id=\"f0003\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 3. </span> The Sunflower robot used in this study. The robot was built at the University of Hertfordshire, significantly extending a basic Pioneer Platform.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0003image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0003_oc.jpg\"}' width=\"500\" height=\"437\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0003\"><p><span class=\"captionLabel\">FIG. 3. </span> The Sunflower robot used in this study. The robot was built at the University of Hertfordshire, significantly extending a basic Pioneer Platform.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0003\"></div> <div class=\"figure figureViewer\" id=\"f0004\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 4. </span> Interacting with the touch-screen interface on the Sunflower robot.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0004image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0004_oc.jpg\"}' width=\"383\" height=\"256\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0004\"><p><span class=\"captionLabel\">FIG. 4. </span> Interacting with the touch-screen interface on the Sunflower robot.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0004\"></div> <div class=\"figure figureViewer\" id=\"f0005\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 5. </span> Medicine dispenser and Scrabble tiles used in the Cognitive Prosthetic task as part of the controlled experiments.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0005image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0005_oc.jpg\"}' width=\"383\" height=\"353\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0005\"><p><span class=\"captionLabel\">FIG. 5. </span> Medicine dispenser and Scrabble tiles used in the Cognitive Prosthetic task as part of the controlled experiments.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0005\"></div> </p></div><div id=\"s0006-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i13\">Fetch and Carry</h3><p>The Fetch and Carry task involved the carrying of objects between di\ufb00erent rooms. This task was performed during episodes such as mealtimes, where the robot could assist with the movement of prepared food from the kitchen to the dining area and returning of dishes to the kitchen. It was also considered to be of utility in the episodes where \u201cDavid\u201d could use it while engaging in his hobby, for example, to move models and tools from storage to a work surface in a di\ufb00erent room.</p><p>The term Fetch and Carry comes from H\u00fcttenrauch and Eklundh <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0023\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2002)</a></span>, who in their case study describe how a user with partial mobility impairment uses a mobile robot as a platform for transporting objects that this person would otherwise be unable to move without assistance from another person. This particular task is interesting due to both the utility of the task and the human\u2013robot interaction issues that it highlights.</p><p>The Fetch and Carry capability of robots can be of use to a wide variety of users because there are many reasons why they may need assistance for transporting objects, ranging from fall injuries to neurodegenerative conditions like Parkinson's (Kamsma et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0024\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1995</a></span>; Walker and Howland <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0052\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1991</a></span>). It is also an interesting task from a human\u2013robot interaction perspective, as it is unique to the physical nature of robots and involves both human and robot interactants negotiating and moving in a shared physical space. As long as the robot is capable of moving between two or more points and is fitted with a suitable container for the transport of objects, a robust and stable realization of this task is well within the current state of the art. For a product prototype implementation for this task, see the Danish Technological Institute (DTI) robot-butler \u201cJames\u201d (Danish Technological Institute <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0011\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2012</a></span>).</p><p>The experimental instantiation of the Fetch and Carry task involved the participants moving 100 plastic balls from a net on the kitchen bench to the living room table using only one hand. This was a constraint that was easily implemented while being challenging to the participants. While the balls were very light, requiring little physical strength, they were quite unwieldy in numbers larger than four or five, so required several trips back and forth to transport them all.</p><p>Assistance as envisaged with the Cognitive Prosthetic and Fetch and Carry tasks can be used in response to changed circumstances, such as recovery from illness and accidents, as well as rehabilitation after strokes, where the prospective user will have to learn new skills to aid in daily living, or gradually recover mastery of old skills.</p><p>For the experimental instantiation of both these tasks, we decided to choose tasks that, while not strenuous, would present a challenge to the participants, and in which the use of a robot would have a clear impact on the task. In addition, it was hoped that the experimental constraints would add novelty to the task, allowing us to see the impact of changes in participant task mastery.</p></div></div><div id=\"s0007\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i14\" class=\"section-heading-2\">RESEARCH QUESTIONS</h2><div id=\"s0007-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i15\">Di\ufb00erentiation of Tasks on the NASA TLX</h3><p>The first research question was whether or not we could di\ufb00erentiate between the tasks using their NASA Task Load Index (TLX), a measure for di\ufb00erent types of workload that is described in more detail in the methodology section. It was expected that the two tasks would load more strongly on their \u201cprimary\u201d dimensions (namely, Fetch and Carry along the Physical Dimension, and Cognitive Prosthetic along the Mental Dimension). It was also of interest to see whether these tasks changed over time (i.e., whether practice changed the nature of the tasks in terms of experienced workload).</p><p>Research Question 1: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">How did the two tasks differ from each other in terms of experienced workload at the initial presentation?</p></li><li><p class=\"inline\">How did the experience of the tasks change over time?</p></li></ol></p></div><div id=\"s0007-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i16\">Impact of the Robot</h3><p>We were also interested in how the use of the robot would alter the perceived workload of the two tasks, and how this impact changed over time. While we expected the use of the robot to impact the di\ufb00erent tasks along their primary dimensions by reducing participants\u2019 workload in the initial interactions with the robot as an aid, we were also interested in how the robot impacted the workload on these tasks along the other dimensions.</p><p>Research Question 2: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">How did the robot impact the experienced workload on the tasks along the different dimensions of the NASA TLX?</p></li><li><p class=\"inline\">How did the impact of the robot change over time?</p></li></ol></p></div><div id=\"s0007-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i17\">The Experience of the Task and the Robot</h3><p>Our final interest was in how participants reasoned about the tasks, and how they described the tasks in terms of what contributed to their workload and their experience of the robot's assistance.</p><p>Research Question 3: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">How did the participants reason about the tasks? Did they see them as \u201cnatural\u201d and relevant to their own everyday experience?</p></li><li><p class=\"inline\">How did participants describe the role of the robot in the task? What where the benefits of its use, and what were the drawbacks?</p></li></ol></p></div></div><div id=\"s0008\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i18\" class=\"section-heading-2\">METHODOLOGY</h2><div id=\"s0008-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i19\">Apparatus</h3><p>Two robots were used in this study. The first was the UH Sunflower robot, which uses a Pioneer base (commercially available from MobileRobots) but with significant modifications (See <a href=\"#f0003\">Figure\u00a03</a>). The main mode of direct interaction with this robot is its touch-screen (<a href=\"#f0004\">Figure\u00a04</a>), which can be used to both display information to the user and issue commands to the robot. Sunflower also has an extending tray that can be used to carry light objects. The Sunflower robot is similar in shape and interaction capabilities to other robots intended for domestic use (e.g., Coradeschi et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0010\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>; Lammer et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0031\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>; Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0028\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>). The second robot used in the study was a SONY AIBO.<sup>1</sup></p><p>In addition, laptop PCs were set up for Skype calls.</p><p>The apparatus for the Fetch and Carry task consisted of the previously mentioned 100 play balls. The apparatus for the Cognitive Prosthetic task was comprised of the generic medicine tray and scrabble tiles as shown in <a href=\"#f0005\">Figure\u00a05</a>. Both of these are widely available commercially.</p></div><div id=\"s0008-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i20\">Experimental Setup</h3><p>Participants were asked to visit the robot house once a week for a period of 10 weeks, in order to study how participants\u2019 views of, and interactions with, the robots changed over time. See <a class=\"ref showTableEventRef\" data-ID=\"t0001\">Table\u00a01</a> for an overview of the sessions that the participants took part in. References made in this article to a specific week are based on <a class=\"ref showTableEventRef\" data-ID=\"t0001\">Table 1</a>. While the participants would only do the controlled, task-based prototyping experiment in weeks 1, 2, 5, and 8, it is important to note that in these other sessions they interacted with the robot, using its touch-screen interface and moving in the same space as the robot, thus familiarizing themselves with the robot and its use between the constrained task-based experiments. Each session took about 1\u00a0hour, including debriefing.</p></div><div id=\"s0008-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i21\">Procedure</h3><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0008-0003-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i22\">Introduction</h5><p>The introduction session introduced the UH Robot House and the robots to the participants. The participants were instructed in the use of the Sunflower robot and touch-screen, as well as how this robot responded to scheduled and sensor events. The participants were given a tour of the living areas where they would interact with the robot, and were shown the kitchen cupboards and fridge shelves that would be \u201ctheirs.\u201d In addition, they were introduced to the AIBO robot and its use in remote human\u2013human interaction scenarios. Throughout this tour, participants were encouraged to think of these areas as their home and to put themselves in the mind set of someone living in the house. This was intended to begin the process of framing the narrative (Dindler and Iversen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0014\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>) of the open-ended scenarios. It was also intended as a session in which the participants could make themselves as comfortable in the house as possible. The session ended with the baseline experiment.</p></div><div id=\"s0008-0003-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i23\">Open-ended scenarios</h5><p>As mentioned earlier, there were two open-ended scenarios that were presented twice to the participants. At the beginning of each open-ended scenario session, the participants were given a narrative framing of the context of the scenario that they were taking part in. They were told the time of day, and also what had transpired immediately before the beginning of the scenario. Scenario A began in the morning and the participants were told the following: <div class=\"quote\"><p>Imagine that you have now woken up. In the introductory session you gave us some preferences for what you would like to do in the early morning. The robot has stored these preferences and will try to help you do them. When you are ready, you will come out of the bedroom and sit down on the sofa. The robot will then approach you.</p></div></p><p>Scenario B began in the afternoon: <div class=\"quote\"><p>Imagine that it is afternoon and you have just returned home and have just sat down on the sofa. You have planned to watch some TV. In the introductory session, you gave us some preferences as to what TV programs you like to watch and also what sorts of snacks and drinks that you prefer to eat. The robot has stored these preferences. It will also respond to events such as phone calls and doorbells. When you are ready to begin, sit down on the sofa and the robot will approach you.</p></div></p><p>After this briefing, the scenarios ran as outlined previously. Participants were asked to fill in questionnaires after the scenario was completed.</p></div><div id=\"s0008-0003-0003\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i24\">Constrained experiments</h5><p>There were two sets of conditions for the experiment: <ol class=\"NLM_list NLM_list-list_type-order\"><li><p class=\"inline\">Task: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">Fetch and Carry.</p></li><li><p class=\"inline\">Cognitive Prosthetic.</p></li></ol></p></li><li><p class=\"inline\">Robot: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">Human-Only.</p></li><li><p class=\"inline\">Robot and Human.</p></li></ol></p></li></ol></p><p>In the baseline experiment in Week 1, participants undertook both task conditions in the human-only condition. The presentation order of the two tasks was counterbalanced in order to account for a presentation e\ufb00ect. In weeks 2, 5 and 8, participants did both task conditions for both of the robot conditions for a total of 4 trials in these weeks. The presentation order was within each week, for both Task and Robot conditions. Participants were given a questionnaire to respond to after each run of a task.</p></div></div></div><div id=\"s0008-0004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i25\">Robot Use</h3><p>The use of the robot was adapted to each task: For the Fetch and Carry task, participants were allowed to use the extendible tray of the robot as an additional platform to transport the plastic balls to the living room table. The participants could instruct the robot to move between the locations using the touch-screen interface. For the Cognitive Prosthetic task, the participants could access the list through the touch-screen interface. The participants could only access one quarter of the list at any given time, and could only choose which portion of the list to access while in the kitchen. This meant that in order to access the whole list, they would have to make several journeys between the living room and the kitchen over the course of the trial.</p><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0008-0004-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i26\">Instructions</h5><p>Before each task, participants were shown the apparatus involved in each task, and had the task explained to them. For the robot condition, participants were shown how to use the robot, and how to operate the touch-screen interface relevant for that particular task. Participants were asked to try to complete the task as quickly as possible. They were told that their performance was not being assessed, and that if the task took longer than 10 minutes to complete, the experimenters would stop the experiment.</p></div></div></div><div id=\"s0008-0005\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i27\">Measures: NASA Task Load Index</h3><p>We used the NASA Task Load Index (TLX) as the primary measure for the evaluation of the constrained tasks. The NASA TLX is a questionnaire-based means of measuring workload for specific tasks along several di\ufb00erent dimensions. It is particularly intended for examining human\u2013machine interactions (Hart and Staveland <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0020\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1988</a></span>). As it is a posttask measure, administering it to a participant would not a\ufb00ect task performance in the manner that a concurrent measure such as a think-aloud protocol might (Russo et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0039\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1989</a></span>). Despite it being a subjective, posttask measure, studies have shown it to be a reliable and valid tool for examining task di\ufb03culty and performance (Rubio et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0038\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2004</a></span>). Since its conception, it has been used across a wide variety of domains and tasks (Hart <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0019\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>). It was chosen over the more focused Human\u2013Robot Interaction Workload Measurement (HRI-WM) (Yagoda <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0055\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>) because the main focus of our study was on the participants\u2019 experience of the tasks themselves, rather than an assessment of how they interacted with the robot. The NASA TLX measures workload along six dimensions, shown in <a class=\"ref showTableEventRef\" data-ID=\"t0002\">Table\u00a02</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 2 </span> Dimensions of the NASA Task Load Index</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0002-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0002&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0002\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0008-0006\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i28\">Ad Hoc Questions</h3><p>In addition to the NASA TLX, participants were asked open-ended questions, inviting them to describe their experiences of the tasks themselves, as well as the role of the robot within them. These questions are shown in <a class=\"ref showTableEventRef\" data-ID=\"t0003\">Table\u00a03</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 3 </span> Open-ended questions</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0003-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0003&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0003\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0008-0007\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i29\">Participants</h3><p>Twelve participants took part in the study, recruited through advertisements on the University of Hertfordshire Intranet, mailing lists, and social networks. There were eight males and four females in the sample. The mean age was 32\u00a0years and the median age was 26\u00a0years, and the age range was 18\u201364\u00a0years. The use of human participants had been approved by the University of Hertfordshire Ethics Committee under protocol number 1112/39.</p></div></div><div id=\"s0009\" class=\"NLM_sec NLM_sec-type_results NLM_sec_level_1\"><h2 id=\"_i30\" class=\"section-heading-2\">RESULTS</h2><p>The results for the constrained tasks with respect to the original research questions were as follows.</p><div id=\"s0009-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i31\">Research Question 1: Characteristics of the Task</h3><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0009-0001-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i32\">Baseline values</h5><p>The di\ufb00erences between the two tasks were examined using a series of t-tests (<a class=\"ref showTableEventRef\" data-ID=\"t0004\">Table\u00a04</a> and <a href=\"#f0006\">Figure\u00a06</a>). As could be expected, the TLX significantly di\ufb00erentiates between the two tasks in terms of the physical and mental dimensions. The most salient di\ufb00erences between the two can be seen along the Physical Dimension (which contributes significantly more to the workload of the Fetch and Carry task) and the Mental Dimension (which contributes significantly more to the workload of the Cognitive Prosthetic task). There is a trend approaching significance for the Frustration Dimension, which suggests that it contributes more to the workload of the Fetch and Carry task. <div class=\"figure figureViewer\" id=\"f0006\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 6. </span> TLX baseline scores for tasks.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0006image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0006_b.gif\"}' width=\"500\" height=\"218\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0006\"><p><span class=\"captionLabel\">FIG. 6. </span> TLX baseline scores for tasks.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0006\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 4 </span> TLX baseline scores for tasks</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0004-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0004&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0004\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0009-0001-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i34\">Long-term change</h5><p>Change across the 8 weeks for the Fetch and Carry task is described in <a class=\"ref showTableEventRef\" data-ID=\"t0005\">Table\u00a05</a> and <a href=\"#f0007\">Figure\u00a07</a>. They suggest that the only significant change for this task was along the E\ufb00ort Dimension, which contributed more to the workload in this task in later weeks than the first week. Change across the 8 weeks for the Cognitive Prosthetic task is described in <a class=\"ref showTableEventRef\" data-ID=\"t0006\">Table\u00a06</a> and <a href=\"#f0008\">Figure\u00a08</a>, suggesting that overall there were no significant changes for this task in terms of what dimensions contributed to the workload on this task. However, a trend approaching significance indicates that the Temporal Dimension contributed less to the workload of this task in later weeks. Also, while the descriptive statistics of <a class=\"ref showTableEventRef\" data-ID=\"t0006\">Table\u00a06</a> suggest that there was an equally substantial mean change in the Mental Dimension, the variance between participants\u2019 individual scores stopped this change from being significant for this sample. <div class=\"figure figureViewer\" id=\"f0007\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 7. </span> Long-term change for Fetch and Carry task.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0007image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0007_b.gif\"}' width=\"500\" height=\"230\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0007\"><p><span class=\"captionLabel\">FIG. 7. </span> Long-term change for Fetch and Carry task.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0007\"></div> <div class=\"figure figureViewer\" id=\"f0008\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 8. </span> Long-term change for Fetch and Carry task.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0008image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0008_b.gif\"}' width=\"500\" height=\"230\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0008\"><p><span class=\"captionLabel\">FIG. 8. </span> Long-term change for Fetch and Carry task.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0008\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 5 </span> Long-term change for Fetch and Carry task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0005-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0005&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0005\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 6 </span> Long-term change for Cognitive Prosthetic task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0006-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0006&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0006\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div></div></div><div id=\"s0009-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i37\">Research Question 2: Robot Impact</h3><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0009-0002-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i38\">Fetch and Carry</h5><p>The overall impact of the robot can be found in <a class=\"ref showTableEventRef\" data-ID=\"t0007\">Table\u00a07</a> and <a href=\"#f0009\">Figure\u00a09</a>. There were significant main e\ufb00ects for the role of the Robot along the Physical, Temporal, Performance, and E\ufb00ort dimensions. However, all of these main e\ufb00ects, with the exception of Performance, were mediated by interaction e\ufb00ects between the role of the robot and the long-term e\ufb00ects, so we consider these interaction e\ufb00ects in the text as well. For Performance, there was a main e\ufb00ect for robot assistance. This e\ufb00ect suggests that performance was experienced as worse with the robot than if the participant acted on his or her own. This e\ufb00ect was very pronounced in week 2 but decreased with time. For the Physical dimension, there was a significant interaction e\ufb00ect between time and assistance. The relationship suggested by the descriptive statistics in <a class=\"ref showTableEventRef\" data-ID=\"t0007\">Table\u00a07</a> and <a href=\"#f0009\">Figure\u00a09b</a> is that the participants found that the robot reduced the workload overall but this e\ufb00ect decreased after week 2. For the Temporal Dimension, there was a significant main e\ufb00ect described in <a href=\"#f0009\">Figure\u00a09c</a>, where participants found that the robot overall increased the Temporal aspects of workload. The interaction e\ufb00ect approaching significance, however, suggests that this e\ufb00ect decreased over time. The robot's impact on the E\ufb00ort Dimension was quite small in weeks 2 and 5. However, by week 8, the assistance of the robot reduced the workload along this dimension (see <a href=\"#f0009\">Figure\u00a09e</a>). <div class=\"figure figureViewer\" id=\"f0009\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 9. </span> Robot impact on Fetch and Carry in terms of experienced workload.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0009image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0009_b.gif\"}' width=\"500\" height=\"380\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0009\"><p><span class=\"captionLabel\">FIG. 9. </span> Robot impact on Fetch and Carry in terms of experienced workload.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0009\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 7 </span> Robot impact on Fetch and Carry</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0007-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0007&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0007\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0009-0002-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i40\">Cognitive Prosthetic</h5><p>The overall impact for the robot on the Cognitive Prosthetic Task is shown in <a class=\"ref showTableEventRef\" data-ID=\"t0008\">Table\u00a08</a> and <a href=\"#f0010\">Figure\u00a010</a>. The impact of robot assistance was primarily along the Mental, Performance, and E\ufb00ort dimensions. There were no interaction e\ufb00ects. <div class=\"figure figureViewer\" id=\"f0010\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 10. </span> Robot impact on Cognitive Prosthetic in terms of experienced workload.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0010image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0010_b.gif\"}' width=\"500\" height=\"382\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0010\"><p><span class=\"captionLabel\">FIG. 10. </span> Robot impact on Cognitive Prosthetic in terms of experienced workload.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0010\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 8 </span> Robot impact on Cognitive Prosthetic</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0008-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0008&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0008\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>Participants viewed the robot as reducing workload along the Mental Dimension. This was consistent across the 3 weeks. On the other hand, the descriptive statistics in <a class=\"ref showTableEventRef\" data-ID=\"t0008\">Table\u00a08</a> suggest that participants saw the robot as adding significantly to the workload along the Performance Dimension (i.e., making it harder to succeed on the task). This e\ufb00ect is less pronounced in the last week. The other significant impact was along the E\ufb00ort Dimension. The descriptive statistics in <a class=\"ref showTableEventRef\" data-ID=\"t0008\">Table\u00a08</a> suggest that participants found they needed to exert less e\ufb00ort when aided by the robot. There were also two nonsignificant trends for the Temporal and Frustration dimensions. These trends suggested that the participants saw the use of the robot as contributing to more workload in these two dimensions, thus making the task both more frustrating and time-critical.</p></div></div></div><div id=\"s0009-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i42\">Research Question 3: The Experience of the Task and the Robot's Role</h3><p>The analysis of participant responses to qualitative questions (see <a class=\"ref showTableEventRef\" data-ID=\"t0003\">Table\u00a03</a>) was conducted in two main stages. In the first stage, one of the researchers examined the open-ended qualitative responses from the questionnaires and categorizsed them into primary themes and subthemes for each task and each week. These themes were then examined across weeks for each of the tasks. This led to the collection of themes identified in the first two columns in <a class=\"ref showTableEventRef\" data-ID=\"t0009 t0010\">Tables\u00a09 and 10</a>. A unified category scheme for both tasks could not be developed, largely due to the large qualitative di\ufb00erences between the tasks. After this categorization, two of the researchers went through the responses and categorized them as major (+), minor (\u2212), and nonexistent (0).</p><p>The themes that were the most prevalent in the responses were categorized as major. Minor themes were those less prevalent but still reported by a small group of participants. Themes that did not appear in the responses for a particular week were categorized as nonexistent. The final categorization and assignment of the themes was done by the researchers, after having compared their coding of responses, discussed discrepancies, and reached a consensus.</p><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0009-0003-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i43\">Fetch and Carry</h5><p>The themes emerging from the participants\u2019 responses are described in detail next and summarized in <a class=\"ref showTableEventRef\" data-ID=\"t0009\">Table\u00a09</a>: <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 9 </span> Themes for the Fetch and Carry task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0009-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0009&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0009\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>Week 1. For the Fetch and Carry task, the two primary themes emerging from Q1 (What made the task di\ufb03cult?) were the physical di\ufb03culty of handling the balls and the constraint of using only one hand when performing the task. They were also evident in the responses to Q2 (What would have made the task easier?) where the possibility of release from this constraint was the predominant theme.</p><p>Week 2. Week 2 saw the introduction of the robot, and Q1 and Q2 were asked for both the human-only and the robot\u2013human condition. For the human-only condition, the theme of the constraint continued among some of the participants. Participants would also contrast the human-only condition with the use of the robot when answering questions related to both conditions. When contrasting the conditions, participants highlighted the practical benefit of being able to perform the tasks in fewer trips. However, the second most prevalent theme in the participants\u2019 statements was the slow speed of the robot. The sample as a whole agreed that the speed of the robot was problematic from a task perspective, with participants having to change their speed of performing the task to accommodate the robot. This was achieved either by walking (more slowly) with the robot to the living room and back, or by waiting at the appropriate place to load or unload to the robot. In response to Q2 for the robot condition, the participants overwhelmingly suggested increasing the speed of the robot and/or the size of the tray. They also suggested that an ability of the robot to manipulate objects by loading itself would be helpful. In addition to purely task-related comparisons, a small group of participants highlighted interactional aspects of doing the task with the robot: that the robot provided company or that the task was more enjoyable when using the robot.</p><p>Week 5. Week 5 saw a continuation of the same themes as in week 2. New themes also emerged related to how participants rated their own performance. Some participants identified changes in their own behavior between conditions. They referred to a type of <i>social loafing</i> (Latane et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0032\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1979</a></span>) that occurred when they did the task with the robot, and that they put more e\ufb00ort in when they were doing the task by themselves. Other participants highlighted mutual adaptation. They reported that they were getting better at coordinating their own and the robot's roles in the task, reducing waiting, and making the use of the robot more e\ufb03cient. The most common strategy was to perform the task in an asynchronous manner, only loading and unloading the robot at convenient times instead of synchronizing each trip. However, for the sample as a whole, the theme of having to wait for the robot was still prevalent. In addition, this week saw statements regarding the touch-screen interface for this task. There were no statements regarding object manipulation capabilities in this week. In addition, participants continued to reference the social aspects of doing the task with the robot.</p><p>Week 8. Week 8 was very similar in terms of themes to Week 5. The main di\ufb00erence was one of prevalence. The theme of mutual adaptation continued and was more widespread, while the theme of having to wait for the robot was much less prevalent this week.</p></div><div id=\"s0009-0003-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i44\">Cognitive Prosthetic</h5><p>The themes arising from the participants\u2019 responses for this dimension are described below and summarized in <a class=\"ref showTableEventRef\" data-ID=\"t0010\">Table\u00a010</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 10 </span> Themes for the Cognitive Prosthetic task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0010-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0010&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0010\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>Week 1. In week 1, two main themes arose in participant responses to Q1. The first was the di\ufb03culty of having to remember the position of the tiles while walking from the kitchen to the living room. The second was the attempt at developing a strategy for solving the task without having to rely on memory alone. Responses to Q2 did, as for the Fetch and Carry task, focus on the constraints of the task\u2014in particular, the placement of the list of tile positions in a separate location from the medicine dispenser, and the list of tiles not being in any discernible order. A small group of participants managed to develop a strategy for doing this task more e\ufb03ciently, which consisted of arranging tiles spatially in one's palm in the same manner that they were to be arranged in the medicine dispenser and then transporting them over and inserting them into the dispenser in the same order. The final theme was an expressed desire for tools to aid in the task. There were two categories of tools: reminder tools, such as a pencil and paper to jot down the appropriate tiles and their positions, and tools to make the strategy described earlier more e\ufb03cient. An example of the latter would be a large tray to arrange and carry all the tiles on at once.</p><p>Week 2. In the human-only condition, the adoption of the strategy just described became more prevalent as fewer participants relied on memory alone to perform the task. This change was also reflected in the suggestions for tools to be used, where items that would aid in the use of this strategy were suggested to a larger extent than in the previous week. When discussing the role of the robot, participants raised several issues. They considered the robot-assisted solution of the task to be easier, as there was no need to either remember anything or adopt a strategy. Participants in particular referred to the infallibility of the robot's memory and how this made them feel less under pressure to perform the task correctly.</p><p>However, participants referenced the interaction with the robot in itself as a source of di\ufb03culty for the task as well. The robot was also described as slow and lacking in flexibility. The relinquishing of control to the robot was also referenced when discussing the procedure used to access the information on the robot.</p><p>Week 5. The results in week 5 followed many of the same themes as week 2. There was a continued increase in the use of the strategy outlined in week 1. By this week the majority of participants used this strategy for the human-only condition. References to interface issues were more prevalent in this week's responses, as were references to the physical aspect of the task, such as manipulating and putting the tiles in the dispenser. This week also saw a new theme of subversion emerging. Two of the participants described how they used the robot the way they wanted to, instead of how they felt they were being expected to. They arranged the tiles spatially on the tray of the robot in the kitchen and then used it to transport them in the correct arrangement to the dispenser in the living room, thus sidestepping the use of the robot as a Cognitive Prosthetic.</p><p>Week 8. Week 8 results were similar to those in week 5. Statements related to the physical carrying out of the task were more prevalent this week than on any other week. The majority of participants stated that the task had become easier for them to do. However, many still referenced the benefits of the robot, in particular its infallibility.</p></div></div></div></div><div id=\"s0010\" class=\"NLM_sec NLM_sec-type_discussion NLM_sec_level_1\"><h2 id=\"_i45\" class=\"section-heading-2\">DISCUSSION</h2><div id=\"s0010-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i46\">Research Question 1\u2014Di\ufb00erences Between the Tasks</h3><p>We were able to di\ufb00erentiate between the tasks in terms of their NASA TLX profile. Initially, the two tasks were significantly di\ufb00erent from each other only along their primary dimension, with a trend for the Fetch and Carry task loading more on the Frustration Dimension. In terms of long-term change, however, the picture was slightly di\ufb00erent. While neither of the two tasks changed on the Frustration Dimension, they did change along other dimensions. The Fetch and Carry task changed in terms of E\ufb00ort, and loaded higher on this dimension in the later weeks. The Cognitive Prosthetic task changed along the Temporal Dimension, and time pressure was considered less important in weeks 5 and 8. This suggests that the use of the NASA TLX for HRI tasks in domestic environments was a valid and meaningful approach.</p></div><div id=\"s0010-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i47\">Research Question 2\u2014Impact of the Robot</h3><p>The robot changed the participants\u2019 experience of the two tasks di\ufb00erently, both in its initial use as well as over time. For the Fetch and Carry task, the robot initially impacted the participants\u2019 ratings of the physical and temporal dimensions. In week 1, while the robot-assisted task was considered less physically strenuous, the participants found the time taken to be burdensome. The trend for the physical dimension continued in the subsequent weeks. However, the impact of the robot on the temporal dimension diminished, suggesting that participants found it easier to use the robot to complete the tasks in weeks 5 and 8. Furthermore, participants found that the use of robot required less e\ufb00ort in the last week, suggesting that there was a learning e\ufb00ect, and that participants were able to use the robot more e\ufb03ciently as time progressed. This was also seen in the manner that the participants reported they used the robot and as well as in their observed usage. In week 2, participants would load themselves and the robot and then follow the robot to the living to unload it. They would then return to the kitchen with the robot. In subsequent weeks, participants would be more likely to not wait for the robot, but rather move around the robot and only load/unload it if they happened to be in the same space as it. This approach employed the robot more efficiently as a supplement to their own capabilities. For the cognitive prosthetic task, the impact of the robot was less clear-cut. Participants rated doing the task with the robot as requiring less mental workload, and this e\ufb00ect persisted throughout the trials. In addition, participants felt that doing the task with the robot required less e\ufb00ort. Despite this, participants rated the use of the robot as requiring more workload in order to perform the task successfully. There was a trend suggesting that for weeks 2 and 5 the use of the robot was seen as more time-consuming; it was also seen as more frustrating across all the trials.</p><p>This suggests that despite the experienced benefit of using the robot in this task, there were still associated problems that made it more time-consuming and frustrating.</p></div><div id=\"s0010-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i48\">Research Question 3\u2014The Experience of the Task</h3><p>The descriptive analysis of the open-ended questions allowed for a deeper and more thorough perspective about the tasks and how they were experienced by the users.</p><p>When discussing the initial tasks, participants referenced the constraints imposed on them. Many of their suggestions for making the task easier involved the removal of these constraints. In the cognitive prosthetic task, the participants also considered the means through which they could access the information on the robot as one of the constraints.</p><p>In addition, the results from the TLX along this task were mirrored in the way that participants reasoned about the task. Participants described the robot as slow and inflexible and expressed a need to change the way that the robot was used in the task, either by changing how information was presented or by changing the usage of the robot. This was a reflection not just on their experience of the robot, but also how their increased mastery of the task made them consider the role of the robot di\ufb00erently. This even led to two of the participants using the robot in a manner unintended by the experimenters. They asserted control by subverting its use and using the Fetch and Carry functionalities to aid in the Cognitive Prosthetic task. For the remainder of the sample, however, there seemed to be a tacit understanding of a trade-o\ufb00 between the lack of human error in this task and the lack of control. In the Fetch and Carry task, however, despite similar descriptors of the robot being used in terms of it slowing down the task, participants adapted their use of the robot. This allowed the participants to work around these shortcomings and receive beneficial assistance from the robot. The changes that the participants mainly wanted to implement in terms of how they interacted with the robot were mainly quantitative changes: giving it more space to carry things and letting it move more quickly, in contrast to the changes in the quality of assistance that were suggested in the Cognitive Prosthetic task. It also emerged that, unlike in the Cognitive Prosthetic task, users referenced the robot as a partner and companion in the Fetch and Carry task. This may reflect the open-ended nature of this interaction, and the opportunity for a natural synchronization of behavior to occur gradually. Stienstra and Marti <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0043\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2012)</a></span> suggest this is a key factor in developing feelings of sociality and empathy in an HRI situation.</p></div><div id=\"s0010-0004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i49\">Ecological Validity</h3><p>The narrative framing of the interactions within the Robot House environment enabled participants to evaluate their interactions in a more relevant and applicable manner than what would have been possible in a traditional laboratory study. Despite the fact that the constrained tasks were part of an experimental study where the participants\u2019 interaction with the robot was tightly controlled, there are several factors that support the ecological validity of this study. These tasks were based on the needs of the user personas, and expected interactions arising from these needs. The parallels between observed behaviors and similar interactions with technologies in everyday settings were also encouraging. In the Fetch and Carry task, the process the participants went through when completing the tasks with the robot was quite similar to that of the user in the H\u00fcttenrauch and Eklundh <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0023\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2002)</a></span> study. In both cases, the users started o\ufb00 by coordinating their behavior closely with the robot, for example, walking with the robot, and synchronizing their own behavior with that of the robot. They then progressed to using the robot in a more asynchronous manner, with less constant control of the robot. These similarities in interactional outcomes support the notion that many of the qualities of a real-world usage of a final stage prototype were successfully translated into the experimental setup. For the Cognitive Prosthetic task, the manner in which the participants described the role of the robot in the task had elements that map well onto how people perceive such technologies in real-world settings. The issues of autonomy and control come up in both theoretical and practical discussions of the use of robotic technologies (Anderson and Anderson <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0001\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Sharkey and Sharkey <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0042\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2012</a></span>). In particular, the resolution of control issues by subverting assistive technologies has also been reported in real-world settings (Loe <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0033\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>), and an analogous process took place within the experiments. This suggests that for the cognitive prosthetic tasks, many of the salient aspects of using such technologies could be e\ufb00ectively conveyed through this constrained method.</p></div><div id=\"s0010-0005\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i50\">Implications</h3><p>The findings highlight the need for a user-centered approach to assistive technologies intended for domestic use. The results from the constrained task experiments strongly stress the need for such assistance to allow for personalization and for the robot assistance to be gradually scaled in order to account for changes in task mastery in the user and for coping strategies that the user may adopt. The TLX scores for the Cognitive Prosthetic tasks suggest that total experienced workload may increase where such scaling and alteration of assistance do not occur, due to frustration and disruption to learned coping strategies, despite the robot's assistance being still considered useful. In addition, the open-ended responses to this task suggested that participants came to regard the robot's assistance as hindering their preferred solution to the task. The scores for the Fetch and Carry task, on the other hand, represent a scenario where the roles of both the robot and the participants were less strongly defined. This left a lot of room for mutual adaptation, which in turn led to a more successful interaction in terms of the TLX scores, and also in terms of the participants\u2019 reasoning about the task and the role of the robot. This suggests that even in constrained tasks, such as the ones presented here, there is a hedonic dimension to interactions that has a role equal to their purely task- and workload-related aspects. This hedonic quality may be impacted by anthropomorphic interaction capabilities, and an interesting future strand of research into task-related domestic human\u2013robot interaction would be to investigate the role of such capabilities in how users respond to performing tasks with robots.</p></div></div><div id=\"s0011\" class=\"NLM_sec NLM_sec-type_conclusions NLM_sec_level_1\"><h2 id=\"_i51\" class=\"section-heading-2\">CONCLUSIONS</h2><p>The work presented in this article has shown the validity of interaction prototyping, in terms of both a high-level narrative approach in which the participants is involved in the playing of the role of a user of a more \u201cmature\u201d version of the technology being prototyped, and that of separating the task-aspect of such interaction. This two-pronged approach to interactions with future and emerging technologies for the purposes of early prototyping is a valid tool for gaining insight into how such interactions may be experienced by the intended users. The findings in this study have allowed us to replicate findings of real-world studies in terms of how participants reason about their potential adoption of such technologies, as well as to quantify the impact of assistance in such tasks using the NASA TLX and to highlight issues relevant for the UH Robot House Scenario and human\u2013robot interaction in general.</p><p>The work described in this article showed how to successfully integrate constrained tasks (as part of controlled experiments) with more \u201cnatural,\u201d open-ended scenarios as part of a long-term study into home companion robots operating in domestic environments. We pointed out experimental and methodological challenges and how they have been addressed in this study. The constrained tasks were based on commercially available tools and as such could potentially be used and replicated by other researchers. Being able to share, replicate, and build upon each others\u2019 results remains one of the big challenges in human\u2013robot interaction, which otherwise remains in danger of staying a widely fragmented field with di\ufb00erent research groups using di\ufb00erent robotic platforms, scenarios, and methodological approaches (Dautenhahn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0012\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>). We therefore hope that, in addition to presenting concrete results from a long-term human\u2013robot interaction study, this article has also raised awareness of the main challenges as well as opportunities in the design of interaction technology that supports long-term human\u2013robot interaction.</p></div></div><script type=\"text/javascript\">\r\n                        window.figureViewer={doi:'10.1080/01972243.2015.1020212',path:'/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604',figures:[{i:'f0001',g:[{m:'utis_a_1020212_f0001_oc.jpg',l:'utis_a_1020212_f0001_oc.jpeg',size:'26 KB'}]}\r\n                            ,{i:'f0002',g:[{m:'utis_a_1020212_f0002_oc.jpg',l:'utis_a_1020212_f0002_oc.jpeg',size:'69 KB'}]}\r\n                            ,{i:'f0003',g:[{m:'utis_a_1020212_f0003_oc.jpg',l:'utis_a_1020212_f0003_oc.jpeg',size:'169 KB'}]}\r\n                            ,{i:'f0004',g:[{m:'utis_a_1020212_f0004_oc.jpg',l:'utis_a_1020212_f0004_oc.jpeg',size:'38 KB'}]}\r\n                            ,{i:'f0005',g:[{m:'utis_a_1020212_f0005_oc.jpg',l:'utis_a_1020212_f0005_oc.jpeg',size:'67 KB'}]}\r\n                            ,{i:'f0006',g:[{m:'utis_a_1020212_f0006_b.gif',l:'utis_a_1020212_f0006_b.jpeg',size:'24 KB'}]}\r\n                            ,{i:'f0007',g:[{m:'utis_a_1020212_f0007_b.gif',l:'utis_a_1020212_f0007_b.jpeg',size:'28 KB'}]}\r\n                            ,{i:'f0008',g:[{m:'utis_a_1020212_f0008_b.gif',l:'utis_a_1020212_f0008_b.jpeg',size:'26 KB'}]}\r\n                            ,{i:'f0009',g:[{m:'utis_a_1020212_f0009_b.gif',l:'utis_a_1020212_f0009_b.jpeg',size:'73 KB'}]}\r\n                            ,{i:'f0010',g:[{m:'utis_a_1020212_f0010_b.gif',l:'utis_a_1020212_f0010_b.jpeg',size:'68 KB'}]}\r\n                            ]}</script><script type=\"text/javascript\">window.tableViewer={doi:'10.1080/01972243.2015.1020212',path:'/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604',tables:[{i:'t0001'},{i:'t0002'},{i:'t0003'},{i:'t0004'},{i:'t0005'},{i:'t0006'},{i:'t0007'},{i:'t0008'},{i:'t0009'},{i:'t0010'}]}</script><script type=\"text/javascript\">window.tableIDIndexMap = {\"id\":-1};window.tableIDIndexMap['t0001'] = 1; window.tableIDIndexMap['t0002'] = 2; window.tableIDIndexMap['t0003'] = 3; window.tableIDIndexMap['t0004'] = 4; window.tableIDIndexMap['t0005'] = 5; window.tableIDIndexMap['t0006'] = 6; window.tableIDIndexMap['t0007'] = 7; window.tableIDIndexMap['t0008'] = 8; window.tableIDIndexMap['t0009'] = 9; window.tableIDIndexMap['t0010'] = 10; </script><div id=\"table-content-t0001\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 1 </span> Overview of sessions</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Week</th><th align=\"center\" class=\"rowsep1 align_center last\">Session content</th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 1</td><td align=\"left\" class=\" align_left last\">Introduction to the Robot House, familiarization with the robots and their</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u2003interface. Baseline experiment.</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 2</td><td align=\"left\" class=\" align_left last\">Review of Robot House, robots, and interface. Repeat of experiment.</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 3</td><td align=\"left\" class=\" align_left last\">Open-ended scenario A</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 4</td><td align=\"left\" class=\" align_left last\">Open-ended scenario B</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 5</td><td align=\"left\" class=\" align_left last\">Repeat of experiment</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 6</td><td align=\"left\" class=\" align_left last\">Open-ended scenario A</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 7</td><td align=\"left\" class=\" align_left last\">Open-ended scenario B</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 8</td><td align=\"left\" class=\" align_left last\">Repeat of experiment</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Week 10</td><td align=\"left\" class=\" align_left last\">Debriefing</td></tr></tbody></table><div class=\"NLM_table-wrap-foot\" id=\"IDtable-wrap-foot\"><div class=\"fn-group\"></div></div></div><div id=\"table-content-t0002\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 2 </span> Dimensions of the NASA Task Load Index</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"center\" class=\"rowsep1 align_center last\">Workload in terms of \u2026</th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left last\">\u2026 reasoning remembering, planning, thinking</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left last\">\u2026 strength and endurance, dexterity</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left last\">\u2026 pace, time pressure, speed</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left last\">\u2026 success and satisfaction</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left last\">\u2026 e\ufb00ort needed to accomplish performance</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left last\">\u2026 annoyance, frustration, stress</td></tr></tbody></table></div><div id=\"table-content-t0003\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 3 </span> Open-ended questions</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Q1.</td><td align=\"left\" class=\" align_left last\">What was the most di\ufb03cult part of doing the task?</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Q2.</td><td align=\"left\" class=\" align_left last\">What would have made the task easier?</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Q3.</td><td align=\"left\" class=\" align_left last\">What were the benefits of doing the task with the robot?</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Q4.</td><td align=\"left\" class=\" align_left last\">What were the drawbacks of doing the task with the robot?</td></tr></tbody></table></div><div id=\"table-content-t0004\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 4 </span> TLX baseline scores for tasks</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">Fetch and Carrymean (SE)</th><th align=\"left\" class=\"rowsep1 align_left\">Cognitive Prostheticmean (SE)</th><th align=\"left\" class=\"rowsep1 align_left\">Mean difference</th><th align=\"left\" class=\"rowsep1 align_left\">95% CI</th><th align=\"center\" class=\"rowsep1 align_center\"><i>t</i>(df)</th><th align=\"center\" class=\"rowsep1 align_center last\"><i>p</i></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">0.50 (.14)</td><td align=\"left\" class=\" align_left\">2.75 (.58)</td><td align=\"char\" char=\".\" class=\" align_char\">\u22122.25</td><td align=\"left\" class=\" align_left\">\u22123.44 to 1.07</td><td align=\"char\" char=\".\" class=\" align_char\">4.20 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">**.01</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">2.46 (.62)</td><td align=\"left\" class=\" align_left\">0.70 (.25)</td><td align=\"char\" char=\".\" class=\" align_char\">1.75</td><td align=\"left\" class=\" align_left\">0.32 to 3.19</td><td align=\"char\" char=\".\" class=\" align_char\">2.69 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">**.02</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">1.89 (.51)</td><td align=\"left\" class=\" align_left\">1.95 (.46)</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.07</td><td align=\"left\" class=\" align_left\">\u22120.99 to 0.86</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.16 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.88</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">0.83 (.33)</td><td align=\"left\" class=\" align_left\">0.63 (.26)</td><td align=\"char\" char=\".\" class=\" align_char\">0.19</td><td align=\"left\" class=\" align_left\">\u22120.72 to 0.86</td><td align=\"char\" char=\".\" class=\" align_char\">0.46 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.65</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left\">1.15 (.30)</td><td align=\"left\" class=\" align_left\">1.40 (.30)</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.25</td><td align=\"left\" class=\" align_left\">\u22120.72 to 1.10</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.62 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.55</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">1.51 (.47)</td><td align=\"left\" class=\" align_left\">0.58 (.18)</td><td align=\"char\" char=\".\" class=\" align_char\">0.94</td><td align=\"left\" class=\" align_left\">\u22120.20 to 2.07</td><td align=\"char\" char=\".\" class=\" align_char\">1.82 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.10</td></tr></tbody></table></div><div id=\"table-content-t0005\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 5 </span> Long-term change for Fetch and Carry task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">Week 1</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left\"><i>F</i>(3, 8)</th><th align=\"left\" class=\"rowsep1 align_left\"><i>p</i></th><th align=\"left\" class=\"rowsep1 align_left last\">\u03b7<sup>2</sup></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">0.50 (.14)</td><td align=\"left\" class=\" align_left\">0.19 (.06)</td><td align=\"left\" class=\" align_left\">0.20 (.06</td><td align=\"left\" class=\" align_left\">0.17 (.14)</td><td align=\"left\" class=\" align_left\">0.99</td><td align=\"left\" class=\" align_left\">.45</td><td align=\"left\" class=\" align_left last\">.27</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">2.46 (.62)</td><td align=\"left\" class=\" align_left\">3.01 (.46)</td><td align=\"left\" class=\" align_left\">3.09 (.58)</td><td align=\"left\" class=\" align_left\">2.70 (.56)</td><td align=\"left\" class=\" align_left\">2.44</td><td align=\"left\" class=\" align_left\">.14</td><td align=\"left\" class=\" align_left last\">.48</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">1.89 (.51)</td><td align=\"left\" class=\" align_left\">1.30 (.29)</td><td align=\"left\" class=\" align_left\">2.25 (.44)</td><td align=\"left\" class=\" align_left\">1.95 (.50)</td><td align=\"left\" class=\" align_left\">1.20</td><td align=\"left\" class=\" align_left\">.37</td><td align=\"left\" class=\" align_left last\">.31</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">0.83 (.33)</td><td align=\"left\" class=\" align_left\">0.58 (.19)</td><td align=\"left\" class=\" align_left\">0.83 (.34)</td><td align=\"left\" class=\" align_left\">0.34 (.05)</td><td align=\"left\" class=\" align_left\">2.15</td><td align=\"left\" class=\" align_left\">.17</td><td align=\"left\" class=\" align_left last\">.45</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left\">1.15 (.30)</td><td align=\"left\" class=\" align_left\">2.50 (.37)</td><td align=\"left\" class=\" align_left\">1.84 (.42)</td><td align=\"left\" class=\" align_left\">2.45 (.40)</td><td align=\"left\" class=\" align_left\">5.48</td><td align=\"left\" class=\" align_left\">*.02</td><td align=\"left\" class=\" align_left last\">.67</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">1.51 (.47)</td><td align=\"left\" class=\" align_left\">0.95 (.25)</td><td align=\"left\" class=\" align_left\">0.88 (.20)</td><td align=\"left\" class=\" align_left\">1.02 (.34)</td><td align=\"left\" class=\" align_left\">1.52</td><td align=\"left\" class=\" align_left\">.28</td><td align=\"left\" class=\" align_left last\">.36</td></tr></tbody></table></div><div id=\"table-content-t0006\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 6 </span> Long-term change for Cognitive Prosthetic task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"center\" class=\"rowsep1 align_center\">Week 1</th><th align=\"center\" class=\"rowsep1 align_center\">Week 2</th><th align=\"center\" class=\"rowsep1 align_center\">Week 5</th><th align=\"center\" class=\"rowsep1 align_center\">Week 8</th><th align=\"center\" class=\"rowsep1 align_center\"><i>F</i>(3, 8)</th><th align=\"center\" class=\"rowsep1 align_center\"><i>p</i></th><th align=\"center\" class=\"rowsep1 align_center last\">\u03b7<sup>2</sup></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">2.75 (.58)</td><td align=\"left\" class=\" align_left\">1.97 (.48)</td><td align=\"left\" class=\" align_left\">2.21 (.51)</td><td align=\"left\" class=\" align_left\">1.81 (.38)</td><td align=\"left\" class=\" align_left\">1.29</td><td align=\"left\" class=\" align_left\">.34</td><td align=\"left\" class=\" align_left last\">.33</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">0.70 (.25)</td><td align=\"left\" class=\" align_left\">0.72 (.30)</td><td align=\"left\" class=\" align_left\">0.63 (.26)</td><td align=\"left\" class=\" align_left\">0.60 (.18)</td><td align=\"left\" class=\" align_left\">0.23</td><td align=\"left\" class=\" align_left\">.87</td><td align=\"left\" class=\" align_left last\">.08</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">1.95 (.46)</td><td align=\"left\" class=\" align_left\">1.78 (.44)</td><td align=\"left\" class=\" align_left\">1.04 (.30)</td><td align=\"left\" class=\" align_left\">1.22 (.43)</td><td align=\"left\" class=\" align_left\">3.87</td><td align=\"left\" class=\" align_left\">*.06</td><td align=\"left\" class=\" align_left last\">.59</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">0.63 (.26)</td><td align=\"left\" class=\" align_left\">0.93 (.28)</td><td align=\"left\" class=\" align_left\">0.76 (.22)</td><td align=\"left\" class=\" align_left\">0.68 (.32)</td><td align=\"left\" class=\" align_left\">1.63</td><td align=\"left\" class=\" align_left\">.26</td><td align=\"left\" class=\" align_left last\">.38</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left\">1.40 (.30)</td><td align=\"left\" class=\" align_left\">1.32 (.31)</td><td align=\"left\" class=\" align_left\">1.25 (.27)</td><td align=\"left\" class=\" align_left\">1.22 (.28)</td><td align=\"left\" class=\" align_left\">1.23</td><td align=\"left\" class=\" align_left\">.36</td><td align=\"left\" class=\" align_left last\">.32</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">0.58 (.18)</td><td align=\"left\" class=\" align_left\">1.08 (.42)</td><td align=\"left\" class=\" align_left\">0.60 (.21)</td><td align=\"left\" class=\" align_left\">.32 (.10)</td><td align=\"left\" class=\" align_left\">2.31</td><td align=\"left\" class=\" align_left\">.15</td><td align=\"left\" class=\" align_left last\">.46</td></tr></tbody></table></div><div id=\"table-content-t0007\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 7 </span> Robot impact on Fetch and Carry</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">\u00a0</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left\">ME<i>F</i>(3, 8)</th><th align=\"left\" class=\"rowsep1 align_left\">ME<i>p</i></th><th align=\"left\" class=\"rowsep1 align_left\">ME\u03b7<sup>2</sup></th><th align=\"left\" class=\"rowsep1 align_left\">In <i>F</i>(3,8)</th><th align=\"left\" class=\"rowsep1 align_left\">In<i>p</i></th><th align=\"left\" class=\"rowsep1 align_left last\">In\u03b7<sup>2</sup></th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.20 (0.20)</td><td align=\"left\" class=\" align_left\">0.20 (0.20)</td><td align=\"left\" class=\" align_left\">0.17 (0.25)</td><td align=\"left\" class=\" align_left\">3.83</td><td align=\"left\" class=\" align_left\">.08</td><td align=\"left\" class=\" align_left\">.29</td><td align=\"left\" class=\" align_left\">1.24</td><td align=\"left\" class=\" align_left\">.33</td><td align=\"left\" class=\" align_left last\">.22</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.95 (1.54)</td><td align=\"left\" class=\" align_left\">0.27 (0.20)</td><td align=\"left\" class=\" align_left\">0.20 (0.20)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">3.26 (1.41)</td><td align=\"left\" class=\" align_left\">3.09 (1.91)</td><td align=\"left\" class=\" align_left\">2.70 (1.86)</td><td align=\"left\" class=\" align_left\">16.16</td><td align=\"left\" class=\" align_left\">.01*</td><td align=\"left\" class=\" align_left\">.62</td><td align=\"left\" class=\" align_left\">13.17</td><td align=\"left\" class=\" align_left\">.01*</td><td align=\"left\" class=\" align_left last\">.75</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.26 (0.54)</td><td align=\"left\" class=\" align_left\">1.83 (1.29)</td><td align=\"left\" class=\" align_left\">1.70 (1.07)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.39 (1.01)</td><td align=\"left\" class=\" align_left\">2.25 (1.47)</td><td align=\"left\" class=\" align_left\">1.95 (1.67)</td><td align=\"left\" class=\" align_left\">7.65</td><td align=\"left\" class=\" align_left\">.04*</td><td align=\"left\" class=\" align_left\">.35</td><td align=\"left\" class=\" align_left\">4.00</td><td align=\"left\" class=\" align_left\">.05*</td><td align=\"left\" class=\" align_left last\">.47</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">6.10 (5.84)</td><td align=\"left\" class=\" align_left\">2.25 (1.68)</td><td align=\"left\" class=\" align_left\">1.67 (1.47)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.61 (0.67)</td><td align=\"left\" class=\" align_left\">0.83 (1.13)</td><td align=\"left\" class=\" align_left\">0.33 (0.16)</td><td align=\"left\" class=\" align_left\">7.65</td><td align=\"left\" class=\" align_left\">.02</td><td align=\"left\" class=\" align_left\">.43</td><td align=\"left\" class=\" align_left\">1.41</td><td align=\"left\" class=\" align_left\">.29</td><td align=\"left\" class=\" align_left last\">.24</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.01 (3.11)</td><td align=\"left\" class=\" align_left\">1.46 (1.78)</td><td align=\"left\" class=\" align_left\">0.56 (0.89)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Effort</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">2.72 (1.03)</td><td align=\"left\" class=\" align_left\">1.84 (1.40)</td><td align=\"left\" class=\" align_left\">2.46 (1.33)</td><td align=\"left\" class=\" align_left\">4.61</td><td align=\"left\" class=\" align_left\">.05*</td><td align=\"left\" class=\" align_left\">.32</td><td align=\"left\" class=\" align_left\">5.64</td><td align=\"left\" class=\" align_left\">.03*</td><td align=\"left\" class=\" align_left last\">.56</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.28 (2.50)</td><td align=\"left\" class=\" align_left\">1.61 (1.00)</td><td align=\"left\" class=\" align_left\">1.22 (0.96)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.03 (0.85)</td><td align=\"left\" class=\" align_left\">0.88 (0.66)</td><td align=\"left\" class=\" align_left\">1.02 (1.14)</td><td align=\"left\" class=\" align_left\">1.83</td><td align=\"left\" class=\" align_left\">.21</td><td align=\"left\" class=\" align_left\">.16</td><td align=\"left\" class=\" align_left\">2.28</td><td align=\"left\" class=\" align_left\">.16</td><td align=\"left\" class=\" align_left last\">.34</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.21 (0.19)</td><td align=\"left\" class=\" align_left\">0.65 (0.96)</td><td align=\"left\" class=\" align_left\">1.05 (0.85)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table></div><div id=\"table-content-t0008\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 8 </span> Robot impact on Cognitive Prosthetic</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">\u00a0</th><th align=\"center\" class=\"rowsep1 align_center\">Week 2</th><th align=\"center\" class=\"rowsep1 align_center\">Week 5</th><th align=\"center\" class=\"rowsep1 align_center\">Week 8</th><th align=\"center\" class=\"rowsep1 align_center\">ME<i>F</i>(3, 8)</th><th align=\"center\" class=\"rowsep1 align_center\">ME<i>p</i></th><th align=\"center\" class=\"rowsep1 align_center\">ME\u03b7<sup>2</sup></th><th align=\"center\" class=\"rowsep1 align_center\">In<i>F</i>(3, 8)</th><th align=\"center\" class=\"rowsep1 align_center\">In<i>p</i></th><th align=\"center\" class=\"rowsep1 align_center\">In\u03b7<sup>2</sup></th><th align=\"left\" class=\"rowsep1 align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">2.13 (1.63)</td><td align=\"left\" class=\" align_left\">2.21 (1.71)</td><td align=\"left\" class=\" align_left\">1.81 (1.26)</td><td align=\"left\" class=\" align_left\">16.24</td><td align=\"left\" class=\" align_left\">.01*</td><td align=\"left\" class=\" align_left\">.62</td><td align=\"left\" class=\" align_left\">0.42</td><td align=\"left\" class=\" align_left\">.67</td><td align=\"left\" class=\" align_left\">.08</td><td class=\"auto-generated last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.76 (0.90)</td><td align=\"left\" class=\" align_left\">0.73 (0.98)</td><td align=\"left\" class=\" align_left\">0.88 (1.72)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.77 (1.08)</td><td align=\"left\" class=\" align_left\">0.63 (0.85)</td><td align=\"left\" class=\" align_left\">0.60 (0.61)</td><td align=\"left\" class=\" align_left\">0.34</td><td align=\"left\" class=\" align_left\">.58</td><td align=\"left\" class=\" align_left\">.01</td><td align=\"left\" class=\" align_left\">2.80</td><td align=\"left\" class=\" align_left\">.11</td><td align=\"left\" class=\" align_left\">.38</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.36 (0.38)</td><td align=\"left\" class=\" align_left\">0.49 (0.65)</td><td align=\"left\" class=\" align_left\">0.83 (0.75)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.93 (1.53)</td><td align=\"left\" class=\" align_left\">1.04 (0.99)</td><td align=\"left\" class=\" align_left\">1.22 (1.41)</td><td align=\"left\" class=\" align_left\">3.63</td><td align=\"left\" class=\" align_left\">.09</td><td align=\"left\" class=\" align_left\">.27</td><td align=\"left\" class=\" align_left\">0.48</td><td align=\"left\" class=\" align_left\">.64</td><td align=\"left\" class=\" align_left\">.10</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.38 (1.64)</td><td align=\"left\" class=\" align_left\">1.92 (1.63)</td><td align=\"left\" class=\" align_left\">1.04 (0.99)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.99 (0.99)</td><td align=\"left\" class=\" align_left\">0.76 (0.72)</td><td align=\"left\" class=\" align_left\">0.68 (1.05)</td><td align=\"left\" class=\" align_left\">5.90</td><td align=\"left\" class=\" align_left\">.04*</td><td align=\"left\" class=\" align_left\">.37</td><td align=\"left\" class=\" align_left\">1.23</td><td align=\"left\" class=\" align_left\">.34</td><td align=\"left\" class=\" align_left\">.21</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.19 (1.14)</td><td align=\"left\" class=\" align_left\">1.14 (1.51)</td><td align=\"left\" class=\" align_left\">0.81 (1.06)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Effort</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.41 (1.05)</td><td align=\"left\" class=\" align_left\">1.25 (0.90)</td><td align=\"left\" class=\" align_left\">1.22 (0.94)</td><td align=\"left\" class=\" align_left\">4.79</td><td align=\"left\" class=\" align_left\">.05*</td><td align=\"left\" class=\" align_left\">.32</td><td align=\"left\" class=\" align_left\">0.23</td><td align=\"left\" class=\" align_left\">.8</td><td align=\"left\" class=\" align_left\">.05</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.94 (0.77)</td><td align=\"left\" class=\" align_left\">0.72 (0.87)</td><td align=\"left\" class=\" align_left\">0.95 (0.70)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.18 (1.47)</td><td align=\"left\" class=\" align_left\">0.60 (0.69)</td><td align=\"left\" class=\" align_left\">0.32 (0.32)</td><td align=\"left\" class=\" align_left\">3.21</td><td align=\"left\" class=\" align_left\">.10</td><td align=\"left\" class=\" align_left\">.23</td><td align=\"left\" class=\" align_left\">0.56</td><td align=\"left\" class=\" align_left\">.59</td><td align=\"left\" class=\" align_left\">.11</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">1.90 (2.16)</td><td align=\"left\" class=\" align_left\">1.10 (1.47)</td><td align=\"left\" class=\" align_left\">1.34 (1.33)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table></div><div id=\"table-content-t0009\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 9 </span> Themes for the Fetch and Carry task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Primary theme</th><th align=\"center\" class=\"rowsep1 align_center\">Subtheme</th><th align=\"center\" class=\"rowsep1 align_center\">Week 1</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Imposed constraint</td><td align=\"left\" class=\" align_left\">Using one hand</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Benefit from the tray</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"3\" align=\"left\" class=\" align_left\">Use of the robot</td><td align=\"left\" class=\" align_left\">Mutual adaptation</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Interface</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Having to wait</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Speed of the robot</td><td align=\"left\" class=\" align_left\">Walking with the robot</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Changing speed</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"3\" align=\"left\" class=\" align_left\">Changing capabilities</td><td align=\"left\" class=\" align_left\">Changing tray</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Object manipulation</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot as partner</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Interactional aspects</td><td align=\"left\" class=\" align_left\">Enjoyment</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Social loafing</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table><div class=\"NLM_table-wrap-foot\" id=\"IDtable-wrap-foot\"><div class=\"fn-group\"></div></div></div><div id=\"table-content-t0010\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 10 </span> Themes for the Cognitive Prosthetic task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Primary theme</th><th align=\"center\" class=\"rowsep1 align_center\">Subtheme</th><th align=\"left\" class=\"rowsep1 align_left\">Week 1</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"3\" align=\"left\" class=\" align_left\">Imposed constraints</td><td align=\"left\" class=\" align_left\">Separation of list and dispenser</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot positioning</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Random order of tiles and position in list</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"4\" align=\"left\" class=\" align_left\">Performing the task</td><td align=\"left\" class=\" align_left\">Di\ufb03culty in trying to remember</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physically manipulating the tiles</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Use of strategy</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Pen and paper</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Nonrobotic tool</td><td align=\"left\" class=\" align_left\">Tray</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Easy</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"4\" align=\"left\" class=\" align_left\">Robot benefits</td><td align=\"left\" class=\" align_left\">Infallible/no pressure</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Subversion</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Slow</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Flexibility</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Robot drawbacks</td><td align=\"left\" class=\" align_left\">Interface issues</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Control</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table><div class=\"NLM_table-wrap-foot\" id=\"IDtable-wrap-foot\"><div class=\"fn-group\"></div></div></div><div id=\"ack\" class=\"NLM_sec\"><h2 class=\"section-heading-2\">ACKNOWLEDGEMENTS</h2><p>We would like to thank our colleagues Michael L. Walters and Joe Saunders for helpful comments, and Fotios Papadopolous for his help with the AIBO robot.</p></div><div id=\"s0012\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i52\" class=\"section-heading-2\">NOTE</h2><p>1. Previous studies examining the application of biologically inspired expressive behaviors to Sunflower had shown that participants found the robot's non-anthropomorphic communicative behavior very e\ufb00ective in terms of conveying the robot's intention (Koay et\u00a0al., <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0026\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>).</p></div><div class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i54\" class=\"section-heading-2\">Funding</h2><p>The research leading to these results has received funding from the European Union's Seventh Framework Programme (FP7/2007-2013) under grant agreement 287624, the ACCOMPANY project, and grant agreement 215554, the LIREC (LIving with Robots and intEractive Companions) project.</p></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneFull\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"c8b0dea6-9842-46af-b708-142fe9107344\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"c8b0dea6-9842-46af-b708-142fe9107344\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div></div><div id=\"references-Section\"><h2 id=\"figures\">REFERENCES</h2><ul class=\"references numeric-ordered-list\"><li id=\"cit0001\"><span><span class=\"hlFld-ContribAuthor\">Anderson, <span class=\"NLM_given-names\">M.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. L.</span> Anderson</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Ethical healthcare agents</span>. In <i>Advanced computational intelligence paradigms in healthcare\u20133</i>, ed. M. Sordo, S. Vaidya, L. C. Jain, <span class=\"NLM_fpage\">233</span>\u2013<span class=\"NLM_lpage\">57</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0001&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-3-540-77662-8_10\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=233-57&author=M.+Anderson&author=S.+L.+Anderson&title=Ethical+healthcare+agents\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0002\"><span><span class=\"hlFld-ContribAuthor\">Bartneck, <span class=\"NLM_given-names\">C.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Rosalia</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Menges</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Deckers</span>. <span class=\"NLM_year\">2005</span>. <span class=\"NLM_article-title\">Robot abuse\u2014A limitation of the media equation</span>. <i>Proceedings of the Interact 2005 Workshop on Agent Abuse</i>, September, Rome, Italy, 54\u20137.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&author=C.+Bartneck&author=C.+Rosalia&author=R.+Menges&author=I.+Deckers&title=Robot+abuse%E2%80%94A+limitation+of+the+media+equation\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0003\"><span><span class=\"hlFld-ContribAuthor\">Bickmore, <span class=\"NLM_given-names\">T.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Cassell</span>. <span class=\"NLM_year\">2005</span>. <span class=\"NLM_chapter-title\">Social dialogue with embodied conversational agents</span>. In <i>Advances in natural multimodal dialogue systems</i>, ed. J. C. J. van Kuppevelt, L. Dybkj\u00e6r, and N. Ole Bernsen, <span class=\"NLM_fpage\">23</span>\u2013<span class=\"NLM_lpage\">54</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0003&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F1-4020-3933-6_2\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&pages=23-54&author=T.+Bickmore&author=J.+Cassell&title=Social+dialogue+with+embodied+conversational+agents\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0004\"><span><span class=\"hlFld-ContribAuthor\">Bickmore, <span class=\"NLM_given-names\">T. W.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Caruso</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Clough-Gorr</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Heeren</span>. <span class=\"NLM_year\">2005</span>. <span class=\"NLM_article-title\">\u2018It's just like you talk to a friend\u2019: Relational agents for older adults</span>. <i>Interacting with Computers</i> 17(6): <span class=\"NLM_fpage\">711</span>\u2013<span class=\"NLM_lpage\">35</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/j.intcom.2005.09.002</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0004&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2Fj.intcom.2005.09.002\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0004&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000233471900007\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=17&publication_year=2005&pages=711-35&issue=6&author=T.+W.+Bickmore&author=L.+Caruso&author=K.+Clough-Gorr&author=T.+Heeren&title=%E2%80%98It%27s+just+like+you+talk+to+a+friend%E2%80%99%3A+Relational+agents+for+older+adults&doi=10.1016%2Fj.intcom.2005.09.002\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0005\"><span><span class=\"hlFld-ContribAuthor\">Blythe, <span class=\"NLM_given-names\">M. A.,</span></span> and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P. C.</span> Wright</span>. <span class=\"NLM_year\">2006</span>. <span class=\"NLM_article-title\">Pastiche scenarios: Fiction as a resource for user centred design</span>. <i>Interacting with Computers</i> 18(5): <span class=\"NLM_fpage\">1139</span>\u2013<span class=\"NLM_lpage\">64</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/j.intcom.2006.02.001</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0005&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2Fj.intcom.2006.02.001\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0005&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000241345400015\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=18&publication_year=2006&pages=1139-64&issue=5&author=M.+A.%2C+Blythe&author=P.+C.+Wright&title=Pastiche+scenarios%3A+Fiction+as+a+resource+for+user+centred+design&doi=10.1016%2Fj.intcom.2006.02.001\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0006\"><span><span class=\"hlFld-ContribAuthor\">Buchenau, <span class=\"NLM_given-names\">M.,</span></span> and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. F.</span> Suri</span>. <span class=\"NLM_year\">2000</span>. <span class=\"NLM_chapter-title\">Experience prototyping</span>. In <i>Proceedings of the 3rd conference on designing interactive systems: Processes, practices, methods, and techniques</i>, <span class=\"NLM_fpage\">424</span>\u2013<span class=\"NLM_lpage\">33</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0006&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F347642.347802\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=424-33&author=M.%2C+Buchenau&author=J.+F.+Suri&title=Experience+prototyping\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0007\"><span><span class=\"hlFld-ContribAuthor\">Carroll, <span class=\"NLM_given-names\">J. M.</span></span> <span class=\"NLM_year\">2000</span>. <span class=\"NLM_article-title\">Five reasons for scenario-based design</span>. <i>Interacting with Computers</i> 13(1): <span class=\"NLM_fpage\">43</span>\u2013<span class=\"NLM_lpage\">60</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/S0953-5438(00)00023-0</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0007&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2FS0953-5438%2800%2900023-0\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0007&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000089375800003\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=13&publication_year=2000&pages=43-60&issue=1&author=J.+M.+Carroll&title=Five+reasons+for+scenario-based+design&doi=10.1016%2FS0953-5438%2800%2900023-0\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0008\"><span><span class=\"hlFld-ContribAuthor\">Chang, <span class=\"NLM_given-names\">Y.-N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.-K.</span> Lim</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Stolterman</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Personas: From theory to practices</span>. In <i>Proceedings of the 5th Nordic conference on human\u2013computer interaction: Building bridges</i>, <span class=\"NLM_fpage\">439</span>\u2013<span class=\"NLM_lpage\">42</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0008&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1463160.1463214\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=439-42&author=Y.-N.+Chang&author=Y.-K.+Lim&author=E.+Stolterman&title=Personas%3A+From+theory+to+practices\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0009\"><span><span class=\"hlFld-ContribAuthor\">Chatley, <span class=\"NLM_given-names\">A. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Christianson</span>. <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Theatre as a discussion tool in human\u2013robot interaction experiments-a pilot study</span>. In <i>Advances in computer\u2013human interactions</i>, <i>2010</i>, <span class=\"NLM_fpage\">73</span>\u2013<span class=\"NLM_lpage\">8</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0009&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FACHI.2010.17\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=73-8&author=A.+R.+Chatley&author=K.+Dautenhahn&author=M.+L.+Walters&author=D.+S.+Syrdal&author=B.+Christianson&title=Theatre+as+a+discussion+tool+in+human%E2%80%93robot+interaction+experiments-a+pilot+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0010\"><span><span class=\"hlFld-ContribAuthor\">Coradeschi, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Cesta</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Cortellessa</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Coraci</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Gonzalez</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Karlsson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Furfari</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Loutfi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Orlandini</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Palumbo</span>, et\u00a0al. <span class=\"NLM_year\">2013</span>. <span class=\"NLM_chapter-title\">Gira\ufb00plus: Combining social interaction and long term monitoring for promoting independent living</span>. In <i>Human system interaction (HSI), 2013</i>, <span class=\"NLM_fpage\">578</span>\u2013<span class=\"NLM_lpage\">85</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0010&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FHSI.2013.6577883\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=578-85&author=S.+Coradeschi&author=A.+Cesta&author=G.+Cortellessa&author=L.+Coraci&author=J.+Gonzalez&author=L.+Karlsson&author=F.+Furfari&author=A.+Loutfi&author=A.+Orlandini&author=F.+Palumbo&title=Gira%EF%AC%80plus%3A+Combining+social+interaction+and+long+term+monitoring+for+promoting+independent+living\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0011\"><span>Danish Technological Institute. <span class=\"NLM_year\">2012</span>. James\u2014Robot butler. <a class=\"ext-link\" href=\"http://robot.dti.dk/en/projects/james-robot-butler.aspx\" target=\"_blank\">http://robot.dti.dk/en/projects/james-robot-butler.aspx</a> (accessed <span class=\"NLM_date-in-citation\"><span class=\"NLM_month\">December</span> <span class=\"NLM_day\">14</span>, <span class=\"NLM_2012\">2012</span></span>).<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Danish+Technological+Institute.+2012.+James%E2%80%94Robot+butler.+http%3A%2F%2Frobot.dti.dk%2Fen%2Fprojects%2Fjames-robot-butler.aspx+%28accessed+December+14%2C+2012%29.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0012\"><span><span class=\"hlFld-ContribAuthor\">Dautenhahn, <span class=\"NLM_given-names\">K.</span></span> <span class=\"NLM_year\">2007</span>. <span class=\"NLM_article-title\">Methodology and themes of human\u2013robot interaction: A growing research field</span>. <i>International Journal of Advanced Robotic Systems</i> 4(1): <span class=\"NLM_fpage\">103</span>\u2013<span class=\"NLM_lpage\">8</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=4&publication_year=2007&pages=103-8&issue=1&author=K.+Dautenhahn&title=Methodology+and+themes+of+human%E2%80%93robot+interaction%3A+A+growing+research+field\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0013\"><span><span class=\"hlFld-ContribAuthor\">Dautenhahn, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Woods</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C. L.</span> Nehaniv</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Sisbot</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Alami</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Sim\u00e9on</span>. <span class=\"NLM_year\">2006</span>. <span class=\"NLM_chapter-title\">How may I serve you?: A robot companion approaching a seated person in a helping context</span>. In <i>Proceedings of the 1st ACM SIGCHI/SIGART conference on human\u2013robot interaction</i>, <span class=\"NLM_fpage\">172</span>\u2013<span class=\"NLM_lpage\">9</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0013&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1121241.1121272\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=172-9&author=K.+Dautenhahn&author=M.+Walters&author=S.+Woods&author=K.+L.+Koay&author=C.+L.+Nehaniv&author=A.+Sisbot&author=R.+Alami&author=T.+Sim%C3%A9on&title=How+may+I+serve+you%3F%3A+A+robot+companion+approaching+a+seated+person+in+a+helping+context\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0014\"><span><span class=\"hlFld-ContribAuthor\">Dindler, <span class=\"NLM_given-names\">C.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">O. S.</span> Iversen</span>. <span class=\"NLM_year\">2007</span>. <span class=\"NLM_article-title\">Fictional inquiry\u2014Design collaboration in a shared narrative space</span>. <i>CoDesign</i> 3(4): <span class=\"NLM_fpage\">213</span>\u2013<span class=\"NLM_lpage\">234</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1080/15710880701500187</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0014&amp;dbid=20&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1080%2F15710880701500187&amp;tollfreelink=2_18_a948775870ae22c47570c8b51f32b52c6487267f3a56ea70318c64598d237d4a\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=3&publication_year=2007&pages=213-234&issue=4&author=C.+Dindler&author=O.+S.+Iversen&title=Fictional+inquiry%E2%80%94Design+collaboration+in+a+shared+narrative+space&doi=10.1080%2F15710880701500187\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0015\"><span><span class=\"hlFld-ContribAuthor\">Du\ufb00y, <span class=\"NLM_given-names\">B. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G. M.</span> O\u2019Hare</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. N.</span> Martin</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. F.</span> Bradley</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Schon</span>. <span class=\"NLM_year\">2003</span>. <span class=\"NLM_chapter-title\">Agent chameleons: Agent minds and bodies</span>. In <i> 16th international conference on computer animation and social agents, 2003</i>, <span class=\"NLM_fpage\">118</span>\u2013<span class=\"NLM_lpage\">25</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0015&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FCASA.2003.1199312\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2003&pages=118-25&author=B.+R.+Du%EF%AC%80y&author=G.+M.+O%E2%80%99Hare&author=A.+N.+Martin&author=J.+F.+Bradley&author=B.+Schon&title=Agent+chameleons%3A+Agent+minds+and+bodies\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0016\"><span><span class=\"hlFld-ContribAuthor\">Duque, <span class=\"NLM_given-names\">I.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Willcock</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Christianson</span>. <span class=\"NLM_year\">2013</span>. <span class=\"NLM_chapter-title\">Knowledge-driven user activity recognition for a smart house: Development and validation of a generic and low-cost, resource-e\ufb03cient system</span>. In <i>ACHI 2013, The sixth international conference on advances in computer\u2013human interactions</i>, Nice, France, February 24\u2013March 1, <span class=\"NLM_fpage\">141</span>\u2013<span class=\"NLM_lpage\">6</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=141-6&author=I.+Duque&author=K.+Dautenhahn&author=K.+L.+Koay&author=I.+Willcock&author=B.+Christianson&title=Knowledge-driven+user+activity+recognition+for+a+smart+house%3A+Development+and+validation+of+a+generic+and+low-cost%2C+resource-e%EF%AC%83cient+system\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0017\"><span><span class=\"hlFld-ContribAuthor\">Fernaeus, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> H\u00e5kansson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Jacobsson</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Ljungblad</span>. <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">How do you play with a robotic toy animal?: A long-term study of Pleo</span>. In <i>Proceedings of the 9th international conference on interaction design and children</i>, <span class=\"NLM_fpage\">39</span>\u2013<span class=\"NLM_lpage\">48</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0017&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1810543.1810549\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=39-48&author=Y.+Fernaeus&author=M.+H%C3%A5kansson&author=M.+Jacobsson&author=S.+Ljungblad&title=How+do+you+play+with+a+robotic+toy+animal%3F%3A+A+long-term+study+of+Pleo\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0018\"><span><span class=\"hlFld-ContribAuthor\">Fussell, <span class=\"NLM_given-names\">S. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Kiesler</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L. D.</span> Setlock</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V.</span> Yew</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">How people anthropomorphize robots</span>. In <i>Proceedings of the 3rd ACM/IEEE international conference on human\u2013robot interaction</i>, <span class=\"NLM_fpage\">145</span>\u2013<span class=\"NLM_lpage\">52</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0018&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1349822.1349842\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=145-52&author=S.+R.+Fussell&author=S.+Kiesler&author=L.+D.+Setlock&author=V.+Yew&title=How+people+anthropomorphize+robots\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0019\"><span><span class=\"hlFld-ContribAuthor\">Hart, <span class=\"NLM_given-names\">S. G.</span></span> <span class=\"NLM_year\">2006</span>. <span class=\"NLM_chapter-title\">NASA-task load index (NASA-TLX); 20 years later</span>. <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 50: <span class=\"NLM_fpage\">904</span>\u2013<span class=\"NLM_lpage\">8</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0019&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1177%2F154193120605000909\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=904-8&author=S.+G.+Hart&title=NASA-task+load+index+%28NASA-TLX%29%3B+20+years+later\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0020\"><span><span class=\"hlFld-ContribAuthor\">Hart, <span class=\"NLM_given-names\">S. G.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L. E.</span> Staveland</span>. <span class=\"NLM_year\">1988</span>. <span class=\"NLM_article-title\">Development of NASA-TLX (task load index): Results of empirical and theoretical research</span>. <i>Human Mental Workload</i> 1(3): <span class=\"NLM_fpage\">139</span>\u2013<span class=\"NLM_lpage\">83</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/S0166-4115(08)62386-9</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0020&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2FS0166-4115%2808%2962386-9\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=1988&pages=139-83&issue=3&author=S.+G.+Hart&author=L.+E.+Staveland&title=Development+of+NASA-TLX+%28task+load+index%29%3A+Results+of+empirical+and+theoretical+research&doi=10.1016%2FS0166-4115%2808%2962386-9\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0021\"><span><span class=\"hlFld-ContribAuthor\">Horne, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Weinman</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Barber</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Elliott</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Morgan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Cribb</span>, et\u00a0al. <span class=\"NLM_year\">2005</span>. <i>Concordance, adherence and compliance in medicine taking</i>. <span class=\"NLM_publisher-loc\">London, UK</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&author=R.+Horne&author=J.+Weinman&author=N.+Barber&author=R.+Elliott&author=M.+Morgan&author=A.+Cribb&title=Concordance%2C+adherence+and+compliance+in+medicine+taking\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0022\"><span><span class=\"hlFld-ContribAuthor\">Huijnen, <span class=\"NLM_given-names\">C.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Badii</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> van den Heuvel</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Caleb-Solly</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Thiemert</span>. <span class=\"NLM_year\">2011</span>. \u2018<span class=\"NLM_chapter-title\">Maybe it becomes a buddy, but do not call it a robot\u2019\u2014Seamless cooperation between companion robotics and smart homes</span>. In <i>Ambient intelligence</i>, <span class=\"NLM_fpage\">324</span>\u2013<span class=\"NLM_lpage\">29</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0022&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-3-642-25167-2_44\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=324-29&author=C.+Huijnen&author=A.+Badii&author=H.+van+den+Heuvel&author=P.+Caleb-Solly&author=D.+Thiemert&title=Maybe+it+becomes+a+buddy%2C+but+do+not+call+it+a+robot%E2%80%99%E2%80%94Seamless+cooperation+between+companion+robotics+and+smart+homes\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0023\"><span><span class=\"hlFld-ContribAuthor\">H\u00fcttenrauch, <span class=\"NLM_given-names\">H.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. S.</span> Eklundh</span>. <span class=\"NLM_year\">2002</span>. <span class=\"NLM_chapter-title\">Fetch-and-carry with CERO: Observations from a long-term user study with a service robot</span>. In <i>11th IEEE international workshop on robot and human interactive communication, 2002, Proceedings</i>, ed. D. Keyson, M. L. Maher, N. Streitz, A. D. Cheok, J. C. Augusto, R. Wichert, G. Englebienne, H. Aghajan, and B. Kr\u00f6se, <span class=\"NLM_fpage\">158</span>\u2013<span class=\"NLM_lpage\">63</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0023&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2002.1045615\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=158-63&author=H.+H%C3%BCttenrauch&author=K.+S.+Eklundh&title=Fetch-and-carry+with+CERO%3A+Observations+from+a+long-term+user+study+with+a+service+robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0024\"><span><span class=\"hlFld-ContribAuthor\">Kamsma, <span class=\"NLM_given-names\">Y. P.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W. H.</span> Brouwer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. P.</span> Lakke</span>. <span class=\"NLM_year\">1995</span>. <span class=\"NLM_article-title\">Training of compensational strategies for impaired gross motor skills in Parkinson's disease</span>. <i>Physiotherapy Theory and Practice</i> 11(4): <span class=\"NLM_fpage\">209</span>\u2013<span class=\"NLM_lpage\">29</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.3109/09593989509036407</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0024&amp;dbid=20&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.3109%2F09593989509036407&amp;tollfreelink=2_18_27bce6455306b13bcd4b08dad2149196457f73cc55e6dfc1360715d5231f6138\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=11&publication_year=1995&pages=209-29&issue=4&author=Y.+P.+Kamsma&author=W.+H.+Brouwer&author=J.+P.+Lakke&title=Training+of+compensational+strategies+for+impaired+gross+motor+skills+in+Parkinson%27s+disease&doi=10.3109%2F09593989509036407\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0025\"><span><span class=\"hlFld-ContribAuthor\">Kidd, <span class=\"NLM_given-names\">C. D.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Breazeal</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Robots at home: Understanding long-term human\u2013robot interaction</span>. In <i>Intelligent robots and systems, 2008, IROS 2008</i>, <span class=\"NLM_fpage\">3230</span>\u2013<span class=\"NLM_lpage\">5</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0025&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FIROS.2008.4651113\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=3230-5&author=C.+D.+Kidd&author=C.+Breazeal&title=Robots+at+home%3A+Understanding+long-term+human%E2%80%93robot+interaction\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0026\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Lakatos</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> G\u00e1csi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Bereczky</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Mikl\u00f3si</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>. <span class=\"NLM_year\">2013</span>. <span class=\"NLM_chapter-title\">Hey! There is someone at your door. A hearing robot using visual communication signals of hearing dogs to communicate intent</span>. In <i> IEEE Symposium on Artificial Life (ALIFE), 2013</i>, 90\u201397. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0026&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FALIFE.2013.6602436\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&author=K.+L.+Koay&author=G.+Lakatos&author=D.+Syrdal&author=M.+G%C3%A1csi&author=B.+Bereczky&author=K.+Dautenhahn&author=A.+Mikl%C3%B3si&author=M.+L.+Walters&title=Hey%21+There+is+someone+at+your+door.+A+hearing+robot+using+visual+communication+signals+of+hearing+dogs+to+communicate+intent\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0027\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E. A.</span> Sisbot</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Alami</span>. <span class=\"NLM_year\">2007</span>. <span class=\"NLM_chapter-title\">Exploratory study of a robot approaching a person in the context of handing over an object</span>. In <i>AAAI spring symposium: Multidisciplinary collaboration for socially assistive robotics</i>, <span class=\"NLM_fpage\">18</span>\u2013<span class=\"NLM_lpage\">24</span>. <span class=\"NLM_publisher-loc\">Menlo Park, CA</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=18-24&author=K.+L.+Koay&author=E.+A.+Sisbot&author=D.+S.+Syrdal&author=M.+L.+Walters&author=K.+Dautenhahn&author=R.+Alami&title=Exploratory+study+of+a+robot+approaching+a+person+in+the+context+of+handing+over+an+object\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0028\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Ashgari-Oskoei</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2014</span>. <span class=\"NLM_article-title\">Social roles and baseline proxemic preferences for a domestic service robot</span>. <i>International Journal of Social Robotics</i> 6(4): 469\u201388. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1007/s12369-014-0232-4</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0028&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2Fs12369-014-0232-4\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=K.+L.+Koay&author=D.+S.+Syrdal&author=M.+Ashgari-Oskoei&author=M.+L.+Walters&author=K.+Dautenhahn&title=Social+roles+and+baseline+proxemic+preferences+for+a+domestic+service+robot&doi=10.1007%2Fs12369-014-0232-4\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0029\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Arent</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Malek</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Kreczmer</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_chapter-title\">Companion migration\u2013initial participants\u2019 feedback from a video-based prototyping study</span>. In <i>Mixed reality and human\u2013robot interaction</i>, ed. X. Wang, <span class=\"NLM_fpage\">133</span>\u2013<span class=\"NLM_lpage\">51</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0029&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-94-007-0582-1_8\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=133-51&author=K.+L.+Koay&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=K.+Arent&author=L.+Malek&author=B.+Kreczmer&title=Companion+migration%E2%80%93initial+participants%E2%80%99+feedback+from+a+video-based+prototyping+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0030\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">Five weeks in the robot house\u2014Exploratory human\u2013robot interaction trials in a domestic setting</span>. In <i>Advances in computer\u2013human interactions, 2009, ACHI\u201909</i>, <span class=\"NLM_fpage\">219</span>\u2013<span class=\"NLM_lpage\">26</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=219-26&author=K.+L.+Koay&author=D.+S.+Syrdal&author=M.+L.+Walters&author=K.+Dautenhahn&title=Five+weeks+in+the+robot+house%E2%80%94Exploratory+human%E2%80%93robot+interaction+trials+in+a+domestic+setting\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0031\"><span><span class=\"hlFld-ContribAuthor\">Lammer, <span class=\"NLM_given-names\">L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Huber</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Weiss</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Vincze</span>. <span class=\"NLM_year\">2014</span>. <span class=\"NLM_chapter-title\">Mutual care: How older adults react when they should help their care robot</span>. In <i>AISB2014: Proceedings of the 3rd international symposium on new frontiers in human\u2013robot interaction</i>, <span class=\"NLM_publisher-name\">Routledge</span>, April 1\u20134. <span class=\"NLM_publisher-loc\">London, UK</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=L.+Lammer&author=A.+Huber&author=A.+Weiss&author=M.+Vincze&title=Mutual+care%3A+How+older+adults+react+when+they+should+help+their+care+robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0032\"><span><span class=\"hlFld-ContribAuthor\">Latane, <span class=\"NLM_given-names\">B.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Williams</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Harkins</span>. <span class=\"NLM_year\">1979</span>. <span class=\"NLM_article-title\">Many hands make light the work: The causes and consequences of social loafing</span>. <i>Journal of Personality and Social Psychology</i> 37(6): <span class=\"NLM_fpage\">822</span>\u2013<span class=\"NLM_lpage\">32</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1037/0022-3514.37.6.822</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0032&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1037%2F0022-3514.37.6.822\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0032&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1979HH08400002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=37&publication_year=1979&pages=822-32&issue=6&author=B.+Latane&author=K.+Williams&author=S.+Harkins&title=Many+hands+make+light+the+work%3A+The+causes+and+consequences+of+social+loafing&doi=10.1037%2F0022-3514.37.6.822\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0033\"><span><span class=\"hlFld-ContribAuthor\">Loe, <span class=\"NLM_given-names\">M.</span></span> <span class=\"NLM_year\">2010</span>. <span class=\"NLM_article-title\">Doing it my way: Old women, technology and wellbeing</span>. <i>Sociology of Health &amp; Illness</i> 32(2): <span class=\"NLM_fpage\">319</span>\u2013<span class=\"NLM_lpage\">34</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1111/j.1467-9566.2009.01220.x</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0033&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1111%2Fj.1467-9566.2009.01220.x\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0033&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=20149150\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0033&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000274711000011\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=32&publication_year=2010&pages=319-34&issue=2&author=M.+Loe&title=Doing+it+my+way%3A+Old+women%2C+technology+and+wellbeing&doi=10.1111%2Fj.1467-9566.2009.01220.x\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0034\"><span><span class=\"hlFld-ContribAuthor\">Modayil, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Levinson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Harman</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Halper</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kautz</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Integrating sensing and cueing for more e\ufb00ective activity reminders</span>. In <i>AAAI Fall 2008 Symposium on AI in Eldercare: New solutions to old problems</i>, <span class=\"NLM_fpage\">7</span>\u2013<span class=\"NLM_lpage\">9</span>. <span class=\"NLM_publisher-loc\">Menlo Park, CA</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=7-9&author=J.+Modayil&author=R.+Levinson&author=C.+Harman&author=D.+Halper&author=H.+Kautz&title=Integrating+sensing+and+cueing+for+more+e%EF%AC%80ective+activity+reminders\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0035\"><span><span class=\"hlFld-ContribAuthor\">Newell, <span class=\"NLM_given-names\">A. F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Carmichael</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Morgan</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Dickinson</span>. <span class=\"NLM_year\">2006</span>. <span class=\"NLM_article-title\">The use of theatre in requirements gathering and usability studies</span>. <i>Interacting with Computers</i> 18(5): <span class=\"NLM_fpage\">996</span>\u2013<span class=\"NLM_lpage\">1011</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/j.intcom.2006.05.003</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0035&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2Fj.intcom.2006.05.003\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0035&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000241345400007\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=18&publication_year=2006&pages=996-1011&issue=5&author=A.+F.+Newell&author=A.+Carmichael&author=M.+Morgan&author=A.+Dickinson&title=The+use+of+theatre+in+requirements+gathering+and+usability+studies&doi=10.1016%2Fj.intcom.2006.05.003\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0036\"><span><span class=\"hlFld-ContribAuthor\">Parlitz, <span class=\"NLM_given-names\">C.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> H\u00e4gele</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Klein</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Seifert</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_article-title\">Care-O-Bot 3\u2014Rationale for human\u2013robot interaction design</span>. <i>Proceedings of 39th International Symposium on Robotics (ISR)</i>, Seoul, Korea, October 15\u201317, <span class=\"NLM_fpage\">275</span>\u2013<span class=\"NLM_lpage\">80</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=275-80&author=C.+Parlitz&author=M.+H%C3%A4gele&author=P.+Klein&author=J.+Seifert&author=K.+Dautenhahn&title=Care-O-Bot+3%E2%80%94Rationale+for+human%E2%80%93robot+interaction+design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0037\"><span><span class=\"hlFld-ContribAuthor\">Payr, <span class=\"NLM_given-names\">S.</span></span> <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Closing and closure in human\u2013companion interactions: Analyzing video data from a field study</span>. In <i>2010 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">476</span>\u2013<span class=\"NLM_lpage\">81</span>. <span class=\"NLM_publisher-loc\">New York, NY</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0037&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2010.5598625\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=476-81&author=S.+Payr&title=Closing+and+closure+in+human%E2%80%93companion+interactions%3A+Analyzing+video+data+from+a+field+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0038\"><span><span class=\"hlFld-ContribAuthor\">Rubio, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> D\u00edaz</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Mart\u00edn</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Puente</span>. <span class=\"NLM_year\">2004</span>. <span class=\"NLM_article-title\">Evaluation of subjective mental workload: A comparison of SWAT, NASA-TLX, and workload profile methods</span>. <i>Applied Psychology</i> 53(1): <span class=\"NLM_fpage\">61</span>\u2013<span class=\"NLM_lpage\">86</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1111/j.1464-0597.2004.00161.x</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0038&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1111%2Fj.1464-0597.2004.00161.x\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0038&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000188753000004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=53&publication_year=2004&pages=61-86&issue=1&author=S.+Rubio&author=E.+D%C3%ADaz&author=J.+Mart%C3%ADn&author=J.+M.+Puente&title=Evaluation+of+subjective+mental+workload%3A+A+comparison+of+SWAT%2C+NASA-TLX%2C+and+workload+profile+methods&doi=10.1111%2Fj.1464-0597.2004.00161.x\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0039\"><span><span class=\"hlFld-ContribAuthor\">Russo, <span class=\"NLM_given-names\">J. E.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E. J.</span> Johnson</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. L.</span> Stephens</span>. <span class=\"NLM_year\">1989</span>. <span class=\"NLM_article-title\">The validity of verbal protocols</span>. <i>Memory &amp; Cognition</i> 17(6): <span class=\"NLM_fpage\">759</span>\u2013<span class=\"NLM_lpage\">69</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.3758/BF03202637</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0039&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.3758%2FBF03202637\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0039&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=2811673\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0039&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1989AX07500012\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=17&publication_year=1989&pages=759-69&issue=6&author=J.+E.+Russo&author=E.+J.+Johnson&author=D.+L.+Stephens&title=The+validity+of+verbal+protocols&doi=10.3758%2FBF03202637\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0040\"><span><span class=\"hlFld-ContribAuthor\">Schwartz, <span class=\"NLM_given-names\">D.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Wang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Zeitz</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. E.</span> Goss</span>. <span class=\"NLM_year\">1962</span>. <span class=\"NLM_article-title\">Medication errors made by elderly, chronically ill patients</span>. <i>American Journal of Public Health and the Nation's Health</i> 52(12): <span class=\"NLM_fpage\">2018</span>\u2013<span class=\"NLM_lpage\">2029</span> http://dx.doi.org/<span class=\"NLM_pub-id\">10.2105/AJPH.52.12.2018</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0040&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.2105%2FAJPH.52.12.2018\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0040&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=13987359\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0040&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1962B392600018\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=52&publication_year=1962&pages=2018-2029&issue=12&author=D.+Schwartz&author=M.+Wang&author=L.+Zeitz&author=M.+E.+Goss&title=Medication+errors+made+by+elderly%2C+chronically+ill+patients&doi=10.2105%2FAJPH.52.12.2018\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0041\"><span><span class=\"hlFld-ContribAuthor\">Seland, <span class=\"NLM_given-names\">G.</span></span> <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">Empowering end users in design of mobile technology using role play as a method: Reflections on the role-play conduction</span>. In <i>Human centered design</i>, ed. M. Kurosu, <span class=\"NLM_fpage\">912</span>\u2013<span class=\"NLM_lpage\">21</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0041&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-3-642-02806-9_105\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=912-21&author=G.+Seland&title=Empowering+end+users+in+design+of+mobile+technology+using+role+play+as+a+method%3A+Reflections+on+the+role-play+conduction\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0042\"><span><span class=\"hlFld-ContribAuthor\">Sharkey, <span class=\"NLM_given-names\">A.,</span></span> and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Sharkey</span>. <span class=\"NLM_year\">2012</span>. <span class=\"NLM_article-title\">Granny and the robots: Ethical issues in robot care for the elderly</span>. <i>Ethics and Information Technology</i> 14(1): <span class=\"NLM_fpage\">27</span>\u2013<span class=\"NLM_lpage\">40</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1007/s10676-010-9234-6</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0042&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2Fs10676-010-9234-6\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0042&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000301566800004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=14&publication_year=2012&pages=27-40&issue=1&author=A.%2C+Sharkey&author=N.+Sharkey&title=Granny+and+the+robots%3A+Ethical+issues+in+robot+care+for+the+elderly&doi=10.1007%2Fs10676-010-9234-6\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0043\"><span><span class=\"hlFld-ContribAuthor\">Stienstra, <span class=\"NLM_given-names\">J.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Marti</span>. <span class=\"NLM_year\">2012</span>. <span class=\"NLM_chapter-title\">Squeeze me: Gently please</span>. In <i>Proceedings of the 7th Nordic conference on human\u2013computer interaction: Making sense through design</i>, <span class=\"NLM_fpage\">746</span>\u2013<span class=\"NLM_lpage\">50</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0043&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F2399016.2399131\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&pages=746-50&author=J.+Stienstra&author=P.+Marti&title=Squeeze+me%3A+Gently+please\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0044\"><span><span class=\"hlFld-ContribAuthor\">Sung, <span class=\"NLM_given-names\">J.-Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. E.</span> Grinter</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H. I.</span> Christensen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Guo</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Housewives or technophiles?: Understanding domestic robot owners</span>. In <i>2008 3rd ACM/IEEE International Conference on human\u2013robot interaction (HRI)</i>, <span class=\"NLM_fpage\">129</span>\u2013<span class=\"NLM_lpage\">36</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0044&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1349822.1349840\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=129-36&author=J.-Y.+Sung&author=R.+E.+Grinter&author=H.+I.+Christensen&author=L.+Guo&title=Housewives+or+technophiles%3F%3A+Understanding+domestic+robot+owners\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0045\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W. C.</span> Ho</span>. <span class=\"NLM_year\">2014</span>. <span class=\"NLM_article-title\">Views from within a narrative: Evaluating long-term human\u2013robot interaction in a naturalistic environment using open-ended scenarios</span>. <i>Cognitive Computation</i> 6(4): 741\u201359..<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=K.+L.+Koay&author=W.+C.+Ho&title=Views+from+within+a+narrative%3A+Evaluating+long-term+human%E2%80%93robot+interaction+in+a+naturalistic+environment+using+open-ended+scenarios\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0046\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>. <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">The negative attitudes towards robots scale and reactions to robot behaviour in a live human\u2013robot interaction study</span>. <i>New frontiers in human\u2013robot interaction, a symposium at the AISB2009 Convention</i>, <span class=\"NLM_publisher-loc\">Edinburgh, UK</span>, April <span class=\"NLM_fpage\">6</span>\u2013<span class=\"NLM_lpage\">9</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=6-9&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=K.+L.+Koay&author=M.+L.+Walters&title=The+negative+attitudes+towards+robots+scale+and+reactions+to+robot+behaviour+in+a+live+human%E2%80%93robot+interaction+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0047\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N. R.</span> Otero</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_chapter-title\">The theatre methodology for facilitating discussion in human\u2013robot interaction on information disclosure in a home environment</span>. In <i>2011 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">479</span>\u2013<span class=\"NLM_lpage\">84</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0047&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2011.6005247\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=479-84&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=M.+L.+Walters&author=K.+L.+Koay&author=N.+R.+Otero&title=The+theatre+methodology+for+facilitating+discussion+in+human%E2%80%93robot+interaction+on+information+disclosure+in+a+home+environment\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0048\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> G\u00e1csi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Video prototyping of dog-inspired non-verbal a\ufb00ective communication for an appearance constrained robot</span>. In <i>2010 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">632</span>\u2013<span class=\"NLM_lpage\">7</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0048&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2010.5598693\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=632-7&author=D.+S.+Syrdal&author=K.+L.+Koay&author=M.+G%C3%A1csi&author=M.+L.+Walters&author=K.+Dautenhahn&title=Video+prototyping+of+dog-inspired+non-verbal+a%EF%AC%80ective+communication+for+an+appearance+constrained+robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0049\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">The boy-robot should bark!\u2014Children's impressions of agent migration into diverse embodiments</span>. In <i>Proceedings of the new frontiers in human\u2013robot interaction, a symposium at the AISB2009 Convention</i>, <span class=\"NLM_publisher-loc\">Edinburgh, UK</span>, April <span class=\"NLM_fpage\">6</span>\u2013<span class=\"NLM_lpage\">9</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=6-9&author=D.+S.+Syrdal&author=K.+L.+Koay&author=M.+L.+Walters&author=K.+Dautenhahn&title=The+boy-robot+should+bark%21%E2%80%94Children%27s+impressions+of+agent+migration+into+diverse+embodiments\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0050\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Otero</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Video prototyping in human\u2013robot interaction: Results from a qualitative study</span>. In <i>Proceedings of the 15th European conference on cognitive ergonomics: The ergonomics of cool interaction</i>, <span class=\"NLM_fpage\">29</span>\u2013<span class=\"NLM_lpage\">35</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0050&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1473018.1473055\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=29-35&author=D.+S.+Syrdal&author=N.+Otero&author=K.+Dautenhahn&title=Video+prototyping+in+human%E2%80%93robot+interaction%3A+Results+from+a+qualitative+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0051\"><span><span class=\"hlFld-ContribAuthor\">Tapus, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Tapus</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. J.</span> Matari\u0107</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_article-title\">User\u2013robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy</span>. <i>Intelligent Service Robotics</i> 1(2): <span class=\"NLM_fpage\">169</span>\u2013<span class=\"NLM_lpage\">83</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1007/s11370-008-0017-4</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0051&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2Fs11370-008-0017-4\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=2008&pages=169-83&issue=2&author=A.+Tapus&author=C.+Tapus&author=M.+J.+Matari%C4%87&title=User%E2%80%93robot+personality+matching+and+assistive+robot+behavior+adaptation+for+post-stroke+rehabilitation+therapy&doi=10.1007%2Fs11370-008-0017-4\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0052\"><span><span class=\"hlFld-ContribAuthor\">Walker, <span class=\"NLM_given-names\">J. E.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Howland</span>. <span class=\"NLM_year\">1991</span>. <span class=\"NLM_article-title\">Falls and fear of falling among elderly persons living in the community: Occupational therapy interventions</span>. <i>American Journal of Occupational Therapy</i> 45(2): <span class=\"NLM_fpage\">119</span>\u2013<span class=\"NLM_lpage\">22</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.5014/ajot.45.2.119</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0052&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.5014%2Fajot.45.2.119\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0052&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=2035588\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0052&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1991EW43400004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=45&publication_year=1991&pages=119-22&issue=2&author=J.+E.+Walker&author=J.+Howland&title=Falls+and+fear+of+falling+among+elderly+persons+living+in+the+community%3A+Occupational+therapy+interventions&doi=10.5014%2Fajot.45.2.119\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0053\"><span><span class=\"hlFld-ContribAuthor\">Walters, <span class=\"NLM_given-names\">M. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Lohse</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Hanheide</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Wrede</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Green</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> H\u00fcttenrauch</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Sagerer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Severinson Eklund</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_article-title\">Evaluating the robot personality and verbal behavior of domestic robots using video-based studies</span>. <i>Advanced Robotics</i> 25(18): <span class=\"NLM_fpage\">2233</span>\u2013<span class=\"NLM_lpage\">54</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1163/016918611X603800</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0053&amp;dbid=20&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1163%2F016918611X603800&amp;tollfreelink=2_18_24537b9100757d6fa3e14b34d82744337f494015f85d55a2df0627a244ec9fc3\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=cit0053&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000297801000001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=25&publication_year=2011&pages=2233-54&issue=18&author=M.+L.+Walters&author=M.+Lohse&author=M.+Hanheide&author=B.+Wrede&author=D.+S.+Syrdal&author=K.+L.+Koay&author=A.+Green&author=H.+H%C3%BCttenrauch&author=K.+Dautenhahn&author=G.+Sagerer&author=K.+Severinson+Eklund&title=Evaluating+the+robot+personality+and+verbal+behavior+of+domestic+robots+using+video-based+studies&doi=10.1163%2F016918611X603800\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0054\"><span><span class=\"hlFld-ContribAuthor\">Walters, <span class=\"NLM_given-names\">M. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. A.</span> Oskoei</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_chapter-title\">A long-term human\u2013robot proxemic study</span>. In <i>2011 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">137</span>\u2013<span class=\"NLM_lpage\">42</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0054&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2011.6005274\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=137-42&author=M.+L.+Walters&author=M.+A.+Oskoei&author=D.+S.+Syrdal&author=K.+Dautenhahn&title=A+long-term+human%E2%80%93robot+proxemic+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0055\"><span><span class=\"hlFld-ContribAuthor\">Yagoda, <span class=\"NLM_given-names\">R. E.</span></span> <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Development of the human robot interaction workload measurement tool (HRI-WM)</span>. <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 54: <span class=\"NLM_fpage\">304</span>\u2013<span class=\"NLM_lpage\">8</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0055&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1177%2F154193121005400408\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=304-8&author=R.+E.+Yagoda&title=Development+of+the+human+robot+interaction+workload+measurement+tool+%28HRI-WM%29\" target=\"_blank\">[Google Scholar]</a></span></span></span></li></ul></div><div class=\"response\"><div class=\"sub-article-title\"></div></div>\n</article>\n</div>\n<div class=\"tab tab-pane\" id=\"relatedContent\">\n</div>\n<div class=\"tab tab-pane \" id=\"metrics-content\">\n<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics\" data-pb-dropzone=\"publicationContentDropZoneMetrics\">\n\n<div class=\"widget literatumArticleMetricsWidget none  widget-none\" id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"literatumArticleMetricsWidget\" data-ajax-widget-id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"access__limit\" data-pb-dropzone=\"accessLimitPage\">\n</div>\n</div>\n</div>\n</div>\n<input id=\"viewLargeImageCaption\" type=\"hidden\" value=\"View Large Image\" /></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-4 \">\n<div class=\"contents\" data-pb-dropzone=\"contents2\">\n<div class=\"widget general-bookmark-share none  widget-none  widget-compact-all\" id=\"c8494935-e102-4ff5-9395-4ffa44a77f1c\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\">\n<ul>\n<li>\n<div class=\"addthis_toolbox addthis_20x20_style\">\n<div class=\"custom_images\">\n<a class=\"addthis_button_twitter\">\n<span class=\"at-icon-twitter\"></span>\n</a>\n<a class=\"addthis_button_facebook\">\n<span class=\"at-icon-facebook\"></span>\n</a>\n<a class=\"addthis_button_email\">\n<span class=\"at-icon-email\"></span>\n</a>\n<a class=\"addthis_button_none\">\n<span class=\"at-icon-none\"></span>\n</a>\n<a class=\"addthis_button_compact\" tabindex=\"-1\"><span class=\"at-icon-wrapper\"></span>\n<span aria-describedby=\"shareOptions-description\">\n<span class=\"off-screen\" id=\"shareOptions-description\">More Share Options</span>\n</span>\n</a>\n</div>\n</div>\n</li>\n</ul>\n<script type=\"text/javascript\">\n    \n    var script = document.createElement('script');\n    script.type='text/javascript';\n    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';\n    script.async = true;\n    $('head').append(script)\n</script>\n</div>\n</div>\n</div>\n<div class=\"widget general-html none  widget-none\" id=\"16111d74-c554-42b2-a277-f2727ad2b285\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \">&nbsp;</div>\n</div>\n</div>\n<div class=\"widget layout-tabs none further-fonts collapsed-view further-tab-margin collapsed-sticky widget-none  widget-compact-vertical\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"layout-tabs-dropzone\" data-pb-dropzone=\"layoutTabsDropzone\">\n\n<div class=\"widget general-html none furtherReadingTitle widget-none  widget-compact-all\" id=\"982b80ad-6fe6-4b98-9675-9b6eef03d365\">\n<div class=\"wrapped \">\n<h2 class=\"widget-header header-none  header-compact-all\">Related research <span class=\"tooltip-collapse\"></span></h2>\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"info hide\">\n<p><b>People also read</b> lists articles that other readers of this article have read.</p>\n<p><b>Recommended articles</b> lists articles that we recommend and is powered by our AI driven recommendation engine.</p>\n<p><b>Cited by</b> lists all citing articles based on Crossref citations.<br />Articles with the Crossref icon will open in a new tab.</p>\n</div></div>\n</div>\n</div>\n</div>\n<div class=\"tabs tabs-widget \" aria-live=\"polite\" aria-atomic=\"true\" aria-relevant=\"additions\">\n<ul class=\"tab-nav\" role=\"tablist\">\n<li class=\"active\" role=\"tab\">\n<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" title=\"show People also read\" class=\"\">People also read</a>\n</li>\n<li class=\"\" role=\"tab\">\n<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\" title=\"show Recommended articles\" class=\"\">Recommended articles</a>\n</li>\n<li class=\"\" role=\"tab\">\n<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" title=\"show Cited by\" class=\"frwidget-tabs--cby\">Cited by</a>\n</li>\n</ul>\n<div class=\"tab-content\">\n<div class=\"tab-pane active\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\">\n<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" data-pb-dropzone-name=\"People also read\">\n<div class=\"widget ajaxCFCRWidget none  widget-none  widget-compact-all\" id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCFCRWidget\" data-ajax-widget-id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\">\n<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-b6de7b7c-de82-45a5-9538-313dd15c6659\" data-pb-dropzone-name=\"Recommended articles\">\n<div class=\"widget ajaxAtmCRWidget none  widget-none  widget-compact-all\" id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxAtmCRWidget\" data-ajax-widget-id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\" data-ajax-spin=\"true\" data-ajax-observe=\"true\"></div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\">\n<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" data-pb-dropzone-name=\"Cited by\">\n\n<div class=\"widget ajaxCitedByWidget none  widget-none  widget-compact-all\" id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCitedByWidget\" data-ajax-widget-id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n<div class=\"widget pageFooter none  widget-none  widget-compact-all\" id=\"d97c173f-d838-4de1-bbd7-ed69f0d36a91\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><footer class=\"page-footer\">\n<div data-pb-dropzone=\"main\">\n<div class=\"widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all\" id=\"1f15adc0-4a59-4d27-93fe-8cbb14a5108a\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md gutterless \">\n<div class=\"col-md-1-1 fit-padding\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget pbOptimizerWidget none  widget-none  widget-compact-all\" id=\"af788167-0054-4892-bd47-5de7cbd64256\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div data-optimizer data-widget-id=\"af788167-0054-4892-bd47-5de7cbd64256\" id=\"widget-af788167-0054-4892-bd47-5de7cbd64256\" data-observer>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"64a44adf-45ed-4da3-be26-ef25beb9dbee\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"6918e9df-910a-4206-9bd0-1a02bc17f740\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container-fluid\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-1-2 footer_left_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"aa9510dd-52ed-4b74-8211-fb510cd9468e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Information for</h3>\n<ul>\n<li><a href=\"https://authorservices.taylorandfrancis.com/\">Authors</a></li>\n<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/\">Corporate partners</a></li>\n<li><a href=\"https://editorresources.taylorandfrancisgroup.com/\">Editors</a></li>\n<li><a href=\"/page/librarians\">Librarians</a></li>\n<li><a href=\"/societies\">Societies</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-sm-1-2 footer_right_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Open access</h3>\n<ul>\n<li><a href=\"/openaccess\">Overview</a></li>\n<li><a href=\"/openaccess/openjournals\">Open journals</a></li>\n<li><a href=\"/openaccess/openselect\">Open Select</a></li>\n<li><a href=\"/openaccess/dove\">Dove Medical Press</a></li>\n<li><a href=\"/openaccess/f1000\">F1000Research</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"fc564559-f496-499c-87c7-d851f371f061\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container-fluid\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-1-2 footer_left_col\">\n <div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"cdd1a577-15dc-4271-8941-33a105ec6510\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Opportunities</h3>\n<ul>\n<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/marketing/\">Reprints and e-prints</a></li>\n<li><a href=\"https://taylorandfrancis.com/partnership/commercial/advertising-solutions/\" class=\"footer-ad-click\">Advertising solutions</a></li>\n<li><a href=\"https://taylorandfrancis.com/partnership/commercial/accelerated-publication/\">Accelerated publication</a></li>\n<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/purchasing-options/\">Corporate access solutions</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-sm-1-2 footer_right_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"f3fb3d36-db42-4373-9d0e-432958bf2fbc\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Help and information</h3>\n<ul>\n<li><a href=\"https://help.tandfonline.com\">Help and contact</a></li>\n<li><a href=\"https://newsroom.taylorandfrancisgroup.com/\">Newsroom</a></li>\n<li><a href=\"/action/showPublications?pubType=journal\">All journals</a></li>\n<li><a href=\"https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot\">Books</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"b2eecf80-9109-455e-a805-028552718986\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"b997c64c-ce48-41ce-b3d6-9cb2d1c99131\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container-fluid\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-1-2 footer_left_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"914433f6-0ea6-4a47-9781-07564061be86\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-social-label\">\n<h3>Keep up to date</h3>\n</div>\n<div class=\"font-size-correction-sml\">Register to receive personalised research and resources by email</div>\n<div class=\"bs\">\n<div class=\"pull-left links font-size-correction\">\n<a class=\"font-size-correction-link\" href=\"https://taylorandfrancis.formstack.com/forms/tfoguest_signup\"><i class=\"fa fa-envelope-square\" title=\"Register to receive personalised research and resources by email\"></i>Sign me up</a>\n</div></div></div>\n</div>\n</div>\n<div class=\"widget literatumSocialLinks none  widget-none  widget-compact-all\" id=\"3b6a5e53-cd62-452f-adc1-92e187a0849d\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"bs\">\n<div class=\"pull-left links\">\n<a href=\"http://facebook.com/TaylorandFrancisGroup\">\n<i class=\"icon-facebook\" title=\"Taylor and Francis Group Facebook page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"fb-description\">\n<span class=\"off-screen\" id=\"fb-description\">Taylor and Francis Group Facebook page</span>\n</span>\n</a>\n</div>\n<div class=\"pull-left links\">\n<a href=\"https://twitter.com/tandfonline\">\n<i class=\"fa fa-twitter-square\" title=\"Taylor and Francis Group Twitter page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"twitter-description\">\n<span class=\"off-screen\" id=\"twitter-description\">Taylor and Francis Group Twitter page</span>\n</span>\n</a>\n</div>\n<div class=\"pull-left links\">\n<a href=\"http://linkedin.com/company/taylor-&-francis-group\">\n<i class=\"fa fa-linkedin-square\" title=\"Taylor and Francis Group LinkedIn page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"linkedin-description\">\n<span class=\"off-screen\" id=\"linkedin-description\">Taylor and Francis Group Linkedin page</span>\n</span>\n</a>\n</div>\n<div class=\"clearfix\"></div>\n<div class=\"pull-left links\">\n<a href=\"https://www.youtube.com/user/TaylorandFrancis\">\n<i class=\"fa fa-youtube-square\" title=\"Taylor and Francis Group YouTube page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"youtube-description\">\n<span class=\"off-screen\" id=\"youtube-description\">Taylor and Francis Group Youtube page</span>\n</span>\n</a>\n</div>\n<div class=\"pull-left links\">\n<a href=\"http://www.weibo.com/tandfchina\">\n<i class=\"fa fa-weibo\" title=\"Taylor and Francis Group Weibo page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"weibo-description\">\n<span class=\"off-screen\" id=\"weibo-description\">Taylor and Francis Group Weibo page</span>\n</span>\n</a>\n</div>\n<div class=\"clearfix\"></div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-sm-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none  widget-none  widget-compact-horizontal\" id=\"8d803f96-081d-4768-ab7d-280a77af723b\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-3-4 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget general-html none footer-info-container widget-none  widget-compact-vertical\" id=\"b247ecb9-84c9-4762-b270-20f8be1f0ae4\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"informa-group-info\">\n<span>Copyright \u00a9 2021 Informa UK Limited</span>\n<span><a href=\"https://informa.com/privacy-policy/\">Privacy policy</a></span>\n<span><a href=\"/cookies\">Cookies</a></span>\n<span><a href=\"/terms-and-conditions\">Terms & conditions</a></span>\n<span><a href=\"/accessibility\">Accessibility</a></span>\n<p>Registered in England & Wales No. 3099067<br />\n5 Howick Place | London | SW1P 1WG</p>\n</div></div>\n</div>\n</div>\n </div>\n</div>\n<div class=\"col-sm-1-4 footer_tandf_logo\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget general-image none  widget-none  widget-compact-vertical\" id=\"b6bde365-079b-454f-94f6-1841291656a1\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><a href=\"http://taylorandfrancis.com/\" title=\"Taylor and Francis Group\">\n<img src=\"/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png\" alt=\"Taylor and Francis Group\" />\n</a></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget cookiePolicy none  widget-none  widget-compact-all\" id=\"cea739ac-da2c-4d77-9cf1-cb3e0da7e31e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"banner\">\n<a href=\"#\" class=\"btn\">Accept</a>\n<p class=\"message\">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href=\"/cookies\">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>\n</div></div>\n</div>\n</div>\n</div>\n</footer></div>\n</div>\n</div>\n</div>\n</div>\n<script type=\"text/javascript\" src=\"/wro/kriw~product.js\"></script>\n<script>\nloadCSS(\"/wro/kriw~lastInBody-css.css\");\nloadCSS(\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\");\nwindow.scriptSettings=[{js: \"/wro/kriw~jwplayer.js\",selector:'.mediaThumbnailContainer'},\n{js:'/wro/kriw~ajax-widgets.js',css:\"/wro/kriw~ajax-widgets.css\",selector:'.ajaxWidget'},\n{js: '/wro/kriw~loi-api.js',selector:'.toc-fns,.literatumListOfIssuesResponsiveWidget,.literatumListOfIssuesWidget'}\n,{js:\"/wro/kriw~seamless-access-fn.js\",selector: \".seamlessAccess_wrapper,.institutional-login\"}];\nwindow.addEventListener('load',TandfUtils.scriptLoader);\n</script>\n<noscript>\n    <link rel=\"stylesheet\" href=\"/wro/kriw~lastInBody-css.css\">\n    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\">\n</noscript>\n<script defer src=\"https://static.cloudflareinsights.com/beacon.min.js/v64f9daad31f64f81be21cbef6184a5e31634941392597\" integrity=\"sha512-gV/bogrUTVP2N3IzTDKzgP0Js1gg4fbwtYB6ftgLbKQu/V8yH2+lrKCfKHelh4SO3DPzKj4/glTO+tNJGDnb0A==\" data-cf-beacon='{\"rayId\":\"6b66d9e21bdc7986\",\"token\":\"b6951d00f50a499ab38e94f58955e14d\",\"version\":\"2021.11.0\",\"si\":100}' crossorigin=\"anonymous\"></script>\n</body>\n</html>\n", "response": ["GET /doi/abs/10.1080/01972243.2015.1020212 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "HTTP/1.1 302 Found", "Date: Tue, 30 Nov 2021 20:33:29 GMT", "Content-Type: text/html; charset=utf-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "Cache-Control: private", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Set-Cookie: JSESSIONID=c8e957bc-31b4-4bce-ba29-de4b933ea25e; path=/; domain=.tandfonline.com; age=-1; HttpOnly; SameSite=None; Secure", "Set-Cookie: JSESSIONID=c8e957bc-31b4-4bce-ba29-de4b933ea25e; path=/; domain=.tandfonline.com; age=-1; HttpOnly; SameSite=None; Secure", "Set-Cookie: SERVER=WZ6myaEXBLGG7Eibg8MVDg==; domain=.tandfonline.com; path=/; secure; HttpOnly", "Set-Cookie: MAID=WL3p5sQMhB2yeHgV3lhrPw==; domain=.tandfonline.com; path=/; secure; expires=Mon, 26-Sep-2022 20:33:29 GMT; HttpOnly", "Set-Cookie: MACHINE_LAST_SEEN=2021-11-30T12%3A33%3A29.196-08%3A00; domain=.tandfonline.com; path=/; secure; expires=Mon, 26-Sep-2022 20:33:29 GMT; HttpOnly", "Set-Cookie: I2KBRCK=1; domain=.tandfonline.com; path=/; secure; expires=Wed, 30-Nov-2022 20:33:29 GMT; HttpOnly", "P3P: CP=\"NOI DSP ADM OUR IND OTC\"", "Location: https://www.tandfonline.com/doi/abs/10.1080/01972243.2015.1020212?cookieSet=1", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66d9dc88617986-GRU", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "The URL has moved <a href=\"https://www.tandfonline.com/doi/abs/10.1080/01972243.2015.1020212?cookieSet=1\">here</a>", "GET /doi/abs/10.1080/01972243.2015.1020212?cookieSet=1 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: JSESSIONID=c8e957bc-31b4-4bce-ba29-de4b933ea25e; SERVER=WZ6myaEXBLGG7Eibg8MVDg==; MAID=WL3p5sQMhB2yeHgV3lhrPw==; MACHINE_LAST_SEEN=2021-11-30T12%3A33%3A29.196-08%3A00; I2KBRCK=1", "HTTP/1.1 302 Found", "Date: Tue, 30 Nov 2021 20:33:29 GMT", "Content-Type: text/html; charset=utf-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "Cache-Control: private", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Location: https://www.tandfonline.com/doi/abs/10.1080/01972243.2015.1020212", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66d9de3c337986-GRU", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "The URL has moved <a href=\"https://www.tandfonline.com/doi/abs/10.1080/01972243.2015.1020212\">here</a>", "GET /doi/abs/10.1080/01972243.2015.1020212 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: I2KBRCK=1; JSESSIONID=c8e957bc-31b4-4bce-ba29-de4b933ea25e; MACHINE_LAST_SEEN=2021-11-30T12%3A33%3A29.196-08%3A00; MAID=WL3p5sQMhB2yeHgV3lhrPw==; SERVER=WZ6myaEXBLGG7Eibg8MVDg==", "HTTP/1.1 302 Found", "Date: Tue, 30 Nov 2021 20:33:29 GMT", "Content-Type: text/html; charset=utf-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Cache-Control: no-cache", "Pragma: no-cache", "X-Webstats-RespID: 105fc3a7dfbd270df79555d77f44974b", "Location: https://www.tandfonline.com/doi/full/10.1080/01972243.2015.1020212", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66d9dfef5d7986-GRU", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "The URL has moved <a href=\"https://www.tandfonline.com/doi/full/10.1080/01972243.2015.1020212\">here</a>", "GET /doi/full/10.1080/01972243.2015.1020212 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: I2KBRCK=1; JSESSIONID=c8e957bc-31b4-4bce-ba29-de4b933ea25e; MACHINE_LAST_SEEN=2021-11-30T12%3A33%3A29.196-08%3A00; MAID=WL3p5sQMhB2yeHgV3lhrPw==; SERVER=WZ6myaEXBLGG7Eibg8MVDg==", "HTTP/1.1 200 OK", "Date: Tue, 30 Nov 2021 20:33:30 GMT", "Content-Type: text/html; charset=UTF-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Cache-Control: no-cache, no-store", "Pragma: no-cache", "X-Webstats-RespID: d0dd611d4cf8ad1746f90d35abd4fd50", "Content-Language: en", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66d9e21bdc7986-GRU", "Content-Encoding: gzip", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "<!DOCTYPE html>", "<html lang=\"en\" class=\"pb-page\" data-request-id=\"88e9bb19-6c8c-4dd1-aa13-adf1de87c691\"><head data-pb-dropzone=\"head\"><meta name=\"pbContext\" content=\";requestedJournal:journal:utis20;issue:issue:10.1080/utis20.v031.i03;csubtype:string:Special;page:string:Article/Chapter View;ctype:string:Journal Content;article:article:10.1080/01972243.2015.1020212;journal:journal:utis20;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;subPage:string:Full Text\" />", "<link rel=\"schema.DC\" href=\"http://purl.org/DC/elements/1.0/\" /><meta name=\"citation_journal_title\" content=\"The Information Society\" /><meta name=\"dc.Title\" content=\"Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping\" /><meta name=\"dc.Creator\" content=\"Dag Sverre  Syrdal\" /><meta name=\"dc.Creator\" content=\"Kerstin  Dautenhahn\" /><meta name=\"dc.Creator\" content=\"Kheng Lee  Koay\" /><meta name=\"dc.Creator\" content=\"Wan Ching  Ho\" /><meta name=\"dc.Subject\" content=\"assistive robotics; domestic robots; human\u2013robot interaction; prototyping\" /><meta name=\"dc.Description\" content=\"In order to investigate how the use of robots may impact everyday tasks, twelve participants in our study interacted with a University of Hertfordshire Sunflower robot over a period of 8 weeks in t...\" /><meta name=\"Description\" content=\"In order to investigate how the use of robots may impact everyday tasks, twelve participants in our study interacted with a University of Hertfordshire Sunflower robot over a period of 8 weeks in t...\" /><meta name=\"dc.Publisher\" content=\"Routledge\" /><meta name=\"dc.Date\" scheme=\"WTN8601\" content=\"13 May 2015\" /><meta name=\"dc.Type\" content=\"research-article\" /><meta name=\"dc.Format\" content=\"text/HTML\" /><meta name=\"dc.Identifier\" scheme=\"publisher-id\" content=\"1020212\" /><meta name=\"dc.Identifier\" scheme=\"doi\" content=\"10.1080/01972243.2015.1020212\" /><meta name=\"dc.Source\" content=\"https://doi.org/10.1080/01972243.2015.1020212\" /><meta name=\"dc.Language\" content=\"en\" /><meta name=\"dc.Coverage\" content=\"world\" /><meta name=\"dc.Rights\" content=\"\u00a9 2015 The Author(s). Published with license by Taylor &amp; Francis\u00a9 Dag Sverre Syrdal, Kerstin Dautenhahn, Kheng Lee Koay, and Wan Ching Ho.\" /><meta name=\"keywords\" content=\"assistive robotics,domestic robots,human\u2013robot interaction,prototyping\" /><meta name=\"citation_fulltext_world_readable\" content=\"\" />", "<link rel=\"meta\" type=\"application/atom+xml\" href=\"https://doi.org/10.1080%2F01972243.2015.1020212\" />", "<link rel=\"meta\" type=\"application/rdf+json\" href=\"https://doi.org/10.1080%2F01972243.2015.1020212\" />", "<link rel=\"meta\" type=\"application/unixref+xml\" href=\"https://doi.org/10.1080%2F01972243.2015.1020212\" />", "<title>Full article: Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</title>", "<meta charset=\"UTF-8\">", "<meta name=\"robots\" content=\"noarchive\" />", "<meta property=\"og:title\" content=\"Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping\" />", "<meta property=\"og:type\" content=\"article\" />", "<meta property=\"og:url\" content=\"https://www.tandfonline.com/doi/abs/10.1080/01972243.2015.1020212\" />", "<meta property=\"og:image\" content=\"https://www.tandfonline.com/doi/cover-img/10.1080/utis20.v031.i03\" />", "<meta property=\"og:site_name\" content=\"Taylor & Francis\" />", "<meta property=\"og:description\" content=\"(2015). Integrating Constrained Experiments in Long-Term Human&#x2013;Robot Interaction Using Task- and Scenario-Based Prototyping. The Information Society: Vol. 31, Beyond Industrial Robotics: Social Robots Entering Public and Domestic Spheres, pp. 265-283.\" />", "<meta name=\"twitter:card\" content=\"summary_large_image\">", "<meta name=\"twitter:site\" content=\"@tandfonline\">", "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />", "<link href=\"//connect.facebook.net\" rel=\"preconnect\" />", "<link href=\"//go.taylorandfrancis.com\" rel=\"preconnect\" />", "<link href=\"//pi.pardot.com\" rel=\"preconnect\" />", "<link href=\"//static.hotjar.com\" rel=\"preconnect\" />", "<link href=\"//cdn.pbgrd.com\" rel=\"preconnect\" />", "<link href=\"//f1-eu.readspeaker.com\" rel=\"preconnect\" />", "<link href=\"//www.googleadservices.com\" rel=\"preconnect\" />", "<link href=\"https://m.addthis.com\" rel=\"preconnect\" />", "<link href=\"https://wl.figshare.com\" rel=\"preconnect\" />", "<link href=\"https://pagead2.googlesyndication.com\" rel=\"preconnect\" />", "<link href=\"https://www.googletagmanager.com\" rel=\"preconnect\" />", "<link href=\"https://www.google-analytics.com\" rel=\"preconnect\" />", "<link href=\"https://fonts.googleapis.com\" rel=\"preconnect\" />", "<link href=\"https://fonts.gstatic.com\" rel=\"preconnect\" />", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/css/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/icomoon/icomoon.woff?g276mb\" type=\"font/woff\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-regular.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link type=\"text/css\" rel=\"stylesheet\" href=\"/wro/kriw~product.css\">", "<link rel=\"stylesheet\" type=\"text/css\" href=\"/pb/css/t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" id=\"pb-css\" data-pb-css-id=\"t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" />", "<script type=\"text/javascript\" src=\"//cdn.pbgrd.com/core-tandf.js\" async></script>", "<script data-ad-client=\"ca-pub-5143040550582507\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\" async></script>", "<script>", "    (function(h,o,t,j,a,r){", "        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};", "        h._hjSettings={hjid:864760,hjsv:6};", "        a=o.getElementsByTagName('head')[0];", "        r=o.createElement('script');r.async=1;", "        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;", "        a.appendChild(r);", "    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');", "</script>", "<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>", "<script type=\"text/javascript\">", "        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};", "    </script>", "<script type=\"application/javascript\" src=\"//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr\" id=\"read-speaker\" async></script>", "<script>var tandfData={\"search\":{\"cbRec\":1},\"seamlessAccess\":{\"apiUrl\":\"https://service.seamlessaccess.org/ps/\",\"context\":\"seamlessaccess.org\"},\"identity\":{\"isSpv\":false,\"isAuthenticated\":false},\"pubCount\":{\"citedCount\":7},\"actionLog\":{\"eventGroupKey\":\"34a360bb-b0c7-4bd7-9698-5d10d67fc9b7\"}};</script>", "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">", "<link rel=\"canonical\" href=\"https://www.tandfonline.com/doi/full/10.1080/01972243.2015.1020212\" />", "</head>", "<body class=\"pb-ui\">", "<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>", "<noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>", "<script type=\"text/javascript\" src=\"/wro/kriw~jquery-3.5.0.js\"></script>", "<div class=\"skipContent off-screen\"><a href=\"#top-content-scroll\" class=\"skipToContent\" title=\"Skip to Main Content\" tabIndex=\"0\">Skip to Main Content</a></div>", "<script type=\"text/javascript\">(function(e){var t=e.getElementsByClassName(\"skipToContent\");t.length>0&&(t[0].onclick=function(){var t=e.getElementById(\"top-content-scroll\");null==t&&(t=e.getElementsByClassName(\"top-content-scroll\").item(0)),t.setAttribute(\"tabindex\",\"0\"),t.focus()})})(document);</script>", "<div id=\"pb-page-content\" data-ng-non-bindable>", "<div data-pb-dropzone=\"main\" data-pb-dropzone-name=\"Main\">", "<div class=\"widget pageHeader none  widget-none  widget-compact-all\" id=\"a4d4fdd3-c594-4d68-9f06-b69b8b37ed56\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><header class=\"page-header\" aria-label=\"Main Banner\">", "<div data-pb-dropzone=\"main\">", "<div class=\"widget responsive-layout none header-top widget-none  widget-compact-all\" id=\"036fa949-dc25-4ffe-9df0-d7daefee281b\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-xs  \">", "<div class=\"col-xs-1-6 header-index\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal\" id=\"e817489e-2520-418b-a731-b62e247e74df\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\" title=\"Taylor and Francis Online\">", "<img src=\"/pb-assets/Global/tfo_logo-1444989687640.png\" alt=\"Taylor and Francis Online\" />", "</a></div>", "</div>", "</div>", "<div class=\"widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal\" id=\"b3fe8380-8b88-4558-b004-6485d3aea155\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\">", "<img src=\"/pb-assets/Global/tfo_logo_sm-1459688573210.png\" />", "</a></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-xs-5-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget layout-inline-content alignRight  widget-none  widget-compact-all\" id=\"a8a37801-55c7-4566-bdef-e4e738967e38\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">", "<div class=\"widget layout-inline-content none customLoginBar widget-none\" id=\"fbe90803-b9c8-4bef-9365-cb53cc4bfa0e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">", "<div class=\"widget literatumInstitutionBanner none bannerWidth widget-none\" id=\"3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"institution-image hidden-xs logout-institution-image\">", "</div></div>", "</div>", "</div>", "<div class=\"widget literatumNavigationLoginBar none  widget-none  widget-compact-all\" id=\"1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"loginBar not-logged-in\">", "<span class=\"icon-user\"></span>", "<a href=\"/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F01972243.2015.1020212\" class=\"sign-in-link\">", "Log in", "</a>", "<span class=\"loginSeprator\">&nbsp;|&nbsp;</span>", "<a href=\"/action/registration?redirectUri=%2F\" class=\"register-link\">", "Register", "</a>", "</div></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget eCommerceCartIndicatorWidget none literatumCartLink widget-none\" id=\"9de10bb5-08af-48bc-b9f6-3f6433229f3e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><a href=\"/action/showCart?FlowID=1\" class=\"cartLabel\">", "<span class=\"hidden-xs hidden-sm visible-tl-inline-block\">Cart</span>", "<span class=\"cartItems\" data-id=\"cart-size\" role=\"status\">", "</span>", "</a></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none breadcrumbs-container widget-none  widget-compact-all\" id=\"64c16283-4b04-4d90-ac0f-4db85fcd0cf5\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumBreadcrumbs none breadcrumbs-widget widget-none  widget-compact-all\" id=\"b1c121c1-cbbd-4241-8774-7120f3a783e8\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\">", "<nav aria-label=\"Breadcrumb\">", "<ol class=\"breadcrumbs\">", "<li class=\"\">", "<a href=\"/\" class=\"bc-click\">", "Home", "</a>", "</li>", "<li class=\"\">", "<a href=\"/action/showPublications?pubType=journal\" class=\"bc-click\">", "All Journals", "</a>", "</li>", "<li class=\"\">", "<a href=\"/toc/utis20/current\" class=\"bc-click\">", "The Information Society", "</a>", "</li>", "<li class=\"\">", "<a href=\"/loi/utis20\" class=\"bc-click\">", " List of Issues", "</a>", "</li>", "<li class=\"\">", "<a href=\"/toc/utis20/31/3\" class=\"bc-click\">", "Volume 31, Issue 3", "</a>", "</li>", "<li class=\"\">", "<a href=\"#\" class=\"bc-click\" aria-current=\"page\">", "Integrating Constrained Experiments in L ....", "</a>", "</li>", "</ol>", "</nav>", "<script type=\"application/ld+json\">", "    {", "        \"@context\": \"https://schema.org\",", "        \"@type\": \"BreadcrumbList\",", "        \"itemListElement\":", "        [{", "            \"@type\": \"ListItem\",", "            \"position\": \"1\",", "            \"name\": \"Home\"", "            ,\"item\": \"https://www.tandfonline.com/\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"2\",", "            \"name\": \"All Journals\"", "            ,\"item\": \"https://www.tandfonline.com/action/showPublications?pubType=journal\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"3\",", "            \"name\": \"The Information Society\"", "            ,\"item\": \"https://www.tandfonline.com/toc/utis20/current\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"4\",", "            \"name\": \"List of Issues\"", "            ,\"item\": \"https://www.tandfonline.com/loi/utis20\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"5\",", "            \"name\": \"Volume 31, Issue 3\"", "            ,\"item\": \"https://www.tandfonline.com/toc/utis20/31/3\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"6\",", "            \"name\": \"Integrating Constrained Experiments in L .... \"", "            ", "        }", "        ]", "    }", "</script></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</header></div>", "</div>", "</div>", "<div data-widget-def=\"pageBody\" data-widget-id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\" role=\"main\">", "<div class=\"widget pageBody none  widget-none  widget-compact-all\" id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\">", "<div class=\"page-body pagefulltext\">", "<div data-pb-dropzone=\"main\">", "<div class=\"widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all\" id=\"1728e801-36cd-4288-9f53-392bad29506a\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md gutterless \">", "<div class=\"col-md-5-12 search_container \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget quickSearchWidget none search-customize-width widget-none  widget-compact-all\" id=\"d46e3260-1f5c-4802-821a-28a03a699c82\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"quickSearchFormContainer \">", "<form action=\"/action/doSearch\" name=\"quickSearch\" class=\"quickSearchForm \" title=\"Quick Search\" role=\"search\" method=\"get\" onsubmit=\"appendSearchFilters(this)\" aria-label=\"Quick Search\"><span class=\"simpleSearchBoxContainer\">", "<input name=\"AllField\" class=\"searchText main-search-field autocomplete\" value=\"\" type=\"search\" id=\"searchText\" title=\"Type search term here\" aria-label=\"Search\" placeholder=\"Enter keywords, authors, DOI, ORCID etc\" autocomplete=\"off\" data-history-items-conf=\"3\" data-publication-titles-conf=\"3\" data-publication-items-conf=\"3\" data-topics-conf=\"3\" data-contributors-conf=\"3\" data-fuzzy-suggester=\"false\" data-auto-complete-target=\"title-auto-complete\" />", "</span>", "<span class=\"searchDropDownDivRight\">", "<label for=\"searchInSelector\" class=\"visuallyhidden\">Search in:</label>", "<select id=\"searchInSelector\" name=\"SeriesKey\" class=\"js__searchInSelector\">", "<option value=\"utis20\" id=\"thisJournal\" data-search-in=\"thisJournal\">", "This Journal", "</option>", "<option value=\"\" data-search-in=\"default\">", "Anywhere", "</option>", "</select>", "</span>", "<span class=\"quick-search-btn\">", "<input class=\"mainSearchButton searchButtons pointer\" title=\"Search\" role=\"button\" type=\"submit\" value=\"\" aria-label=\"Search\" />", "</span></form>", "</div>", "<div class=\"advancedSearchLinkDropZone\" data-pb-dropzone=\"advancedSearchLinkDropZone\">", "<div class=\"widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all\" id=\"323e2a31-1c81-4995-bd17-8e149458c214\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><a href=\"/search/advanced\" class=\"advSearchArticle\">Advanced search</a></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-7-12 serNav_container\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget literatumSeriesNavigation none  widget-none\" id=\"7730bfe1-9fca-4cf4-a6d6-2a0148105437\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"issueSerialNavigation journal\">", "<div class=\"cover\">", "<img src=\"/action/showCoverImage?doi=10.1080/utis20.v031.i03\" alt=\"Publication Cover\" width=\"90\" height=\"120\" />", "</div>", "<div class=\"info trim-spaces\">", "<div class=\"title-container\">", "<h1 class=\"journal-heading\">", "<a href=\"/toc/utis20/current\">", "The Information Society", "</a>", "</h1>", "<div class=\"subtitle\">", "<span>", "An International Journal", "</span>", "</div>", "<span class=\"issue-heading\">", "Volume 31, 2015 - <a href=\"/toc/utis20/31/3\" class=\"nav-toc-list\">Issue 3</a><span class=\"specialTitle\"><a href=\"/toc/utis20/31/3\">: Beyond Industrial Robotics: Social Robots Entering Public and Domestic Spheres</a>", "</span>", "</span>", "</div>", "<div class=\"seriesNavDropZone\" data-pb-dropzone=\"seriesNavDropZone\">", "<div class=\"widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal\" id=\"753455df-1eeb-47ca-bdc9-e19022075973\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"serial-action\">", "<a href=\"https://rp.tandfonline.com/submission/create?journalCode&#x3D;UTIS\" class=\"green submitAnArticle\"><span>Submit an article</span></a>", "<a href=\"/toc/utis20/current\" class=\"jHomepage\"><span>Journal homepage</span></a>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none  widget-none  widget-compact-vertical\" id=\"e42aea8f-434a-4d39-aaef-f56af3ff00dc\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all\" id=\"6aacf107-e82d-494d-a14c-0c00bba52560\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"accessLogo\">", "<div>", "<img class=\"accessIconLocation\" src=\"/pb-assets/3rdPartyLogos/accessOA-1452596421933.png\" alt=\"Open access\" />", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none publicationContentHeader widget-none  widget-compact-all\" id=\"63f402e4-3498-4709-8d7d-ee8e69f93467\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical\" id=\"5afd8b6d-7e09-43ff-8ad6-afa3764e543c\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"articleMetricsContainer\">", "<div class=\"content compactView\">", "<div class=\"section\">", "<div class=\"value\">", "1,437", "</div>", "<div class=\"title\">", "Views", "</div>", "</div>", "<div class=\"section\">", "<div class=\"value\">", "7", "</div>", "<div class=\"title\">", "CrossRef citations to date", "</div>", "</div>", "<div class=\"section score\">", "<div class=\"altmetric-score true\">", "<div class=\"value\" data-doi=\"10.1080/01972243.2015.1020212\">", "<span class=\"metrics-score\">0</span>", "</div>", "<div class=\"title\">", "Altmetric", "</div>", "</div>", "</div>", "<script>tandfData.altmetric={key:'be0ef6915d1b2200a248b7195d01ef22'}</script>", "<script src=\"/wro/kriw~altmetric.js\" async></script>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-2-3 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all\" id=\"fa57727f-b942-4eb8-9ed2-ecfe11ac03f5\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div id=\"read-speaker-container\" style=\"display: block; height: 40px\">", "<div id=\"readspeaker_button1\" class=\"rs_skip rsbtn rs_preserve\">", "<a href=\"//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F01972243.2015.1020212\" rel=\"nofollow\" class=\"rsbtn_play\" accesskey=\"L\" title=\"Listen to this page using ReadSpeaker webReader\" style=\"border-radius: 0 11.4px 11.4px 2px;\">", "<span class=\"rsbtn_left rsimg rspart\"><span class=\"rsbtn_text\"><span>Listen</span></span></span>", "<span class=\"rsbtn_right rsimg rsplay rspart\"></span>", "</a>", "</div>", "</div>", "<div class=\"toc-heading\">ARTICLES</div>", "<h1><span class=\"NLM_article-title hlFld-title\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span></h1><div class=\"literatumAuthors\"><div class=\"publicationContentAuthors\"><div class=\"hlFld-ContribAuthor\"><span class=\"NLM_contrib-group\"><span class=\"contribDegrees corresponding \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Syrdal%2C+Dag+Sverre\">Dag Sverre Syrdal<i class=\"fa fa-envelope\" aria-hidden=\"true\" style=\"margin-left: 0.5em;\"></i></a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom<span class=\"corr-sec\"><span class=\"heading\">Correspondence</span><span class=\"corr-email\"><i class=\"fa fa-envelope\" style=\"color: #10147E; padding-right: 7px\" aria-hidden=\"true\"></i><a href=\"mailto:d.s.syrdal@herts.ac.uk\">d.s.syrdal@herts.ac.uk</a></span><br /></span></span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Dautenhahn%2C+Kerstin\">Kerstin Dautenhahn</a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Koay%2C+Kheng+Lee\">Kheng Lee Koay</a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom</span></div> &amp; </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Ho%2C+Wan+Ching\">Wan Ching Ho</a><span class=\"overlay\"> Adaptive Systems Research Group, School of Computer Science, University of Hertfordshire, Hatfield, United Kingdom</span></div></span></span></div></div></div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none  widget-none  widget-compact-all\" id=\"5f562208-b1d5-4e5a-81c7-356431240f04\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">", "<div class=\"row row-md gutterless \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget layout-inline-content none  widget-none  widget-compact-all\" id=\"87ac5840-18fa-4a14-8eca-065b90ede3d7\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">", "<div class=\"widget literatumContentItemPageRange none  widget-none  widget-compact-all\" id=\"45057865-d60c-414c-bc81-646debb621b0\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><span class=\"contentItemPageRange\">Pages 265-283", "</span></div>", "</div>", "</div>", "<div class=\"widget literatumContentItemHistory none  widget-none  widget-compact-all\" id=\"32bf868e-52ce-411a-9dc3-717743aad997\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div>Received 28 Dec 2013</div><div>Accepted 15 Aug 2014</div><div>Published online: 13 May 2015</div></div>", "</div>", "</div>", "<div class=\"widget literatumArticleToolsWidget none  widget-none  widget-compact-all\" id=\"ed673666-7b5d-470e-bd33-c5c679d996cb\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"articleTools\">", "<ul class=\"linkList\">", "<li class=\"downloadCitations\">", "<a href=\"/action/showCitFormats?doi=10.1080%2F01972243.2015.1020212\"><i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>Download citation</a>", "</li>", "<li class=\"dx-doi\">", "<a href=\"https://doi.org/10.1080/01972243.2015.1020212\"><i class=\"fa fa-external-link-square\" style=\"margin: 0 0.25rem 0 0\" aria-hidden=\"true\"></i>https://doi.org/10.1080/01972243.2015.1020212</a>", "</li>", "<script src=\"/wro/kriw~crossmark.js\" async></script>", "<li class=\"cross_mark\">", "<a class=\"cross_mark--link\" data-doi=\"10.1080/01972243.2015.1020212\" data-target=\"crossmark\" href=\"#\">", "<img src=\"/templates/jsp/images/CROSSMARK_Color_horizontal.svg\" alt=\"CrossMark Logo\" width=\"100\" height=\"22px\" />", "<span aria-describedby=\"crossMark-description\"><span class=\"off-screen\" id=\"crossMark-description\">CrossMark</span></span>", "</a>", "</li>", "</ul>", "</div></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents2\">", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none publicationContentBody widget-none\" id=\"f4a74f7a-9ba2-4605-86b1-8094cb1f01de\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget sectionsNavigation none  widget-none\" id=\"f15bd2de-bb18-4067-8ab9-03ea3be30bf7\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"sections-nav\" role=\"navigation\" aria-label=\"Article Navigation\"><span class=\"title\">In this article<a href=\"#\" class=\"close\" tabindex=\"-1\"><span aria-label=\"close button for mobile\"><span class=\"off-screen\" id=\"close-description\">Close</span></span></a></span><ul class=\"sections-list\"><li><span class=\"sub-art-heading\"><a href=\"#_i2\">PROTOTYPING OF HUMAN\u2013ROBOT INTERACTION</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i3\">UNIVERSITY OF HERTFORDSHIRE ROBOT HOUSE</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i4\">CONSTRUCTED PERSONAS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i5\">OPEN-ENDED SCENARIOS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i8\">CONSTRAINED EXPERIMENTS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i14\">RESEARCH QUESTIONS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i18\">METHODOLOGY</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i30\">RESULTS</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i45\">DISCUSSION</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i51\">CONCLUSIONS</a></span><ul class=\"sub-art-titles\"></ul></li><li><a href=\"#ack\">Acknowledgements</a></li><li><a href=\"#references-Section\">References</a></li></ul></div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-7-12 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget responsive-layout none rs_readArea widget-none  widget-compact-all\" id=\"9751b4f9-64b9-44c0-955b-f75246902839\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all\" id=\"d29f04e9-776c-4996-a0d8-931023161e00\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">", "    MathJax.Hub.Config({", "        \"HTML-CSS\": {scale: 70, linebreaks: {automatic: true, width: \"container\"}},", "        SVG: {linebreaks: {automatic: true, width: \"25%\"}},", "        menuSettings: {zoom: \"Click\"},", "        /* This is necessary to lazy loading. */", "        skipStartupTypeset: true", "    });", "</script>", "<script type=\"application/javascript\" async src=\"/wro/kriw~mathjax.js\"></script>", "<div class=\"articleMeta ja\">", "<div class=\"tocHeading\">", "<h2>ARTICLES</h2>", "</div>", "<div class=\"hlFld-Title\">", "<div class=\"publicationContentTitle\">", "<h1 class=\"chaptertitle\">", "Integrating Constrained Experiments in Long-Term Human&#x2013;Robot Interaction Using Task- and Scenario-Based Prototyping", "</h1>", "</div>", "</div>", "<div class=\"copyrightStatement\">", "</div>", "<div class=\"articleMetaDrop publicationContentDropZone\" data-pb-dropzone=\"articleMetaDropZone\">", "</div>", "<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone1\" data-pb-dropzone=\"articleMetaDropZone1\">", "</div>", "<div class=\"copyrightline\">", "</div>", "<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone2\" data-pb-dropzone=\"articleMetaDropZone2\">", "</div>", "</div>", "<div class=\"publication-tabs ja publication-tabs-dropdown\">", "<div class=\"tabs tabs-widget\">", "<ul class=\"tab-nav\" role=\"tablist\">", "<li class=\"active\" role=\"tab\">", "<a href=\"/doi/full/10.1080/01972243.2015.1020212?scroll=top&amp;needAccess=true\" class=\"show-full\">", "<i class=\"fa fa-file-text\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">", "Full Article", "</span>", "</a>", "</li>", "<li role=\"tab\">", "<a href=\"/doi/figure/10.1080/01972243.2015.1020212?scroll=top&amp;needAccess=true\" class=\"show-figure\">", "<i class=\"fa fa-image\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">Figures & data</span>", "</a>", "</li>", "<li role=\"tab\">", "<a href=\"/doi/ref/10.1080/01972243.2015.1020212?scroll=top\" class=\"show-references\">", "<i class=\"fa fa-book\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">References</span>", "</a>", "</li>", "<li class=\"citedbyTab \" role=\"tab\">", "<a href=\"/doi/citedby/10.1080/01972243.2015.1020212?scroll=top&amp;needAccess=true\">", "<i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">", "Citations", "</span>", "</a>", "</li>", "<li role=\"tab\" class=\"metrics-tab\">", "<a href=\"#metrics-content\" class=\"show-metrics\">", "<i class=\"fa fa-bar-chart\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">Metrics</span>", "</a>", "</li>", "<li role=\"tab\" class=\"licencing-tab \">", "<a href=\"/action/showCopyRight?scroll=top&amp;doi=10.1080%2F01972243.2015.1020212\" class=\"show-copyright\">", "<i class=\"fa fa-copyright\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">Licensing</span>", "</a>", "</li>", "<li role=\"tab\" class=\"permissions-tab \">", "<a href=\"/doi/abs/10.1080/01972243.2015.1020212?tab=permissions&amp;scroll=top\" class=\"show-permissions\">", "<i class=\"fa fa-print\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">", "Reprints & Permissions</span></a>", "</li>", "<li class=\"pdf-tab \" role=\"tab\">", "<a href=\"/doi/pdf/10.1080/01972243.2015.1020212?needAccess=true\" class=\"show-pdf\" role=\"button\" target=\"_blank\">", "<span class=\"nav-data\">", "PDF", "</span>", "</a>", "</li>", "</ul>", "<div class=\"tab-content \">", "<a id=\"top-content-scroll\"></a>", "<div class=\"tab tab-pane active\">", "<article class=\"article\">", "<p class=\"fulltext\"></p><div class=\"hlFld-Abstract\"><p class=\"fulltext\"></p><div class=\"sectionInfo abstractSectionHeading\"><h2 id=\"abstract\" class=\"section-heading-2\">Abstract</h2></div><div class=\"abstractSection abstractInFull\"><p>In order to investigate how the use of robots may impact everyday tasks, twelve participants in our study interacted with a University of Hertfordshire Sunflower robot over a period of 8 weeks in the university's Robot House. Participants performed two constrained tasks, one physical and one cognitive, four times over this period. Participant responses were recorded using a variety of measures including the System Usability Scale and the NASA Task Load Index. The use of the robot had an impact on the experienced workload of the participants di\ufb00erently for the two tasks, and this e\ufb00ect changed over time. In the physical task, there was evidence of adaptation to the robot's behavior. For the cognitive task, the use of the robot was experienced as more frustrating in the later weeks.</p></div></div><div class=\"abstractKeywords\"><div class=\"hlFld-KeywordText\"><div><p class=\"kwd-title\" aria-label=\"Keywords\">Keywords: </p><a href=\"/keyword/Assistive+Robotics\" class=\"kwd-btn keyword-click\" role=\"button\">assistive robotics</a><a href=\"/keyword/Domestic+Robots\" class=\"kwd-btn keyword-click\" role=\"button\">domestic robots</a><a href=\"/keyword/Human%E2%80%93robot+Interaction\" class=\"kwd-btn keyword-click\" role=\"button\">human\u2013robot interaction</a><a href=\"/keyword/Prototyping\" class=\"kwd-btn keyword-click\" role=\"button\">prototyping</a></div></div></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneAbs\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div></div><div class=\"hlFld-Fulltext\"><div id=\"s0001\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><p>In the field of human\u2013robot interaction, domestic, human-centered environments present serious challenges for prototyping human\u2013machine interactions. In particular, when addressing future and emergent technologies, it is a challenge to enable interactions that are situated in such a way that they are meaningful to the user, and allow users to translate this experience to their everyday life. Moreover, the experience of such interactions is subjective, and the relationship between interactants, technologies, and situations can be complex and dynamic (Buchenau and Suri <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0006\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2000</a></span>). On the technical side, cutting-edge technologies often do not have the stability required to function autonomously in an e\ufb00ective and safe manner for sustained periods of time outside of highly constrained settings. However, such feedback is critical for guiding the development of these technologies. This necessitates a high degree of pragmatism and creativity when developing appropriate methodologies for examining how prospective users interact with these technologies, and how these interactions may benefit or hinder the user (Dautenhahn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0012\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>).</p><p>While there have been studies of actual robots acting autonomously in a domestic environment without continuous oversight by experimenters, either the robots employed have had limited movement capabilities, and served mainly as physically embodied conversational agents (not unlike those described in Bickmore and Cassell <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0003\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>) as in the KSERA project (Payr <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0037\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>), or the robots were market-ready products (Fernaeus et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0017\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>; Sung et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0044\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>) or at a late stage in the development cycle (Kidd and Breazeal <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0025\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>). Furthermore, due to the cost in time and resources to set up and run the experiments, live interactions with robotic technologies in complex usage scenarios usually involve only a relatively small number of participants (Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>; Huijnen et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0022\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>). While it is often desirable to run studies with the largest number of participants possible for greater generalizability, there is also the need for studies that allow for a wide range of interactions to capture data on human\u2013robot interaction in all its richness. This balance lies at the heart of our e\ufb00orts to develop, adapt, and use prototyping methodologies for domestic human\u2013robot interaction (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0050\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>).</p></div><div id=\"s0002\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i2\" class=\"section-heading-2\">PROTOTYPING OF HUMAN\u2013ROBOT INTERACTION</h2><p>Broadly, there are two different approaches to prototyping of human\u2013robot interaction. The first one is a holistic, scenario-based approach (Carroll <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0007\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2000</a></span>), which takes a high-level view of the situations and tries to capture the experience of the interaction through narratives. Here the participants\u2019 interactions with the robot are framed within a narrative that allows them to evaluate the potential impact of the prototype in everyday life situations. These scenarios can be presented to the participants as written stories (Blythe and Wright <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0005\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>), videos (Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>; Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0048\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>), theater performances (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0047\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>; Chatley et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0009\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>; Newell et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0035\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>), or live human\u2013robot interactions (Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0030\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>). The second approach is more reductionist and condenses and abstracts the salient features of the interaction into a controlled experimental setup. This approach has been used successfully for studying human\u2013robot proxemics (Tapus et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0051\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0027\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>; Dautenhahn et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0013\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>), specific robot behavior styles (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0046\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>; Fussell et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0018\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Bartneck et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0002\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>), and di\ufb00erent user groups.</p><p>These two approaches are not mutually exclusive. For instance, Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2011)</a></span> combined a high-level narrative with a highly constrained experimental manipulation in a video study. However, each has clear strengths and weaknesses when compared to the other. The narrative approach provides insights into how robotic technologies may impact on people's lives on a more conceptual level. It does not, however give the participant the clear ability to experience and di\ufb00erentiate between the ways that the particularities of a robot's behavior or characteristics impact specific interactions. Highly controlled, experimental studies, on the other hand, are often lacking in ecological validity, but allow for in-depth understanding of specific aspects of the interaction.</p><p>The study presented here fruitfully brought together both approaches: The controlled experiments were integrated with open-ended scenarios as part of a long-term study (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0045\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>). These studies were conducted in the University of Hertfordshire Robot House.</p></div><div id=\"s0003\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i3\" class=\"section-heading-2\">UNIVERSITY OF HERTFORDSHIRE ROBOT HOUSE</h2><p>The UH Robot House is a residential house, near the University of Hertfordshire campus, that has been adapted for human\u2013robot interaction studies. It has been augmented into a \u201csmart home\u201d with low-cost, resource-e\ufb03cient sensor systems that inform the robots about user activities and other events in the environment (Duque et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0016\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>). Moreover, it offers ecological validity because it is a real working house, with kitchen appliances, a TV, a doorbell, and so on. Throughout the studies presented here, participants primarily used the living room, dining area, and kitchen, sometimes responding to events (visitors, deliveries, etc.) at the front door, with an extra room used in the briefing for the open-ended scenario. In general, the Robot House serves as an e\ufb00ective test bed for prototyping domestic human\u2013robot interactions. Its infrastructure supports interactions with a range of robots such as the UH Sunflower robot (Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0026\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>), PeopleBots (Walters et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0053\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>), and the IPA Care-O-Bot 3 (Parlitz et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0036\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0028\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>).</p></div><div id=\"s0004\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i4\" class=\"section-heading-2\">CONSTRUCTED PERSONAS</h2><p>Personas are understood in human\u2013computer interaction as fictional yet highly realized users of a given technology (Chang et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0008\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>). By creating and extrapolating behaviors, goals, histories, and characteristics of these, it is possible to tightly focus the technological development. The specific personas used to guide the scenario development in the Robot House were a couple in their mid-to-late sixties. The personas were given work, interests, and health issues, which are summarized next.</p><p><i>The Husband (David)</i> is recently retired from a white-collar profession. He is looking forward to spend some time focusing on his hobbies, which include reading, watching documentaries, and building military models. He has a heart condition, which requires him to take medication regularly. For some reason, he often forgets to take this medication and has to be reminded by his wife daily. He also has a condition (likely arthritis in the knees) that gives him some mobility issues.</p><p><i>The Wife (Judy)</i> works from home most days. Her husband's recent retirement and associated distractions are causing her some stress, and the couple some tension. She normally stays in her home o\ufb03ce almost exclusively during her working hours, interacting with David primarily at mealtimes. She is used to computing technology, relying on it to work e\ufb00ectively from her home o\ufb03ce. This has also enabled her and David to maintain close contact (using Skype and other social media) with their children and grandchildren.</p><p>Based on the lives of these personas, we created a \u201ctypical\u201d day comprised of episodes in which the robot was utilized to aid \u201cJudy\u201d and \u201cDavid\u201d in their daily activities. See <a href=\"#f0001 f0002\">Figures\u00a01 and 2</a> for episodes from a scenario based on a \u201ctypical\u201d day for the user personas. The evaluation scenarios were created by examining the possible roles that the robot could play in the di\ufb00erent episodes that comprised a \u201ctypical\u201d day for the two user personas. This was done both as high-level narrative-based interactions, which presented scenarios where the interaction with the robot was within a specific context for the participants, and through constrained and experimental examinations of the role of the robot within specific tasks.</p></div><div id=\"s0005\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i5\" class=\"section-heading-2\">OPEN-ENDED SCENARIOS</h2><p>The open-ended scenarios sought to convey the impact of the agent within a wider context to the participants in an evaluation study. To achieve this, two open-ended scenarios were created. They were inspired by the Persona Scenarios (as shown in <a href=\"#f0001\">Figure\u00a01</a>) but di\ufb00ered in that they were intended for a single user, and would be meaningful to an experimental participant within the context of a 1-hour duration interaction (for long-term studies a duration of 1\u00a0hour maximum for each session was considered appropriate in order to avoid fatiguing the participants). The scenarios were grounded in an imagined daily life, with the robot adopting an assistive role: allowing the participants to inform the robot about their preferences in terms of drinks, snacks, leisure activities, and TV programs that they preferred. These elements were used in individual episodes whereby each scenario was performed twice during the long-term studies, according to the schedule shown in <a class=\"ref showTableEventRef\" data-ID=\"t0001\">Table\u00a01</a>. In these episodes, the participants were asked to engage in a structured role-play-like scenario (Seland <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0041\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>) in order to investigate the role of the robot in a manner that could be directly related to the participants\u2019 everyday experience. Therefore, they could directly experience the impact of the robot. These scenarios also investigated particular issues that were of interest to our research, such as human and robot communication and \u201cagent migration\u201d (see explanation in the following). <div class=\"figure figureViewer\" id=\"f0001\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 1. </span> Episode from a \u201cNormal Day\u201d for the user personas (1).</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0001image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0001_oc.jpg\"}' width=\"383\" height=\"286\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0001\"><p><span class=\"captionLabel\">FIG. 1. </span> Episode from a \u201cNormal Day\u201d for the user personas (1).</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0001\"></div> <div class=\"figure figureViewer\" id=\"f0002\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 2. </span> Episode from a \u201cNormal Day\u201d for the user personas (2).</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0002image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0002_oc.jpg\"}' width=\"439\" height=\"500\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0002\"><p><span class=\"captionLabel\">FIG. 2. </span> Episode from a \u201cNormal Day\u201d for the user personas (2).</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0002\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 1 </span> Overview of sessions</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0001-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0001&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0001\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>These scenarios were based around episodes in two \u201cimaginary\u201d days and were intended to investigate interactions with and responses to the robot in an everyday setting. The first episode took place in the \u201cmorning\u201d and focused on the expressive capabilities of the Sunflower robot. The second episode was set during the \u201cafternoon\u201d and was focused on the participants\u2019 impression of agent migration\u2014the ability of an agent's \u201cmind\u201d to move between di\ufb00erent robot and virtual embodiments (Syrdal et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0046\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2009</a></span>; Du\ufb00y et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0015\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2003</a></span>). Here, the agent's \u201cmind\u201d comprises its memory, its interaction history, and a sense of context; for example, it can remember the user's preferences while moving between di\ufb00erent embodiments, and can continue tasks begun in one embodiment within another. This allows the agent to take advantage of features and functionalities of more than one embodiment while maintaining the persistent features that make it unique and recognizable from a user's perspective. These attributes include awareness of interaction history and context, as well as persistent customizable features. In the scenario, the migration took place between a Sunflower and a SONY Aibo robot. For both of these scenarios, participants were briefed as to the time of day and the particulars of the situation they were going to take part in (Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0029\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2011</a></span>).</p></div><div id=\"s0006\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i8\" class=\"section-heading-2\">CONSTRAINED EXPERIMENTS</h2><div id=\"s0006-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i9\">Cognitive Prosthetic</h3><p>The scenarios identified several instances in which the robot companion would be able to assist the user by providing information. This information could be provided in the form of reminders of appointments, mealtimes, and medicines. In the chosen scenario the robot's task was to remind \u201cDavid\u201d to take his heart medication.</p><p>Adherence to a prescribed regimen of medication can be difficult for many patients. Early approaches (as exemplified by Schwartz et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0040\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1962</a></span>) presented this as being caused by a shortfall in the ability of the patient, who was seen as making mistakes. More recent approaches consider a wider range of reasons for nonadherence to prescribed medicine regimens. In addition to the cognitive abilities of the patient, the new approaches also take into account other factors such as the complexity of the medication schedule, perceived e\ufb03cacy of the treatment, and perceived risk of side e\ufb00ects (Horne et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0021\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>).</p><p>While this particular scenario used the robot purely to remind the user of his schedule in a manner similar to that of cognitive prosthetics on hand-held platforms (Modayil et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0034\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>), this functionality can also be combined with more persuasive technologies that use relational and other strategies in order to encourage habits conducive to the health of the user (Bickmore et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0004\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2005</a></span>). However, this was not the focus of the current study, which focused purely on the cognitive prosthetic aspect of such technologies and its impact within the performance of a task.</p><p>The experimental instantiation of the Cognitive Prosthetic task involved participants putting Scrabble tiles into the correct spaces of a medicine dispenser on the living room table (see <a href=\"#f0005\">Figure\u00a05</a>, shown later), relying on a master list that had to remain on the kitchen bench. There were 28 spaces for the tiles, and both the position of the tiles in the dispenser and their position on the list in the kitchen were randomized. <div class=\"figure figureViewer\" id=\"f0003\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 3. </span> The Sunflower robot used in this study. The robot was built at the University of Hertfordshire, significantly extending a basic Pioneer Platform.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0003image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0003_oc.jpg\"}' width=\"500\" height=\"437\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0003\"><p><span class=\"captionLabel\">FIG. 3. </span> The Sunflower robot used in this study. The robot was built at the University of Hertfordshire, significantly extending a basic Pioneer Platform.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0003\"></div> <div class=\"figure figureViewer\" id=\"f0004\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 4. </span> Interacting with the touch-screen interface on the Sunflower robot.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0004image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0004_oc.jpg\"}' width=\"383\" height=\"256\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0004\"><p><span class=\"captionLabel\">FIG. 4. </span> Interacting with the touch-screen interface on the Sunflower robot.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0004\"></div> <div class=\"figure figureViewer\" id=\"f0005\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 5. </span> Medicine dispenser and Scrabble tiles used in the Cognitive Prosthetic task as part of the controlled experiments.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0005image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0005_oc.jpg\"}' width=\"383\" height=\"353\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0005\"><p><span class=\"captionLabel\">FIG. 5. </span> Medicine dispenser and Scrabble tiles used in the Cognitive Prosthetic task as part of the controlled experiments.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0005\"></div> </p></div><div id=\"s0006-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i13\">Fetch and Carry</h3><p>The Fetch and Carry task involved the carrying of objects between di\ufb00erent rooms. This task was performed during episodes such as mealtimes, where the robot could assist with the movement of prepared food from the kitchen to the dining area and returning of dishes to the kitchen. It was also considered to be of utility in the episodes where \u201cDavid\u201d could use it while engaging in his hobby, for example, to move models and tools from storage to a work surface in a di\ufb00erent room.</p><p>The term Fetch and Carry comes from H\u00fcttenrauch and Eklundh <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0023\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2002)</a></span>, who in their case study describe how a user with partial mobility impairment uses a mobile robot as a platform for transporting objects that this person would otherwise be unable to move without assistance from another person. This particular task is interesting due to both the utility of the task and the human\u2013robot interaction issues that it highlights.</p><p>The Fetch and Carry capability of robots can be of use to a wide variety of users because there are many reasons why they may need assistance for transporting objects, ranging from fall injuries to neurodegenerative conditions like Parkinson's (Kamsma et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0024\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1995</a></span>; Walker and Howland <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0052\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1991</a></span>). It is also an interesting task from a human\u2013robot interaction perspective, as it is unique to the physical nature of robots and involves both human and robot interactants negotiating and moving in a shared physical space. As long as the robot is capable of moving between two or more points and is fitted with a suitable container for the transport of objects, a robust and stable realization of this task is well within the current state of the art. For a product prototype implementation for this task, see the Danish Technological Institute (DTI) robot-butler \u201cJames\u201d (Danish Technological Institute <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0011\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2012</a></span>).</p><p>The experimental instantiation of the Fetch and Carry task involved the participants moving 100 plastic balls from a net on the kitchen bench to the living room table using only one hand. This was a constraint that was easily implemented while being challenging to the participants. While the balls were very light, requiring little physical strength, they were quite unwieldy in numbers larger than four or five, so required several trips back and forth to transport them all.</p><p>Assistance as envisaged with the Cognitive Prosthetic and Fetch and Carry tasks can be used in response to changed circumstances, such as recovery from illness and accidents, as well as rehabilitation after strokes, where the prospective user will have to learn new skills to aid in daily living, or gradually recover mastery of old skills.</p><p>For the experimental instantiation of both these tasks, we decided to choose tasks that, while not strenuous, would present a challenge to the participants, and in which the use of a robot would have a clear impact on the task. In addition, it was hoped that the experimental constraints would add novelty to the task, allowing us to see the impact of changes in participant task mastery.</p></div></div><div id=\"s0007\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i14\" class=\"section-heading-2\">RESEARCH QUESTIONS</h2><div id=\"s0007-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i15\">Di\ufb00erentiation of Tasks on the NASA TLX</h3><p>The first research question was whether or not we could di\ufb00erentiate between the tasks using their NASA Task Load Index (TLX), a measure for di\ufb00erent types of workload that is described in more detail in the methodology section. It was expected that the two tasks would load more strongly on their \u201cprimary\u201d dimensions (namely, Fetch and Carry along the Physical Dimension, and Cognitive Prosthetic along the Mental Dimension). It was also of interest to see whether these tasks changed over time (i.e., whether practice changed the nature of the tasks in terms of experienced workload).</p><p>Research Question 1: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">How did the two tasks differ from each other in terms of experienced workload at the initial presentation?</p></li><li><p class=\"inline\">How did the experience of the tasks change over time?</p></li></ol></p></div><div id=\"s0007-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i16\">Impact of the Robot</h3><p>We were also interested in how the use of the robot would alter the perceived workload of the two tasks, and how this impact changed over time. While we expected the use of the robot to impact the di\ufb00erent tasks along their primary dimensions by reducing participants\u2019 workload in the initial interactions with the robot as an aid, we were also interested in how the robot impacted the workload on these tasks along the other dimensions.</p><p>Research Question 2: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">How did the robot impact the experienced workload on the tasks along the different dimensions of the NASA TLX?</p></li><li><p class=\"inline\">How did the impact of the robot change over time?</p></li></ol></p></div><div id=\"s0007-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i17\">The Experience of the Task and the Robot</h3><p>Our final interest was in how participants reasoned about the tasks, and how they described the tasks in terms of what contributed to their workload and their experience of the robot's assistance.</p><p>Research Question 3: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">How did the participants reason about the tasks? Did they see them as \u201cnatural\u201d and relevant to their own everyday experience?</p></li><li><p class=\"inline\">How did participants describe the role of the robot in the task? What where the benefits of its use, and what were the drawbacks?</p></li></ol></p></div></div><div id=\"s0008\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i18\" class=\"section-heading-2\">METHODOLOGY</h2><div id=\"s0008-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i19\">Apparatus</h3><p>Two robots were used in this study. The first was the UH Sunflower robot, which uses a Pioneer base (commercially available from MobileRobots) but with significant modifications (See <a href=\"#f0003\">Figure\u00a03</a>). The main mode of direct interaction with this robot is its touch-screen (<a href=\"#f0004\">Figure\u00a04</a>), which can be used to both display information to the user and issue commands to the robot. Sunflower also has an extending tray that can be used to carry light objects. The Sunflower robot is similar in shape and interaction capabilities to other robots intended for domestic use (e.g., Coradeschi et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0010\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>; Lammer et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0031\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>; Koay et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0028\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2014</a></span>). The second robot used in the study was a SONY AIBO.<sup>1</sup></p><p>In addition, laptop PCs were set up for Skype calls.</p><p>The apparatus for the Fetch and Carry task consisted of the previously mentioned 100 play balls. The apparatus for the Cognitive Prosthetic task was comprised of the generic medicine tray and scrabble tiles as shown in <a href=\"#f0005\">Figure\u00a05</a>. Both of these are widely available commercially.</p></div><div id=\"s0008-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i20\">Experimental Setup</h3><p>Participants were asked to visit the robot house once a week for a period of 10 weeks, in order to study how participants\u2019 views of, and interactions with, the robots changed over time. See <a class=\"ref showTableEventRef\" data-ID=\"t0001\">Table\u00a01</a> for an overview of the sessions that the participants took part in. References made in this article to a specific week are based on <a class=\"ref showTableEventRef\" data-ID=\"t0001\">Table 1</a>. While the participants would only do the controlled, task-based prototyping experiment in weeks 1, 2, 5, and 8, it is important to note that in these other sessions they interacted with the robot, using its touch-screen interface and moving in the same space as the robot, thus familiarizing themselves with the robot and its use between the constrained task-based experiments. Each session took about 1\u00a0hour, including debriefing.</p></div><div id=\"s0008-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i21\">Procedure</h3><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0008-0003-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i22\">Introduction</h5><p>The introduction session introduced the UH Robot House and the robots to the participants. The participants were instructed in the use of the Sunflower robot and touch-screen, as well as how this robot responded to scheduled and sensor events. The participants were given a tour of the living areas where they would interact with the robot, and were shown the kitchen cupboards and fridge shelves that would be \u201ctheirs.\u201d In addition, they were introduced to the AIBO robot and its use in remote human\u2013human interaction scenarios. Throughout this tour, participants were encouraged to think of these areas as their home and to put themselves in the mind set of someone living in the house. This was intended to begin the process of framing the narrative (Dindler and Iversen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0014\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>) of the open-ended scenarios. It was also intended as a session in which the participants could make themselves as comfortable in the house as possible. The session ended with the baseline experiment.</p></div><div id=\"s0008-0003-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i23\">Open-ended scenarios</h5><p>As mentioned earlier, there were two open-ended scenarios that were presented twice to the participants. At the beginning of each open-ended scenario session, the participants were given a narrative framing of the context of the scenario that they were taking part in. They were told the time of day, and also what had transpired immediately before the beginning of the scenario. Scenario A began in the morning and the participants were told the following: <div class=\"quote\"><p>Imagine that you have now woken up. In the introductory session you gave us some preferences for what you would like to do in the early morning. The robot has stored these preferences and will try to help you do them. When you are ready, you will come out of the bedroom and sit down on the sofa. The robot will then approach you.</p></div></p><p>Scenario B began in the afternoon: <div class=\"quote\"><p>Imagine that it is afternoon and you have just returned home and have just sat down on the sofa. You have planned to watch some TV. In the introductory session, you gave us some preferences as to what TV programs you like to watch and also what sorts of snacks and drinks that you prefer to eat. The robot has stored these preferences. It will also respond to events such as phone calls and doorbells. When you are ready to begin, sit down on the sofa and the robot will approach you.</p></div></p><p>After this briefing, the scenarios ran as outlined previously. Participants were asked to fill in questionnaires after the scenario was completed.</p></div><div id=\"s0008-0003-0003\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i24\">Constrained experiments</h5><p>There were two sets of conditions for the experiment: <ol class=\"NLM_list NLM_list-list_type-order\"><li><p class=\"inline\">Task: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">Fetch and Carry.</p></li><li><p class=\"inline\">Cognitive Prosthetic.</p></li></ol></p></li><li><p class=\"inline\">Robot: <ol class=\"NLM_list NLM_list-list_type-alpha-lower\"><li><p class=\"inline\">Human-Only.</p></li><li><p class=\"inline\">Robot and Human.</p></li></ol></p></li></ol></p><p>In the baseline experiment in Week 1, participants undertook both task conditions in the human-only condition. The presentation order of the two tasks was counterbalanced in order to account for a presentation e\ufb00ect. In weeks 2, 5 and 8, participants did both task conditions for both of the robot conditions for a total of 4 trials in these weeks. The presentation order was within each week, for both Task and Robot conditions. Participants were given a questionnaire to respond to after each run of a task.</p></div></div></div><div id=\"s0008-0004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i25\">Robot Use</h3><p>The use of the robot was adapted to each task: For the Fetch and Carry task, participants were allowed to use the extendible tray of the robot as an additional platform to transport the plastic balls to the living room table. The participants could instruct the robot to move between the locations using the touch-screen interface. For the Cognitive Prosthetic task, the participants could access the list through the touch-screen interface. The participants could only access one quarter of the list at any given time, and could only choose which portion of the list to access while in the kitchen. This meant that in order to access the whole list, they would have to make several journeys between the living room and the kitchen over the course of the trial.</p><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0008-0004-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i26\">Instructions</h5><p>Before each task, participants were shown the apparatus involved in each task, and had the task explained to them. For the robot condition, participants were shown how to use the robot, and how to operate the touch-screen interface relevant for that particular task. Participants were asked to try to complete the task as quickly as possible. They were told that their performance was not being assessed, and that if the task took longer than 10 minutes to complete, the experimenters would stop the experiment.</p></div></div></div><div id=\"s0008-0005\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i27\">Measures: NASA Task Load Index</h3><p>We used the NASA Task Load Index (TLX) as the primary measure for the evaluation of the constrained tasks. The NASA TLX is a questionnaire-based means of measuring workload for specific tasks along several di\ufb00erent dimensions. It is particularly intended for examining human\u2013machine interactions (Hart and Staveland <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0020\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1988</a></span>). As it is a posttask measure, administering it to a participant would not a\ufb00ect task performance in the manner that a concurrent measure such as a think-aloud protocol might (Russo et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0039\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1989</a></span>). Despite it being a subjective, posttask measure, studies have shown it to be a reliable and valid tool for examining task di\ufb03culty and performance (Rubio et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0038\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2004</a></span>). Since its conception, it has been used across a wide variety of domains and tasks (Hart <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0019\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2006</a></span>). It was chosen over the more focused Human\u2013Robot Interaction Workload Measurement (HRI-WM) (Yagoda <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0055\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>) because the main focus of our study was on the participants\u2019 experience of the tasks themselves, rather than an assessment of how they interacted with the robot. The NASA TLX measures workload along six dimensions, shown in <a class=\"ref showTableEventRef\" data-ID=\"t0002\">Table\u00a02</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 2 </span> Dimensions of the NASA Task Load Index</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0002-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0002&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0002\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0008-0006\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i28\">Ad Hoc Questions</h3><p>In addition to the NASA TLX, participants were asked open-ended questions, inviting them to describe their experiences of the tasks themselves, as well as the role of the robot within them. These questions are shown in <a class=\"ref showTableEventRef\" data-ID=\"t0003\">Table\u00a03</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 3 </span> Open-ended questions</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0003-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0003&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0003\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0008-0007\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i29\">Participants</h3><p>Twelve participants took part in the study, recruited through advertisements on the University of Hertfordshire Intranet, mailing lists, and social networks. There were eight males and four females in the sample. The mean age was 32\u00a0years and the median age was 26\u00a0years, and the age range was 18\u201364\u00a0years. The use of human participants had been approved by the University of Hertfordshire Ethics Committee under protocol number 1112/39.</p></div></div><div id=\"s0009\" class=\"NLM_sec NLM_sec-type_results NLM_sec_level_1\"><h2 id=\"_i30\" class=\"section-heading-2\">RESULTS</h2><p>The results for the constrained tasks with respect to the original research questions were as follows.</p><div id=\"s0009-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i31\">Research Question 1: Characteristics of the Task</h3><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0009-0001-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i32\">Baseline values</h5><p>The di\ufb00erences between the two tasks were examined using a series of t-tests (<a class=\"ref showTableEventRef\" data-ID=\"t0004\">Table\u00a04</a> and <a href=\"#f0006\">Figure\u00a06</a>). As could be expected, the TLX significantly di\ufb00erentiates between the two tasks in terms of the physical and mental dimensions. The most salient di\ufb00erences between the two can be seen along the Physical Dimension (which contributes significantly more to the workload of the Fetch and Carry task) and the Mental Dimension (which contributes significantly more to the workload of the Cognitive Prosthetic task). There is a trend approaching significance for the Frustration Dimension, which suggests that it contributes more to the workload of the Fetch and Carry task. <div class=\"figure figureViewer\" id=\"f0006\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 6. </span> TLX baseline scores for tasks.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0006image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0006_b.gif\"}' width=\"500\" height=\"218\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0006\"><p><span class=\"captionLabel\">FIG. 6. </span> TLX baseline scores for tasks.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0006\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 4 </span> TLX baseline scores for tasks</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0004-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0004&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0004\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0009-0001-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i34\">Long-term change</h5><p>Change across the 8 weeks for the Fetch and Carry task is described in <a class=\"ref showTableEventRef\" data-ID=\"t0005\">Table\u00a05</a> and <a href=\"#f0007\">Figure\u00a07</a>. They suggest that the only significant change for this task was along the E\ufb00ort Dimension, which contributed more to the workload in this task in later weeks than the first week. Change across the 8 weeks for the Cognitive Prosthetic task is described in <a class=\"ref showTableEventRef\" data-ID=\"t0006\">Table\u00a06</a> and <a href=\"#f0008\">Figure\u00a08</a>, suggesting that overall there were no significant changes for this task in terms of what dimensions contributed to the workload on this task. However, a trend approaching significance indicates that the Temporal Dimension contributed less to the workload of this task in later weeks. Also, while the descriptive statistics of <a class=\"ref showTableEventRef\" data-ID=\"t0006\">Table\u00a06</a> suggest that there was an equally substantial mean change in the Mental Dimension, the variance between participants\u2019 individual scores stopped this change from being significant for this sample. <div class=\"figure figureViewer\" id=\"f0007\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 7. </span> Long-term change for Fetch and Carry task.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0007image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0007_b.gif\"}' width=\"500\" height=\"230\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0007\"><p><span class=\"captionLabel\">FIG. 7. </span> Long-term change for Fetch and Carry task.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0007\"></div> <div class=\"figure figureViewer\" id=\"f0008\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 8. </span> Long-term change for Fetch and Carry task.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0008image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0008_b.gif\"}' width=\"500\" height=\"230\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0008\"><p><span class=\"captionLabel\">FIG. 8. </span> Long-term change for Fetch and Carry task.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0008\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 5 </span> Long-term change for Fetch and Carry task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0005-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0005&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0005\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 6 </span> Long-term change for Cognitive Prosthetic task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0006-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0006&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0006\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div></div></div><div id=\"s0009-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i37\">Research Question 2: Robot Impact</h3><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0009-0002-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i38\">Fetch and Carry</h5><p>The overall impact of the robot can be found in <a class=\"ref showTableEventRef\" data-ID=\"t0007\">Table\u00a07</a> and <a href=\"#f0009\">Figure\u00a09</a>. There were significant main e\ufb00ects for the role of the Robot along the Physical, Temporal, Performance, and E\ufb00ort dimensions. However, all of these main e\ufb00ects, with the exception of Performance, were mediated by interaction e\ufb00ects between the role of the robot and the long-term e\ufb00ects, so we consider these interaction e\ufb00ects in the text as well. For Performance, there was a main e\ufb00ect for robot assistance. This e\ufb00ect suggests that performance was experienced as worse with the robot than if the participant acted on his or her own. This e\ufb00ect was very pronounced in week 2 but decreased with time. For the Physical dimension, there was a significant interaction e\ufb00ect between time and assistance. The relationship suggested by the descriptive statistics in <a class=\"ref showTableEventRef\" data-ID=\"t0007\">Table\u00a07</a> and <a href=\"#f0009\">Figure\u00a09b</a> is that the participants found that the robot reduced the workload overall but this e\ufb00ect decreased after week 2. For the Temporal Dimension, there was a significant main e\ufb00ect described in <a href=\"#f0009\">Figure\u00a09c</a>, where participants found that the robot overall increased the Temporal aspects of workload. The interaction e\ufb00ect approaching significance, however, suggests that this e\ufb00ect decreased over time. The robot's impact on the E\ufb00ort Dimension was quite small in weeks 2 and 5. However, by week 8, the assistance of the robot reduced the workload along this dimension (see <a href=\"#f0009\">Figure\u00a09e</a>). <div class=\"figure figureViewer\" id=\"f0009\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 9. </span> Robot impact on Fetch and Carry in terms of experienced workload.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0009image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0009_b.gif\"}' width=\"500\" height=\"380\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0009\"><p><span class=\"captionLabel\">FIG. 9. </span> Robot impact on Fetch and Carry in terms of experienced workload.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0009\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 7 </span> Robot impact on Fetch and Carry</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0007-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0007&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0007\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p></div><div id=\"s0009-0002-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i40\">Cognitive Prosthetic</h5><p>The overall impact for the robot on the Cognitive Prosthetic Task is shown in <a class=\"ref showTableEventRef\" data-ID=\"t0008\">Table\u00a08</a> and <a href=\"#f0010\">Figure\u00a010</a>. The impact of robot assistance was primarily along the Mental, Performance, and E\ufb00ort dimensions. There were no interaction e\ufb00ects. <div class=\"figure figureViewer\" id=\"f0010\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">FIG. 10. </span> Robot impact on Cognitive Prosthetic in terms of experienced workload.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"f0010image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604/images/medium/utis_a_1020212_f0010_b.gif\"}' width=\"500\" height=\"382\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-f0010\"><p><span class=\"captionLabel\">FIG. 10. </span> Robot impact on Cognitive Prosthetic in terms of experienced workload.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-f0010\"></div> <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 8 </span> Robot impact on Cognitive Prosthetic</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0008-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0008&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0008\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>Participants viewed the robot as reducing workload along the Mental Dimension. This was consistent across the 3 weeks. On the other hand, the descriptive statistics in <a class=\"ref showTableEventRef\" data-ID=\"t0008\">Table\u00a08</a> suggest that participants saw the robot as adding significantly to the workload along the Performance Dimension (i.e., making it harder to succeed on the task). This e\ufb00ect is less pronounced in the last week. The other significant impact was along the E\ufb00ort Dimension. The descriptive statistics in <a class=\"ref showTableEventRef\" data-ID=\"t0008\">Table\u00a08</a> suggest that participants found they needed to exert less e\ufb00ort when aided by the robot. There were also two nonsignificant trends for the Temporal and Frustration dimensions. These trends suggested that the participants saw the use of the robot as contributing to more workload in these two dimensions, thus making the task both more frustrating and time-critical.</p></div></div></div><div id=\"s0009-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i42\">Research Question 3: The Experience of the Task and the Robot's Role</h3><p>The analysis of participant responses to qualitative questions (see <a class=\"ref showTableEventRef\" data-ID=\"t0003\">Table\u00a03</a>) was conducted in two main stages. In the first stage, one of the researchers examined the open-ended qualitative responses from the questionnaires and categorizsed them into primary themes and subthemes for each task and each week. These themes were then examined across weeks for each of the tasks. This led to the collection of themes identified in the first two columns in <a class=\"ref showTableEventRef\" data-ID=\"t0009 t0010\">Tables\u00a09 and 10</a>. A unified category scheme for both tasks could not be developed, largely due to the large qualitative di\ufb00erences between the tasks. After this categorization, two of the researchers went through the responses and categorized them as major (+), minor (\u2212), and nonexistent (0).</p><p>The themes that were the most prevalent in the responses were categorized as major. Minor themes were those less prevalent but still reported by a small group of participants. Themes that did not appear in the responses for a particular week were categorized as nonexistent. The final categorization and assignment of the themes was done by the researchers, after having compared their coding of responses, discussed discrepancies, and reached a consensus.</p><div class=\"NLM_sec NLM_sec_level_3\"><div id=\"s0009-0003-0001\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i43\">Fetch and Carry</h5><p>The themes emerging from the participants\u2019 responses are described in detail next and summarized in <a class=\"ref showTableEventRef\" data-ID=\"t0009\">Table\u00a09</a>: <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 9 </span> Themes for the Fetch and Carry task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0009-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0009&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0009\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>Week 1. For the Fetch and Carry task, the two primary themes emerging from Q1 (What made the task di\ufb03cult?) were the physical di\ufb03culty of handling the balls and the constraint of using only one hand when performing the task. They were also evident in the responses to Q2 (What would have made the task easier?) where the possibility of release from this constraint was the predominant theme.</p><p>Week 2. Week 2 saw the introduction of the robot, and Q1 and Q2 were asked for both the human-only and the robot\u2013human condition. For the human-only condition, the theme of the constraint continued among some of the participants. Participants would also contrast the human-only condition with the use of the robot when answering questions related to both conditions. When contrasting the conditions, participants highlighted the practical benefit of being able to perform the tasks in fewer trips. However, the second most prevalent theme in the participants\u2019 statements was the slow speed of the robot. The sample as a whole agreed that the speed of the robot was problematic from a task perspective, with participants having to change their speed of performing the task to accommodate the robot. This was achieved either by walking (more slowly) with the robot to the living room and back, or by waiting at the appropriate place to load or unload to the robot. In response to Q2 for the robot condition, the participants overwhelmingly suggested increasing the speed of the robot and/or the size of the tray. They also suggested that an ability of the robot to manipulate objects by loading itself would be helpful. In addition to purely task-related comparisons, a small group of participants highlighted interactional aspects of doing the task with the robot: that the robot provided company or that the task was more enjoyable when using the robot.</p><p>Week 5. Week 5 saw a continuation of the same themes as in week 2. New themes also emerged related to how participants rated their own performance. Some participants identified changes in their own behavior between conditions. They referred to a type of <i>social loafing</i> (Latane et\u00a0al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0032\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">1979</a></span>) that occurred when they did the task with the robot, and that they put more e\ufb00ort in when they were doing the task by themselves. Other participants highlighted mutual adaptation. They reported that they were getting better at coordinating their own and the robot's roles in the task, reducing waiting, and making the use of the robot more e\ufb03cient. The most common strategy was to perform the task in an asynchronous manner, only loading and unloading the robot at convenient times instead of synchronizing each trip. However, for the sample as a whole, the theme of having to wait for the robot was still prevalent. In addition, this week saw statements regarding the touch-screen interface for this task. There were no statements regarding object manipulation capabilities in this week. In addition, participants continued to reference the social aspects of doing the task with the robot.</p><p>Week 8. Week 8 was very similar in terms of themes to Week 5. The main di\ufb00erence was one of prevalence. The theme of mutual adaptation continued and was more widespread, while the theme of having to wait for the robot was much less prevalent this week.</p></div><div id=\"s0009-0003-0002\" class=\"NLM_sec NLM_sec_level_4\"><h5 class=\"section-heading-5\" id=\"_i44\">Cognitive Prosthetic</h5><p>The themes arising from the participants\u2019 responses for this dimension are described below and summarized in <a class=\"ref showTableEventRef\" data-ID=\"t0010\">Table\u00a010</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Integrating Constrained Experiments in Long-Term Human\u2013Robot Interaction Using Task- and Scenario-Based Prototyping</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Syrdal%2C+Dag+Sverre\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Syrdal%2C+Dag+Sverre\"><span class=\"NLM_given-names\">Dag Sverre</span> Syrdal</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Dautenhahn%2C+Kerstin\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Dautenhahn%2C+Kerstin\"><span class=\"NLM_given-names\">Kerstin</span> Dautenhahn</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Koay%2C+Kheng+Lee\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Koay%2C+Kheng+Lee\"><span class=\"NLM_given-names\">Kheng Lee</span> Koay</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Ho%2C+Wan+Ching\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Ho%2C+Wan+Ching\"><span class=\"NLM_given-names\">Wan Ching</span> Ho</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/01972243.2015.1020212\">https://doi.org/10.1080/01972243.2015.1020212</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>13 May 2015</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><p><span class=\"captionLabel\">TABLE 10 </span> Themes for the Cognitive Prosthetic task</p></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"t0010-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=t0010&amp;doi=10.1080%2F01972243.2015.1020212&amp;downloadType=CSV\">CSV</a><a data-id=\"t0010\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div> </p><p>Week 1. In week 1, two main themes arose in participant responses to Q1. The first was the di\ufb03culty of having to remember the position of the tiles while walking from the kitchen to the living room. The second was the attempt at developing a strategy for solving the task without having to rely on memory alone. Responses to Q2 did, as for the Fetch and Carry task, focus on the constraints of the task\u2014in particular, the placement of the list of tile positions in a separate location from the medicine dispenser, and the list of tiles not being in any discernible order. A small group of participants managed to develop a strategy for doing this task more e\ufb03ciently, which consisted of arranging tiles spatially in one's palm in the same manner that they were to be arranged in the medicine dispenser and then transporting them over and inserting them into the dispenser in the same order. The final theme was an expressed desire for tools to aid in the task. There were two categories of tools: reminder tools, such as a pencil and paper to jot down the appropriate tiles and their positions, and tools to make the strategy described earlier more e\ufb03cient. An example of the latter would be a large tray to arrange and carry all the tiles on at once.</p><p>Week 2. In the human-only condition, the adoption of the strategy just described became more prevalent as fewer participants relied on memory alone to perform the task. This change was also reflected in the suggestions for tools to be used, where items that would aid in the use of this strategy were suggested to a larger extent than in the previous week. When discussing the role of the robot, participants raised several issues. They considered the robot-assisted solution of the task to be easier, as there was no need to either remember anything or adopt a strategy. Participants in particular referred to the infallibility of the robot's memory and how this made them feel less under pressure to perform the task correctly.</p><p>However, participants referenced the interaction with the robot in itself as a source of di\ufb03culty for the task as well. The robot was also described as slow and lacking in flexibility. The relinquishing of control to the robot was also referenced when discussing the procedure used to access the information on the robot.</p><p>Week 5. The results in week 5 followed many of the same themes as week 2. There was a continued increase in the use of the strategy outlined in week 1. By this week the majority of participants used this strategy for the human-only condition. References to interface issues were more prevalent in this week's responses, as were references to the physical aspect of the task, such as manipulating and putting the tiles in the dispenser. This week also saw a new theme of subversion emerging. Two of the participants described how they used the robot the way they wanted to, instead of how they felt they were being expected to. They arranged the tiles spatially on the tray of the robot in the kitchen and then used it to transport them in the correct arrangement to the dispenser in the living room, thus sidestepping the use of the robot as a Cognitive Prosthetic.</p><p>Week 8. Week 8 results were similar to those in week 5. Statements related to the physical carrying out of the task were more prevalent this week than on any other week. The majority of participants stated that the task had become easier for them to do. However, many still referenced the benefits of the robot, in particular its infallibility.</p></div></div></div></div><div id=\"s0010\" class=\"NLM_sec NLM_sec-type_discussion NLM_sec_level_1\"><h2 id=\"_i45\" class=\"section-heading-2\">DISCUSSION</h2><div id=\"s0010-0001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i46\">Research Question 1\u2014Di\ufb00erences Between the Tasks</h3><p>We were able to di\ufb00erentiate between the tasks in terms of their NASA TLX profile. Initially, the two tasks were significantly di\ufb00erent from each other only along their primary dimension, with a trend for the Fetch and Carry task loading more on the Frustration Dimension. In terms of long-term change, however, the picture was slightly di\ufb00erent. While neither of the two tasks changed on the Frustration Dimension, they did change along other dimensions. The Fetch and Carry task changed in terms of E\ufb00ort, and loaded higher on this dimension in the later weeks. The Cognitive Prosthetic task changed along the Temporal Dimension, and time pressure was considered less important in weeks 5 and 8. This suggests that the use of the NASA TLX for HRI tasks in domestic environments was a valid and meaningful approach.</p></div><div id=\"s0010-0002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i47\">Research Question 2\u2014Impact of the Robot</h3><p>The robot changed the participants\u2019 experience of the two tasks di\ufb00erently, both in its initial use as well as over time. For the Fetch and Carry task, the robot initially impacted the participants\u2019 ratings of the physical and temporal dimensions. In week 1, while the robot-assisted task was considered less physically strenuous, the participants found the time taken to be burdensome. The trend for the physical dimension continued in the subsequent weeks. However, the impact of the robot on the temporal dimension diminished, suggesting that participants found it easier to use the robot to complete the tasks in weeks 5 and 8. Furthermore, participants found that the use of robot required less e\ufb00ort in the last week, suggesting that there was a learning e\ufb00ect, and that participants were able to use the robot more e\ufb03ciently as time progressed. This was also seen in the manner that the participants reported they used the robot and as well as in their observed usage. In week 2, participants would load themselves and the robot and then follow the robot to the living to unload it. They would then return to the kitchen with the robot. In subsequent weeks, participants would be more likely to not wait for the robot, but rather move around the robot and only load/unload it if they happened to be in the same space as it. This approach employed the robot more efficiently as a supplement to their own capabilities. For the cognitive prosthetic task, the impact of the robot was less clear-cut. Participants rated doing the task with the robot as requiring less mental workload, and this e\ufb00ect persisted throughout the trials. In addition, participants felt that doing the task with the robot required less e\ufb00ort. Despite this, participants rated the use of the robot as requiring more workload in order to perform the task successfully. There was a trend suggesting that for weeks 2 and 5 the use of the robot was seen as more time-consuming; it was also seen as more frustrating across all the trials.</p><p>This suggests that despite the experienced benefit of using the robot in this task, there were still associated problems that made it more time-consuming and frustrating.</p></div><div id=\"s0010-0003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i48\">Research Question 3\u2014The Experience of the Task</h3><p>The descriptive analysis of the open-ended questions allowed for a deeper and more thorough perspective about the tasks and how they were experienced by the users.</p><p>When discussing the initial tasks, participants referenced the constraints imposed on them. Many of their suggestions for making the task easier involved the removal of these constraints. In the cognitive prosthetic task, the participants also considered the means through which they could access the information on the robot as one of the constraints.</p><p>In addition, the results from the TLX along this task were mirrored in the way that participants reasoned about the task. Participants described the robot as slow and inflexible and expressed a need to change the way that the robot was used in the task, either by changing how information was presented or by changing the usage of the robot. This was a reflection not just on their experience of the robot, but also how their increased mastery of the task made them consider the role of the robot di\ufb00erently. This even led to two of the participants using the robot in a manner unintended by the experimenters. They asserted control by subverting its use and using the Fetch and Carry functionalities to aid in the Cognitive Prosthetic task. For the remainder of the sample, however, there seemed to be a tacit understanding of a trade-o\ufb00 between the lack of human error in this task and the lack of control. In the Fetch and Carry task, however, despite similar descriptors of the robot being used in terms of it slowing down the task, participants adapted their use of the robot. This allowed the participants to work around these shortcomings and receive beneficial assistance from the robot. The changes that the participants mainly wanted to implement in terms of how they interacted with the robot were mainly quantitative changes: giving it more space to carry things and letting it move more quickly, in contrast to the changes in the quality of assistance that were suggested in the Cognitive Prosthetic task. It also emerged that, unlike in the Cognitive Prosthetic task, users referenced the robot as a partner and companion in the Fetch and Carry task. This may reflect the open-ended nature of this interaction, and the opportunity for a natural synchronization of behavior to occur gradually. Stienstra and Marti <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0043\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2012)</a></span> suggest this is a key factor in developing feelings of sociality and empathy in an HRI situation.</p></div><div id=\"s0010-0004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i49\">Ecological Validity</h3><p>The narrative framing of the interactions within the Robot House environment enabled participants to evaluate their interactions in a more relevant and applicable manner than what would have been possible in a traditional laboratory study. Despite the fact that the constrained tasks were part of an experimental study where the participants\u2019 interaction with the robot was tightly controlled, there are several factors that support the ecological validity of this study. These tasks were based on the needs of the user personas, and expected interactions arising from these needs. The parallels between observed behaviors and similar interactions with technologies in everyday settings were also encouraging. In the Fetch and Carry task, the process the participants went through when completing the tasks with the robot was quite similar to that of the user in the H\u00fcttenrauch and Eklundh <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0023\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">(2002)</a></span> study. In both cases, the users started o\ufb00 by coordinating their behavior closely with the robot, for example, walking with the robot, and synchronizing their own behavior with that of the robot. They then progressed to using the robot in a more asynchronous manner, with less constant control of the robot. These similarities in interactional outcomes support the notion that many of the qualities of a real-world usage of a final stage prototype were successfully translated into the experimental setup. For the Cognitive Prosthetic task, the manner in which the participants described the role of the robot in the task had elements that map well onto how people perceive such technologies in real-world settings. The issues of autonomy and control come up in both theoretical and practical discussions of the use of robotic technologies (Anderson and Anderson <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0001\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2008</a></span>; Sharkey and Sharkey <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0042\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2012</a></span>). In particular, the resolution of control issues by subverting assistive technologies has also been reported in real-world settings (Loe <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0033\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2010</a></span>), and an analogous process took place within the experiments. This suggests that for the cognitive prosthetic tasks, many of the salient aspects of using such technologies could be e\ufb00ectively conveyed through this constrained method.</p></div><div id=\"s0010-0005\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i50\">Implications</h3><p>The findings highlight the need for a user-centered approach to assistive technologies intended for domestic use. The results from the constrained task experiments strongly stress the need for such assistance to allow for personalization and for the robot assistance to be gradually scaled in order to account for changes in task mastery in the user and for coping strategies that the user may adopt. The TLX scores for the Cognitive Prosthetic tasks suggest that total experienced workload may increase where such scaling and alteration of assistance do not occur, due to frustration and disruption to learned coping strategies, despite the robot's assistance being still considered useful. In addition, the open-ended responses to this task suggested that participants came to regard the robot's assistance as hindering their preferred solution to the task. The scores for the Fetch and Carry task, on the other hand, represent a scenario where the roles of both the robot and the participants were less strongly defined. This left a lot of room for mutual adaptation, which in turn led to a more successful interaction in terms of the TLX scores, and also in terms of the participants\u2019 reasoning about the task and the role of the robot. This suggests that even in constrained tasks, such as the ones presented here, there is a hedonic dimension to interactions that has a role equal to their purely task- and workload-related aspects. This hedonic quality may be impacted by anthropomorphic interaction capabilities, and an interesting future strand of research into task-related domestic human\u2013robot interaction would be to investigate the role of such capabilities in how users respond to performing tasks with robots.</p></div></div><div id=\"s0011\" class=\"NLM_sec NLM_sec-type_conclusions NLM_sec_level_1\"><h2 id=\"_i51\" class=\"section-heading-2\">CONCLUSIONS</h2><p>The work presented in this article has shown the validity of interaction prototyping, in terms of both a high-level narrative approach in which the participants is involved in the playing of the role of a user of a more \u201cmature\u201d version of the technology being prototyped, and that of separating the task-aspect of such interaction. This two-pronged approach to interactions with future and emerging technologies for the purposes of early prototyping is a valid tool for gaining insight into how such interactions may be experienced by the intended users. The findings in this study have allowed us to replicate findings of real-world studies in terms of how participants reason about their potential adoption of such technologies, as well as to quantify the impact of assistance in such tasks using the NASA TLX and to highlight issues relevant for the UH Robot House Scenario and human\u2013robot interaction in general.</p><p>The work described in this article showed how to successfully integrate constrained tasks (as part of controlled experiments) with more \u201cnatural,\u201d open-ended scenarios as part of a long-term study into home companion robots operating in domestic environments. We pointed out experimental and methodological challenges and how they have been addressed in this study. The constrained tasks were based on commercially available tools and as such could potentially be used and replicated by other researchers. Being able to share, replicate, and build upon each others\u2019 results remains one of the big challenges in human\u2013robot interaction, which otherwise remains in danger of staying a widely fragmented field with di\ufb00erent research groups using di\ufb00erent robotic platforms, scenarios, and methodological approaches (Dautenhahn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0012\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2007</a></span>). We therefore hope that, in addition to presenting concrete results from a long-term human\u2013robot interaction study, this article has also raised awareness of the main challenges as well as opportunities in the design of interaction technology that supports long-term human\u2013robot interaction.</p></div></div><script type=\"text/javascript\">", "                        window.figureViewer={doi:'10.1080/01972243.2015.1020212',path:'/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604',figures:[{i:'f0001',g:[{m:'utis_a_1020212_f0001_oc.jpg',l:'utis_a_1020212_f0001_oc.jpeg',size:'26 KB'}]}", "                            ,{i:'f0002',g:[{m:'utis_a_1020212_f0002_oc.jpg',l:'utis_a_1020212_f0002_oc.jpeg',size:'69 KB'}]}", "                            ,{i:'f0003',g:[{m:'utis_a_1020212_f0003_oc.jpg',l:'utis_a_1020212_f0003_oc.jpeg',size:'169 KB'}]}", "                            ,{i:'f0004',g:[{m:'utis_a_1020212_f0004_oc.jpg',l:'utis_a_1020212_f0004_oc.jpeg',size:'38 KB'}]}", "                            ,{i:'f0005',g:[{m:'utis_a_1020212_f0005_oc.jpg',l:'utis_a_1020212_f0005_oc.jpeg',size:'67 KB'}]}", "                            ,{i:'f0006',g:[{m:'utis_a_1020212_f0006_b.gif',l:'utis_a_1020212_f0006_b.jpeg',size:'24 KB'}]}", "                            ,{i:'f0007',g:[{m:'utis_a_1020212_f0007_b.gif',l:'utis_a_1020212_f0007_b.jpeg',size:'28 KB'}]}", "                            ,{i:'f0008',g:[{m:'utis_a_1020212_f0008_b.gif',l:'utis_a_1020212_f0008_b.jpeg',size:'26 KB'}]}", "                            ,{i:'f0009',g:[{m:'utis_a_1020212_f0009_b.gif',l:'utis_a_1020212_f0009_b.jpeg',size:'73 KB'}]}", "                            ,{i:'f0010',g:[{m:'utis_a_1020212_f0010_b.gif',l:'utis_a_1020212_f0010_b.jpeg',size:'68 KB'}]}", "                            ]}</script><script type=\"text/javascript\">window.tableViewer={doi:'10.1080/01972243.2015.1020212',path:'/na101/home/literatum/publisher/tandf/journals/content/utis20/2015/utis20.v031.i03/01972243.2015.1020212/20150604',tables:[{i:'t0001'},{i:'t0002'},{i:'t0003'},{i:'t0004'},{i:'t0005'},{i:'t0006'},{i:'t0007'},{i:'t0008'},{i:'t0009'},{i:'t0010'}]}</script><script type=\"text/javascript\">window.tableIDIndexMap = {\"id\":-1};window.tableIDIndexMap['t0001'] = 1; window.tableIDIndexMap['t0002'] = 2; window.tableIDIndexMap['t0003'] = 3; window.tableIDIndexMap['t0004'] = 4; window.tableIDIndexMap['t0005'] = 5; window.tableIDIndexMap['t0006'] = 6; window.tableIDIndexMap['t0007'] = 7; window.tableIDIndexMap['t0008'] = 8; window.tableIDIndexMap['t0009'] = 9; window.tableIDIndexMap['t0010'] = 10; </script><div id=\"table-content-t0001\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 1 </span> Overview of sessions</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Week</th><th align=\"center\" class=\"rowsep1 align_center last\">Session content</th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 1</td><td align=\"left\" class=\" align_left last\">Introduction to the Robot House, familiarization with the robots and their</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u2003interface. Baseline experiment.</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 2</td><td align=\"left\" class=\" align_left last\">Review of Robot House, robots, and interface. Repeat of experiment.</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 3</td><td align=\"left\" class=\" align_left last\">Open-ended scenario A</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 4</td><td align=\"left\" class=\" align_left last\">Open-ended scenario B</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 5</td><td align=\"left\" class=\" align_left last\">Repeat of experiment</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 6</td><td align=\"left\" class=\" align_left last\">Open-ended scenario A</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 7</td><td align=\"left\" class=\" align_left last\">Open-ended scenario B</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Week 8</td><td align=\"left\" class=\" align_left last\">Repeat of experiment</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Week 10</td><td align=\"left\" class=\" align_left last\">Debriefing</td></tr></tbody></table><div class=\"NLM_table-wrap-foot\" id=\"IDtable-wrap-foot\"><div class=\"fn-group\"></div></div></div><div id=\"table-content-t0002\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 2 </span> Dimensions of the NASA Task Load Index</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"center\" class=\"rowsep1 align_center last\">Workload in terms of \u2026</th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left last\">\u2026 reasoning remembering, planning, thinking</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left last\">\u2026 strength and endurance, dexterity</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left last\">\u2026 pace, time pressure, speed</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left last\">\u2026 success and satisfaction</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left last\">\u2026 e\ufb00ort needed to accomplish performance</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left last\">\u2026 annoyance, frustration, stress</td></tr></tbody></table></div><div id=\"table-content-t0003\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 3 </span> Open-ended questions</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Q1.</td><td align=\"left\" class=\" align_left last\">What was the most di\ufb03cult part of doing the task?</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Q2.</td><td align=\"left\" class=\" align_left last\">What would have made the task easier?</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Q3.</td><td align=\"left\" class=\" align_left last\">What were the benefits of doing the task with the robot?</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Q4.</td><td align=\"left\" class=\" align_left last\">What were the drawbacks of doing the task with the robot?</td></tr></tbody></table></div><div id=\"table-content-t0004\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 4 </span> TLX baseline scores for tasks</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">Fetch and Carrymean (SE)</th><th align=\"left\" class=\"rowsep1 align_left\">Cognitive Prostheticmean (SE)</th><th align=\"left\" class=\"rowsep1 align_left\">Mean difference</th><th align=\"left\" class=\"rowsep1 align_left\">95% CI</th><th align=\"center\" class=\"rowsep1 align_center\"><i>t</i>(df)</th><th align=\"center\" class=\"rowsep1 align_center last\"><i>p</i></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">0.50 (.14)</td><td align=\"left\" class=\" align_left\">2.75 (.58)</td><td align=\"char\" char=\".\" class=\" align_char\">\u22122.25</td><td align=\"left\" class=\" align_left\">\u22123.44 to 1.07</td><td align=\"char\" char=\".\" class=\" align_char\">4.20 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">**.01</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">2.46 (.62)</td><td align=\"left\" class=\" align_left\">0.70 (.25)</td><td align=\"char\" char=\".\" class=\" align_char\">1.75</td><td align=\"left\" class=\" align_left\">0.32 to 3.19</td><td align=\"char\" char=\".\" class=\" align_char\">2.69 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">**.02</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">1.89 (.51)</td><td align=\"left\" class=\" align_left\">1.95 (.46)</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.07</td><td align=\"left\" class=\" align_left\">\u22120.99 to 0.86</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.16 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.88</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">0.83 (.33)</td><td align=\"left\" class=\" align_left\">0.63 (.26)</td><td align=\"char\" char=\".\" class=\" align_char\">0.19</td><td align=\"left\" class=\" align_left\">\u22120.72 to 0.86</td><td align=\"char\" char=\".\" class=\" align_char\">0.46 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.65</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left\">1.15 (.30)</td><td align=\"left\" class=\" align_left\">1.40 (.30)</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.25</td><td align=\"left\" class=\" align_left\">\u22120.72 to 1.10</td><td align=\"char\" char=\".\" class=\" align_char\">\u22120.62 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.55</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">1.51 (.47)</td><td align=\"left\" class=\" align_left\">0.58 (.18)</td><td align=\"char\" char=\".\" class=\" align_char\">0.94</td><td align=\"left\" class=\" align_left\">\u22120.20 to 2.07</td><td align=\"char\" char=\".\" class=\" align_char\">1.82 (11)</td><td align=\"char\" char=\".\" class=\" align_char last\">.10</td></tr></tbody></table></div><div id=\"table-content-t0005\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 5 </span> Long-term change for Fetch and Carry task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">Week 1</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left\"><i>F</i>(3, 8)</th><th align=\"left\" class=\"rowsep1 align_left\"><i>p</i></th><th align=\"left\" class=\"rowsep1 align_left last\">\u03b7<sup>2</sup></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">0.50 (.14)</td><td align=\"left\" class=\" align_left\">0.19 (.06)</td><td align=\"left\" class=\" align_left\">0.20 (.06</td><td align=\"left\" class=\" align_left\">0.17 (.14)</td><td align=\"left\" class=\" align_left\">0.99</td><td align=\"left\" class=\" align_left\">.45</td><td align=\"left\" class=\" align_left last\">.27</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">2.46 (.62)</td><td align=\"left\" class=\" align_left\">3.01 (.46)</td><td align=\"left\" class=\" align_left\">3.09 (.58)</td><td align=\"left\" class=\" align_left\">2.70 (.56)</td><td align=\"left\" class=\" align_left\">2.44</td><td align=\"left\" class=\" align_left\">.14</td><td align=\"left\" class=\" align_left last\">.48</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">1.89 (.51)</td><td align=\"left\" class=\" align_left\">1.30 (.29)</td><td align=\"left\" class=\" align_left\">2.25 (.44)</td><td align=\"left\" class=\" align_left\">1.95 (.50)</td><td align=\"left\" class=\" align_left\">1.20</td><td align=\"left\" class=\" align_left\">.37</td><td align=\"left\" class=\" align_left last\">.31</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">0.83 (.33)</td><td align=\"left\" class=\" align_left\">0.58 (.19)</td><td align=\"left\" class=\" align_left\">0.83 (.34)</td><td align=\"left\" class=\" align_left\">0.34 (.05)</td><td align=\"left\" class=\" align_left\">2.15</td><td align=\"left\" class=\" align_left\">.17</td><td align=\"left\" class=\" align_left last\">.45</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left\">1.15 (.30)</td><td align=\"left\" class=\" align_left\">2.50 (.37)</td><td align=\"left\" class=\" align_left\">1.84 (.42)</td><td align=\"left\" class=\" align_left\">2.45 (.40)</td><td align=\"left\" class=\" align_left\">5.48</td><td align=\"left\" class=\" align_left\">*.02</td><td align=\"left\" class=\" align_left last\">.67</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">1.51 (.47)</td><td align=\"left\" class=\" align_left\">0.95 (.25)</td><td align=\"left\" class=\" align_left\">0.88 (.20)</td><td align=\"left\" class=\" align_left\">1.02 (.34)</td><td align=\"left\" class=\" align_left\">1.52</td><td align=\"left\" class=\" align_left\">.28</td><td align=\"left\" class=\" align_left last\">.36</td></tr></tbody></table></div><div id=\"table-content-t0006\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 6 </span> Long-term change for Cognitive Prosthetic task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"center\" class=\"rowsep1 align_center\">Week 1</th><th align=\"center\" class=\"rowsep1 align_center\">Week 2</th><th align=\"center\" class=\"rowsep1 align_center\">Week 5</th><th align=\"center\" class=\"rowsep1 align_center\">Week 8</th><th align=\"center\" class=\"rowsep1 align_center\"><i>F</i>(3, 8)</th><th align=\"center\" class=\"rowsep1 align_center\"><i>p</i></th><th align=\"center\" class=\"rowsep1 align_center last\">\u03b7<sup>2</sup></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">2.75 (.58)</td><td align=\"left\" class=\" align_left\">1.97 (.48)</td><td align=\"left\" class=\" align_left\">2.21 (.51)</td><td align=\"left\" class=\" align_left\">1.81 (.38)</td><td align=\"left\" class=\" align_left\">1.29</td><td align=\"left\" class=\" align_left\">.34</td><td align=\"left\" class=\" align_left last\">.33</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">0.70 (.25)</td><td align=\"left\" class=\" align_left\">0.72 (.30)</td><td align=\"left\" class=\" align_left\">0.63 (.26)</td><td align=\"left\" class=\" align_left\">0.60 (.18)</td><td align=\"left\" class=\" align_left\">0.23</td><td align=\"left\" class=\" align_left\">.87</td><td align=\"left\" class=\" align_left last\">.08</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">1.95 (.46)</td><td align=\"left\" class=\" align_left\">1.78 (.44)</td><td align=\"left\" class=\" align_left\">1.04 (.30)</td><td align=\"left\" class=\" align_left\">1.22 (.43)</td><td align=\"left\" class=\" align_left\">3.87</td><td align=\"left\" class=\" align_left\">*.06</td><td align=\"left\" class=\" align_left last\">.59</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">0.63 (.26)</td><td align=\"left\" class=\" align_left\">0.93 (.28)</td><td align=\"left\" class=\" align_left\">0.76 (.22)</td><td align=\"left\" class=\" align_left\">0.68 (.32)</td><td align=\"left\" class=\" align_left\">1.63</td><td align=\"left\" class=\" align_left\">.26</td><td align=\"left\" class=\" align_left last\">.38</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">E\ufb00ort</td><td align=\"left\" class=\" align_left\">1.40 (.30)</td><td align=\"left\" class=\" align_left\">1.32 (.31)</td><td align=\"left\" class=\" align_left\">1.25 (.27)</td><td align=\"left\" class=\" align_left\">1.22 (.28)</td><td align=\"left\" class=\" align_left\">1.23</td><td align=\"left\" class=\" align_left\">.36</td><td align=\"left\" class=\" align_left last\">.32</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">0.58 (.18)</td><td align=\"left\" class=\" align_left\">1.08 (.42)</td><td align=\"left\" class=\" align_left\">0.60 (.21)</td><td align=\"left\" class=\" align_left\">.32 (.10)</td><td align=\"left\" class=\" align_left\">2.31</td><td align=\"left\" class=\" align_left\">.15</td><td align=\"left\" class=\" align_left last\">.46</td></tr></tbody></table></div><div id=\"table-content-t0007\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 7 </span> Robot impact on Fetch and Carry</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">\u00a0</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left\">ME<i>F</i>(3, 8)</th><th align=\"left\" class=\"rowsep1 align_left\">ME<i>p</i></th><th align=\"left\" class=\"rowsep1 align_left\">ME\u03b7<sup>2</sup></th><th align=\"left\" class=\"rowsep1 align_left\">In <i>F</i>(3,8)</th><th align=\"left\" class=\"rowsep1 align_left\">In<i>p</i></th><th align=\"left\" class=\"rowsep1 align_left last\">In\u03b7<sup>2</sup></th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.20 (0.20)</td><td align=\"left\" class=\" align_left\">0.20 (0.20)</td><td align=\"left\" class=\" align_left\">0.17 (0.25)</td><td align=\"left\" class=\" align_left\">3.83</td><td align=\"left\" class=\" align_left\">.08</td><td align=\"left\" class=\" align_left\">.29</td><td align=\"left\" class=\" align_left\">1.24</td><td align=\"left\" class=\" align_left\">.33</td><td align=\"left\" class=\" align_left last\">.22</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.95 (1.54)</td><td align=\"left\" class=\" align_left\">0.27 (0.20)</td><td align=\"left\" class=\" align_left\">0.20 (0.20)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">3.26 (1.41)</td><td align=\"left\" class=\" align_left\">3.09 (1.91)</td><td align=\"left\" class=\" align_left\">2.70 (1.86)</td><td align=\"left\" class=\" align_left\">16.16</td><td align=\"left\" class=\" align_left\">.01*</td><td align=\"left\" class=\" align_left\">.62</td><td align=\"left\" class=\" align_left\">13.17</td><td align=\"left\" class=\" align_left\">.01*</td><td align=\"left\" class=\" align_left last\">.75</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.26 (0.54)</td><td align=\"left\" class=\" align_left\">1.83 (1.29)</td><td align=\"left\" class=\" align_left\">1.70 (1.07)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.39 (1.01)</td><td align=\"left\" class=\" align_left\">2.25 (1.47)</td><td align=\"left\" class=\" align_left\">1.95 (1.67)</td><td align=\"left\" class=\" align_left\">7.65</td><td align=\"left\" class=\" align_left\">.04*</td><td align=\"left\" class=\" align_left\">.35</td><td align=\"left\" class=\" align_left\">4.00</td><td align=\"left\" class=\" align_left\">.05*</td><td align=\"left\" class=\" align_left last\">.47</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">6.10 (5.84)</td><td align=\"left\" class=\" align_left\">2.25 (1.68)</td><td align=\"left\" class=\" align_left\">1.67 (1.47)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.61 (0.67)</td><td align=\"left\" class=\" align_left\">0.83 (1.13)</td><td align=\"left\" class=\" align_left\">0.33 (0.16)</td><td align=\"left\" class=\" align_left\">7.65</td><td align=\"left\" class=\" align_left\">.02</td><td align=\"left\" class=\" align_left\">.43</td><td align=\"left\" class=\" align_left\">1.41</td><td align=\"left\" class=\" align_left\">.29</td><td align=\"left\" class=\" align_left last\">.24</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.01 (3.11)</td><td align=\"left\" class=\" align_left\">1.46 (1.78)</td><td align=\"left\" class=\" align_left\">0.56 (0.89)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Effort</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">2.72 (1.03)</td><td align=\"left\" class=\" align_left\">1.84 (1.40)</td><td align=\"left\" class=\" align_left\">2.46 (1.33)</td><td align=\"left\" class=\" align_left\">4.61</td><td align=\"left\" class=\" align_left\">.05*</td><td align=\"left\" class=\" align_left\">.32</td><td align=\"left\" class=\" align_left\">5.64</td><td align=\"left\" class=\" align_left\">.03*</td><td align=\"left\" class=\" align_left last\">.56</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.28 (2.50)</td><td align=\"left\" class=\" align_left\">1.61 (1.00)</td><td align=\"left\" class=\" align_left\">1.22 (0.96)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.03 (0.85)</td><td align=\"left\" class=\" align_left\">0.88 (0.66)</td><td align=\"left\" class=\" align_left\">1.02 (1.14)</td><td align=\"left\" class=\" align_left\">1.83</td><td align=\"left\" class=\" align_left\">.21</td><td align=\"left\" class=\" align_left\">.16</td><td align=\"left\" class=\" align_left\">2.28</td><td align=\"left\" class=\" align_left\">.16</td><td align=\"left\" class=\" align_left last\">.34</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.21 (0.19)</td><td align=\"left\" class=\" align_left\">0.65 (0.96)</td><td align=\"left\" class=\" align_left\">1.05 (0.85)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table></div><div id=\"table-content-t0008\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 8 </span> Robot impact on Cognitive Prosthetic</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Dimension</th><th align=\"left\" class=\"rowsep1 align_left\">\u00a0</th><th align=\"center\" class=\"rowsep1 align_center\">Week 2</th><th align=\"center\" class=\"rowsep1 align_center\">Week 5</th><th align=\"center\" class=\"rowsep1 align_center\">Week 8</th><th align=\"center\" class=\"rowsep1 align_center\">ME<i>F</i>(3, 8)</th><th align=\"center\" class=\"rowsep1 align_center\">ME<i>p</i></th><th align=\"center\" class=\"rowsep1 align_center\">ME\u03b7<sup>2</sup></th><th align=\"center\" class=\"rowsep1 align_center\">In<i>F</i>(3, 8)</th><th align=\"center\" class=\"rowsep1 align_center\">In<i>p</i></th><th align=\"center\" class=\"rowsep1 align_center\">In\u03b7<sup>2</sup></th><th align=\"left\" class=\"rowsep1 align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Mental</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">2.13 (1.63)</td><td align=\"left\" class=\" align_left\">2.21 (1.71)</td><td align=\"left\" class=\" align_left\">1.81 (1.26)</td><td align=\"left\" class=\" align_left\">16.24</td><td align=\"left\" class=\" align_left\">.01*</td><td align=\"left\" class=\" align_left\">.62</td><td align=\"left\" class=\" align_left\">0.42</td><td align=\"left\" class=\" align_left\">.67</td><td align=\"left\" class=\" align_left\">.08</td><td class=\"auto-generated last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.76 (0.90)</td><td align=\"left\" class=\" align_left\">0.73 (0.98)</td><td align=\"left\" class=\" align_left\">0.88 (1.72)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Physical</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.77 (1.08)</td><td align=\"left\" class=\" align_left\">0.63 (0.85)</td><td align=\"left\" class=\" align_left\">0.60 (0.61)</td><td align=\"left\" class=\" align_left\">0.34</td><td align=\"left\" class=\" align_left\">.58</td><td align=\"left\" class=\" align_left\">.01</td><td align=\"left\" class=\" align_left\">2.80</td><td align=\"left\" class=\" align_left\">.11</td><td align=\"left\" class=\" align_left\">.38</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.36 (0.38)</td><td align=\"left\" class=\" align_left\">0.49 (0.65)</td><td align=\"left\" class=\" align_left\">0.83 (0.75)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Temporal</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.93 (1.53)</td><td align=\"left\" class=\" align_left\">1.04 (0.99)</td><td align=\"left\" class=\" align_left\">1.22 (1.41)</td><td align=\"left\" class=\" align_left\">3.63</td><td align=\"left\" class=\" align_left\">.09</td><td align=\"left\" class=\" align_left\">.27</td><td align=\"left\" class=\" align_left\">0.48</td><td align=\"left\" class=\" align_left\">.64</td><td align=\"left\" class=\" align_left\">.10</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.38 (1.64)</td><td align=\"left\" class=\" align_left\">1.92 (1.63)</td><td align=\"left\" class=\" align_left\">1.04 (0.99)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Performance</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">0.99 (0.99)</td><td align=\"left\" class=\" align_left\">0.76 (0.72)</td><td align=\"left\" class=\" align_left\">0.68 (1.05)</td><td align=\"left\" class=\" align_left\">5.90</td><td align=\"left\" class=\" align_left\">.04*</td><td align=\"left\" class=\" align_left\">.37</td><td align=\"left\" class=\" align_left\">1.23</td><td align=\"left\" class=\" align_left\">.34</td><td align=\"left\" class=\" align_left\">.21</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">2.19 (1.14)</td><td align=\"left\" class=\" align_left\">1.14 (1.51)</td><td align=\"left\" class=\" align_left\">0.81 (1.06)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Effort</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.41 (1.05)</td><td align=\"left\" class=\" align_left\">1.25 (0.90)</td><td align=\"left\" class=\" align_left\">1.22 (0.94)</td><td align=\"left\" class=\" align_left\">4.79</td><td align=\"left\" class=\" align_left\">.05*</td><td align=\"left\" class=\" align_left\">.32</td><td align=\"left\" class=\" align_left\">0.23</td><td align=\"left\" class=\" align_left\">.8</td><td align=\"left\" class=\" align_left\">.05</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">0.94 (0.77)</td><td align=\"left\" class=\" align_left\">0.72 (0.87)</td><td align=\"left\" class=\" align_left\">0.95 (0.70)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Frustration</td><td align=\"left\" class=\" align_left\">Human</td><td align=\"left\" class=\" align_left\">1.18 (1.47)</td><td align=\"left\" class=\" align_left\">0.60 (0.69)</td><td align=\"left\" class=\" align_left\">0.32 (0.32)</td><td align=\"left\" class=\" align_left\">3.21</td><td align=\"left\" class=\" align_left\">.10</td><td align=\"left\" class=\" align_left\">.23</td><td align=\"left\" class=\" align_left\">0.56</td><td align=\"left\" class=\" align_left\">.59</td><td align=\"left\" class=\" align_left\">.11</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Robot</td><td align=\"left\" class=\" align_left\">1.90 (2.16)</td><td align=\"left\" class=\" align_left\">1.10 (1.47)</td><td align=\"left\" class=\" align_left\">1.34 (1.33)</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left\">\u00a0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table></div><div id=\"table-content-t0009\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 9 </span> Themes for the Fetch and Carry task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Primary theme</th><th align=\"center\" class=\"rowsep1 align_center\">Subtheme</th><th align=\"center\" class=\"rowsep1 align_center\">Week 1</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Imposed constraint</td><td align=\"left\" class=\" align_left\">Using one hand</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Benefit from the tray</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"3\" align=\"left\" class=\" align_left\">Use of the robot</td><td align=\"left\" class=\" align_left\">Mutual adaptation</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Interface</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Having to wait</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Speed of the robot</td><td align=\"left\" class=\" align_left\">Walking with the robot</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Changing speed</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"3\" align=\"left\" class=\" align_left\">Changing capabilities</td><td align=\"left\" class=\" align_left\">Changing tray</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Object manipulation</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot as partner</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Interactional aspects</td><td align=\"left\" class=\" align_left\">Enjoyment</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Social loafing</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table><div class=\"NLM_table-wrap-foot\" id=\"IDtable-wrap-foot\"><div class=\"fn-group\"></div></div></div><div id=\"table-content-t0010\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><p><span class=\"captionLabel\">TABLE 10 </span> Themes for the Cognitive Prosthetic task</p></div><colgroup><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /><col align=\"left\" class=\" align_left\" /></colgroup><thead><tr valign=\"top\" class=\"rowsep1\"><th align=\"left\" class=\"rowsep1 align_left\">Primary theme</th><th align=\"center\" class=\"rowsep1 align_center\">Subtheme</th><th align=\"left\" class=\"rowsep1 align_left\">Week 1</th><th align=\"left\" class=\"rowsep1 align_left\">Week 2</th><th align=\"left\" class=\"rowsep1 align_left\">Week 5</th><th align=\"left\" class=\"rowsep1 align_left\">Week 8</th><th align=\"left\" class=\"rowsep1 align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"3\" align=\"left\" class=\" align_left\">Imposed constraints</td><td align=\"left\" class=\" align_left\">Separation of list and dispenser</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Robot positioning</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Random order of tiles and position in list</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"4\" align=\"left\" class=\" align_left\">Performing the task</td><td align=\"left\" class=\" align_left\">Di\ufb03culty in trying to remember</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Physically manipulating the tiles</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Use of strategy</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Pen and paper</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Nonrobotic tool</td><td align=\"left\" class=\" align_left\">Tray</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Easy</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"4\" align=\"left\" class=\" align_left\">Robot benefits</td><td align=\"left\" class=\" align_left\">Infallible/no pressure</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Subversion</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Slow</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Flexibility</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\"><td rowspan=\"2\" align=\"left\" class=\" align_left\">Robot drawbacks</td><td align=\"left\" class=\" align_left\">Interface issues</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">\u2013</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Control</td><td align=\"left\" class=\" align_left\">0</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left\">+</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr></tbody></table><div class=\"NLM_table-wrap-foot\" id=\"IDtable-wrap-foot\"><div class=\"fn-group\"></div></div></div><div id=\"ack\" class=\"NLM_sec\"><h2 class=\"section-heading-2\">ACKNOWLEDGEMENTS</h2><p>We would like to thank our colleagues Michael L. Walters and Joe Saunders for helpful comments, and Fotios Papadopolous for his help with the AIBO robot.</p></div><div id=\"s0012\" class=\"NLM_sec NLM_sec-type_other NLM_sec_level_1\"><h2 id=\"_i52\" class=\"section-heading-2\">NOTE</h2><p>1. Previous studies examining the application of biologically inspired expressive behaviors to Sunflower had shown that participants found the robot's non-anthropomorphic communicative behavior very e\ufb00ective in terms of conveying the robot's intention (Koay et\u00a0al., <span class=\"ref-lnk lazy-ref\"><a data-rid=\"cit0026\" data-refLink=\"_i52 _i54 _i55\" href=\"#\">2013</a></span>).</p></div><div class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i54\" class=\"section-heading-2\">Funding</h2><p>The research leading to these results has received funding from the European Union's Seventh Framework Programme (FP7/2007-2013) under grant agreement 287624, the ACCOMPANY project, and grant agreement 215554, the LIREC (LIving with Robots and intEractive Companions) project.</p></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneFull\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"c8b0dea6-9842-46af-b708-142fe9107344\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"c8b0dea6-9842-46af-b708-142fe9107344\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div></div><div id=\"references-Section\"><h2 id=\"figures\">REFERENCES</h2><ul class=\"references numeric-ordered-list\"><li id=\"cit0001\"><span><span class=\"hlFld-ContribAuthor\">Anderson, <span class=\"NLM_given-names\">M.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. L.</span> Anderson</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Ethical healthcare agents</span>. In <i>Advanced computational intelligence paradigms in healthcare\u20133</i>, ed. M. Sordo, S. Vaidya, L. C. Jain, <span class=\"NLM_fpage\">233</span>\u2013<span class=\"NLM_lpage\">57</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0001&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-3-540-77662-8_10\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=233-57&author=M.+Anderson&author=S.+L.+Anderson&title=Ethical+healthcare+agents\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0002\"><span><span class=\"hlFld-ContribAuthor\">Bartneck, <span class=\"NLM_given-names\">C.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Rosalia</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Menges</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Deckers</span>. <span class=\"NLM_year\">2005</span>. <span class=\"NLM_article-title\">Robot abuse\u2014A limitation of the media equation</span>. <i>Proceedings of the Interact 2005 Workshop on Agent Abuse</i>, September, Rome, Italy, 54\u20137.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&author=C.+Bartneck&author=C.+Rosalia&author=R.+Menges&author=I.+Deckers&title=Robot+abuse%E2%80%94A+limitation+of+the+media+equation\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0003\"><span><span class=\"hlFld-ContribAuthor\">Bickmore, <span class=\"NLM_given-names\">T.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Cassell</span>. <span class=\"NLM_year\">2005</span>. <span class=\"NLM_chapter-title\">Social dialogue with embodied conversational agents</span>. In <i>Advances in natural multimodal dialogue systems</i>, ed. J. C. J. van Kuppevelt, L. Dybkj\u00e6r, and N. Ole Bernsen, <span class=\"NLM_fpage\">23</span>\u2013<span class=\"NLM_lpage\">54</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0003&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F1-4020-3933-6_2\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&pages=23-54&author=T.+Bickmore&author=J.+Cassell&title=Social+dialogue+with+embodied+conversational+agents\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0004\"><span><span class=\"hlFld-ContribAuthor\">Bickmore, <span class=\"NLM_given-names\">T. W.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Caruso</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Clough-Gorr</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Heeren</span>. <span class=\"NLM_year\">2005</span>. <span class=\"NLM_article-title\">\u2018It's just like you talk to a friend\u2019: Relational agents for older adults</span>. <i>Interacting with Computers</i> 17(6): <span class=\"NLM_fpage\">711</span>\u2013<span class=\"NLM_lpage\">35</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/j.intcom.2005.09.002</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0004&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2Fj.intcom.2005.09.002\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0004&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000233471900007\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=17&publication_year=2005&pages=711-35&issue=6&author=T.+W.+Bickmore&author=L.+Caruso&author=K.+Clough-Gorr&author=T.+Heeren&title=%E2%80%98It%27s+just+like+you+talk+to+a+friend%E2%80%99%3A+Relational+agents+for+older+adults&doi=10.1016%2Fj.intcom.2005.09.002\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0005\"><span><span class=\"hlFld-ContribAuthor\">Blythe, <span class=\"NLM_given-names\">M. A.,</span></span> and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P. C.</span> Wright</span>. <span class=\"NLM_year\">2006</span>. <span class=\"NLM_article-title\">Pastiche scenarios: Fiction as a resource for user centred design</span>. <i>Interacting with Computers</i> 18(5): <span class=\"NLM_fpage\">1139</span>\u2013<span class=\"NLM_lpage\">64</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/j.intcom.2006.02.001</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0005&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2Fj.intcom.2006.02.001\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0005&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000241345400015\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=18&publication_year=2006&pages=1139-64&issue=5&author=M.+A.%2C+Blythe&author=P.+C.+Wright&title=Pastiche+scenarios%3A+Fiction+as+a+resource+for+user+centred+design&doi=10.1016%2Fj.intcom.2006.02.001\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0006\"><span><span class=\"hlFld-ContribAuthor\">Buchenau, <span class=\"NLM_given-names\">M.,</span></span> and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. F.</span> Suri</span>. <span class=\"NLM_year\">2000</span>. <span class=\"NLM_chapter-title\">Experience prototyping</span>. In <i>Proceedings of the 3rd conference on designing interactive systems: Processes, practices, methods, and techniques</i>, <span class=\"NLM_fpage\">424</span>\u2013<span class=\"NLM_lpage\">33</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0006&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F347642.347802\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2000&pages=424-33&author=M.%2C+Buchenau&author=J.+F.+Suri&title=Experience+prototyping\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0007\"><span><span class=\"hlFld-ContribAuthor\">Carroll, <span class=\"NLM_given-names\">J. M.</span></span> <span class=\"NLM_year\">2000</span>. <span class=\"NLM_article-title\">Five reasons for scenario-based design</span>. <i>Interacting with Computers</i> 13(1): <span class=\"NLM_fpage\">43</span>\u2013<span class=\"NLM_lpage\">60</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/S0953-5438(00)00023-0</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0007&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2FS0953-5438%2800%2900023-0\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0007&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000089375800003\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=13&publication_year=2000&pages=43-60&issue=1&author=J.+M.+Carroll&title=Five+reasons+for+scenario-based+design&doi=10.1016%2FS0953-5438%2800%2900023-0\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0008\"><span><span class=\"hlFld-ContribAuthor\">Chang, <span class=\"NLM_given-names\">Y.-N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.-K.</span> Lim</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Stolterman</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Personas: From theory to practices</span>. In <i>Proceedings of the 5th Nordic conference on human\u2013computer interaction: Building bridges</i>, <span class=\"NLM_fpage\">439</span>\u2013<span class=\"NLM_lpage\">42</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0008&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1463160.1463214\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=439-42&author=Y.-N.+Chang&author=Y.-K.+Lim&author=E.+Stolterman&title=Personas%3A+From+theory+to+practices\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0009\"><span><span class=\"hlFld-ContribAuthor\">Chatley, <span class=\"NLM_given-names\">A. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Christianson</span>. <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Theatre as a discussion tool in human\u2013robot interaction experiments-a pilot study</span>. In <i>Advances in computer\u2013human interactions</i>, <i>2010</i>, <span class=\"NLM_fpage\">73</span>\u2013<span class=\"NLM_lpage\">8</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0009&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FACHI.2010.17\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=73-8&author=A.+R.+Chatley&author=K.+Dautenhahn&author=M.+L.+Walters&author=D.+S.+Syrdal&author=B.+Christianson&title=Theatre+as+a+discussion+tool+in+human%E2%80%93robot+interaction+experiments-a+pilot+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0010\"><span><span class=\"hlFld-ContribAuthor\">Coradeschi, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Cesta</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Cortellessa</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Coraci</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Gonzalez</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Karlsson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Furfari</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Loutfi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Orlandini</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Palumbo</span>, et\u00a0al. <span class=\"NLM_year\">2013</span>. <span class=\"NLM_chapter-title\">Gira\ufb00plus: Combining social interaction and long term monitoring for promoting independent living</span>. In <i>Human system interaction (HSI), 2013</i>, <span class=\"NLM_fpage\">578</span>\u2013<span class=\"NLM_lpage\">85</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0010&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FHSI.2013.6577883\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=578-85&author=S.+Coradeschi&author=A.+Cesta&author=G.+Cortellessa&author=L.+Coraci&author=J.+Gonzalez&author=L.+Karlsson&author=F.+Furfari&author=A.+Loutfi&author=A.+Orlandini&author=F.+Palumbo&title=Gira%EF%AC%80plus%3A+Combining+social+interaction+and+long+term+monitoring+for+promoting+independent+living\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0011\"><span>Danish Technological Institute. <span class=\"NLM_year\">2012</span>. James\u2014Robot butler. <a class=\"ext-link\" href=\"http://robot.dti.dk/en/projects/james-robot-butler.aspx\" target=\"_blank\">http://robot.dti.dk/en/projects/james-robot-butler.aspx</a> (accessed <span class=\"NLM_date-in-citation\"><span class=\"NLM_month\">December</span> <span class=\"NLM_day\">14</span>, <span class=\"NLM_2012\">2012</span></span>).<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Danish+Technological+Institute.+2012.+James%E2%80%94Robot+butler.+http%3A%2F%2Frobot.dti.dk%2Fen%2Fprojects%2Fjames-robot-butler.aspx+%28accessed+December+14%2C+2012%29.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0012\"><span><span class=\"hlFld-ContribAuthor\">Dautenhahn, <span class=\"NLM_given-names\">K.</span></span> <span class=\"NLM_year\">2007</span>. <span class=\"NLM_article-title\">Methodology and themes of human\u2013robot interaction: A growing research field</span>. <i>International Journal of Advanced Robotic Systems</i> 4(1): <span class=\"NLM_fpage\">103</span>\u2013<span class=\"NLM_lpage\">8</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=4&publication_year=2007&pages=103-8&issue=1&author=K.+Dautenhahn&title=Methodology+and+themes+of+human%E2%80%93robot+interaction%3A+A+growing+research+field\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0013\"><span><span class=\"hlFld-ContribAuthor\">Dautenhahn, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Woods</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C. L.</span> Nehaniv</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Sisbot</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Alami</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Sim\u00e9on</span>. <span class=\"NLM_year\">2006</span>. <span class=\"NLM_chapter-title\">How may I serve you?: A robot companion approaching a seated person in a helping context</span>. In <i>Proceedings of the 1st ACM SIGCHI/SIGART conference on human\u2013robot interaction</i>, <span class=\"NLM_fpage\">172</span>\u2013<span class=\"NLM_lpage\">9</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0013&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1121241.1121272\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=172-9&author=K.+Dautenhahn&author=M.+Walters&author=S.+Woods&author=K.+L.+Koay&author=C.+L.+Nehaniv&author=A.+Sisbot&author=R.+Alami&author=T.+Sim%C3%A9on&title=How+may+I+serve+you%3F%3A+A+robot+companion+approaching+a+seated+person+in+a+helping+context\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0014\"><span><span class=\"hlFld-ContribAuthor\">Dindler, <span class=\"NLM_given-names\">C.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">O. S.</span> Iversen</span>. <span class=\"NLM_year\">2007</span>. <span class=\"NLM_article-title\">Fictional inquiry\u2014Design collaboration in a shared narrative space</span>. <i>CoDesign</i> 3(4): <span class=\"NLM_fpage\">213</span>\u2013<span class=\"NLM_lpage\">234</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1080/15710880701500187</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0014&amp;dbid=20&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1080%2F15710880701500187&amp;tollfreelink=2_18_a948775870ae22c47570c8b51f32b52c6487267f3a56ea70318c64598d237d4a\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=3&publication_year=2007&pages=213-234&issue=4&author=C.+Dindler&author=O.+S.+Iversen&title=Fictional+inquiry%E2%80%94Design+collaboration+in+a+shared+narrative+space&doi=10.1080%2F15710880701500187\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0015\"><span><span class=\"hlFld-ContribAuthor\">Du\ufb00y, <span class=\"NLM_given-names\">B. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G. M.</span> O\u2019Hare</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. N.</span> Martin</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. F.</span> Bradley</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Schon</span>. <span class=\"NLM_year\">2003</span>. <span class=\"NLM_chapter-title\">Agent chameleons: Agent minds and bodies</span>. In <i> 16th international conference on computer animation and social agents, 2003</i>, <span class=\"NLM_fpage\">118</span>\u2013<span class=\"NLM_lpage\">25</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0015&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FCASA.2003.1199312\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2003&pages=118-25&author=B.+R.+Du%EF%AC%80y&author=G.+M.+O%E2%80%99Hare&author=A.+N.+Martin&author=J.+F.+Bradley&author=B.+Schon&title=Agent+chameleons%3A+Agent+minds+and+bodies\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0016\"><span><span class=\"hlFld-ContribAuthor\">Duque, <span class=\"NLM_given-names\">I.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Willcock</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Christianson</span>. <span class=\"NLM_year\">2013</span>. <span class=\"NLM_chapter-title\">Knowledge-driven user activity recognition for a smart house: Development and validation of a generic and low-cost, resource-e\ufb03cient system</span>. In <i>ACHI 2013, The sixth international conference on advances in computer\u2013human interactions</i>, Nice, France, February 24\u2013March 1, <span class=\"NLM_fpage\">141</span>\u2013<span class=\"NLM_lpage\">6</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&pages=141-6&author=I.+Duque&author=K.+Dautenhahn&author=K.+L.+Koay&author=I.+Willcock&author=B.+Christianson&title=Knowledge-driven+user+activity+recognition+for+a+smart+house%3A+Development+and+validation+of+a+generic+and+low-cost%2C+resource-e%EF%AC%83cient+system\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0017\"><span><span class=\"hlFld-ContribAuthor\">Fernaeus, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> H\u00e5kansson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Jacobsson</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Ljungblad</span>. <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">How do you play with a robotic toy animal?: A long-term study of Pleo</span>. In <i>Proceedings of the 9th international conference on interaction design and children</i>, <span class=\"NLM_fpage\">39</span>\u2013<span class=\"NLM_lpage\">48</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0017&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1810543.1810549\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=39-48&author=Y.+Fernaeus&author=M.+H%C3%A5kansson&author=M.+Jacobsson&author=S.+Ljungblad&title=How+do+you+play+with+a+robotic+toy+animal%3F%3A+A+long-term+study+of+Pleo\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0018\"><span><span class=\"hlFld-ContribAuthor\">Fussell, <span class=\"NLM_given-names\">S. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Kiesler</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L. D.</span> Setlock</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V.</span> Yew</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">How people anthropomorphize robots</span>. In <i>Proceedings of the 3rd ACM/IEEE international conference on human\u2013robot interaction</i>, <span class=\"NLM_fpage\">145</span>\u2013<span class=\"NLM_lpage\">52</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0018&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1349822.1349842\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=145-52&author=S.+R.+Fussell&author=S.+Kiesler&author=L.+D.+Setlock&author=V.+Yew&title=How+people+anthropomorphize+robots\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0019\"><span><span class=\"hlFld-ContribAuthor\">Hart, <span class=\"NLM_given-names\">S. G.</span></span> <span class=\"NLM_year\">2006</span>. <span class=\"NLM_chapter-title\">NASA-task load index (NASA-TLX); 20 years later</span>. <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 50: <span class=\"NLM_fpage\">904</span>\u2013<span class=\"NLM_lpage\">8</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0019&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1177%2F154193120605000909\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&pages=904-8&author=S.+G.+Hart&title=NASA-task+load+index+%28NASA-TLX%29%3B+20+years+later\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0020\"><span><span class=\"hlFld-ContribAuthor\">Hart, <span class=\"NLM_given-names\">S. G.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L. E.</span> Staveland</span>. <span class=\"NLM_year\">1988</span>. <span class=\"NLM_article-title\">Development of NASA-TLX (task load index): Results of empirical and theoretical research</span>. <i>Human Mental Workload</i> 1(3): <span class=\"NLM_fpage\">139</span>\u2013<span class=\"NLM_lpage\">83</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/S0166-4115(08)62386-9</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0020&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2FS0166-4115%2808%2962386-9\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=1988&pages=139-83&issue=3&author=S.+G.+Hart&author=L.+E.+Staveland&title=Development+of+NASA-TLX+%28task+load+index%29%3A+Results+of+empirical+and+theoretical+research&doi=10.1016%2FS0166-4115%2808%2962386-9\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0021\"><span><span class=\"hlFld-ContribAuthor\">Horne, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Weinman</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Barber</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Elliott</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Morgan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Cribb</span>, et\u00a0al. <span class=\"NLM_year\">2005</span>. <i>Concordance, adherence and compliance in medicine taking</i>. <span class=\"NLM_publisher-loc\">London, UK</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&author=R.+Horne&author=J.+Weinman&author=N.+Barber&author=R.+Elliott&author=M.+Morgan&author=A.+Cribb&title=Concordance%2C+adherence+and+compliance+in+medicine+taking\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0022\"><span><span class=\"hlFld-ContribAuthor\">Huijnen, <span class=\"NLM_given-names\">C.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Badii</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> van den Heuvel</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Caleb-Solly</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Thiemert</span>. <span class=\"NLM_year\">2011</span>. \u2018<span class=\"NLM_chapter-title\">Maybe it becomes a buddy, but do not call it a robot\u2019\u2014Seamless cooperation between companion robotics and smart homes</span>. In <i>Ambient intelligence</i>, <span class=\"NLM_fpage\">324</span>\u2013<span class=\"NLM_lpage\">29</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0022&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-3-642-25167-2_44\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=324-29&author=C.+Huijnen&author=A.+Badii&author=H.+van+den+Heuvel&author=P.+Caleb-Solly&author=D.+Thiemert&title=Maybe+it+becomes+a+buddy%2C+but+do+not+call+it+a+robot%E2%80%99%E2%80%94Seamless+cooperation+between+companion+robotics+and+smart+homes\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0023\"><span><span class=\"hlFld-ContribAuthor\">H\u00fcttenrauch, <span class=\"NLM_given-names\">H.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. S.</span> Eklundh</span>. <span class=\"NLM_year\">2002</span>. <span class=\"NLM_chapter-title\">Fetch-and-carry with CERO: Observations from a long-term user study with a service robot</span>. In <i>11th IEEE international workshop on robot and human interactive communication, 2002, Proceedings</i>, ed. D. Keyson, M. L. Maher, N. Streitz, A. D. Cheok, J. C. Augusto, R. Wichert, G. Englebienne, H. Aghajan, and B. Kr\u00f6se, <span class=\"NLM_fpage\">158</span>\u2013<span class=\"NLM_lpage\">63</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0023&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2002.1045615\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2002&pages=158-63&author=H.+H%C3%BCttenrauch&author=K.+S.+Eklundh&title=Fetch-and-carry+with+CERO%3A+Observations+from+a+long-term+user+study+with+a+service+robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0024\"><span><span class=\"hlFld-ContribAuthor\">Kamsma, <span class=\"NLM_given-names\">Y. P.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W. H.</span> Brouwer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. P.</span> Lakke</span>. <span class=\"NLM_year\">1995</span>. <span class=\"NLM_article-title\">Training of compensational strategies for impaired gross motor skills in Parkinson's disease</span>. <i>Physiotherapy Theory and Practice</i> 11(4): <span class=\"NLM_fpage\">209</span>\u2013<span class=\"NLM_lpage\">29</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.3109/09593989509036407</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0024&amp;dbid=20&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.3109%2F09593989509036407&amp;tollfreelink=2_18_27bce6455306b13bcd4b08dad2149196457f73cc55e6dfc1360715d5231f6138\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=11&publication_year=1995&pages=209-29&issue=4&author=Y.+P.+Kamsma&author=W.+H.+Brouwer&author=J.+P.+Lakke&title=Training+of+compensational+strategies+for+impaired+gross+motor+skills+in+Parkinson%27s+disease&doi=10.3109%2F09593989509036407\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0025\"><span><span class=\"hlFld-ContribAuthor\">Kidd, <span class=\"NLM_given-names\">C. D.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Breazeal</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Robots at home: Understanding long-term human\u2013robot interaction</span>. In <i>Intelligent robots and systems, 2008, IROS 2008</i>, <span class=\"NLM_fpage\">3230</span>\u2013<span class=\"NLM_lpage\">5</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0025&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FIROS.2008.4651113\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=3230-5&author=C.+D.+Kidd&author=C.+Breazeal&title=Robots+at+home%3A+Understanding+long-term+human%E2%80%93robot+interaction\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0026\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Lakatos</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> G\u00e1csi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Bereczky</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Mikl\u00f3si</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>. <span class=\"NLM_year\">2013</span>. <span class=\"NLM_chapter-title\">Hey! There is someone at your door. A hearing robot using visual communication signals of hearing dogs to communicate intent</span>. In <i> IEEE Symposium on Artificial Life (ALIFE), 2013</i>, 90\u201397. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0026&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FALIFE.2013.6602436\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&author=K.+L.+Koay&author=G.+Lakatos&author=D.+Syrdal&author=M.+G%C3%A1csi&author=B.+Bereczky&author=K.+Dautenhahn&author=A.+Mikl%C3%B3si&author=M.+L.+Walters&title=Hey%21+There+is+someone+at+your+door.+A+hearing+robot+using+visual+communication+signals+of+hearing+dogs+to+communicate+intent\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0027\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E. A.</span> Sisbot</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Alami</span>. <span class=\"NLM_year\">2007</span>. <span class=\"NLM_chapter-title\">Exploratory study of a robot approaching a person in the context of handing over an object</span>. In <i>AAAI spring symposium: Multidisciplinary collaboration for socially assistive robotics</i>, <span class=\"NLM_fpage\">18</span>\u2013<span class=\"NLM_lpage\">24</span>. <span class=\"NLM_publisher-loc\">Menlo Park, CA</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=18-24&author=K.+L.+Koay&author=E.+A.+Sisbot&author=D.+S.+Syrdal&author=M.+L.+Walters&author=K.+Dautenhahn&author=R.+Alami&title=Exploratory+study+of+a+robot+approaching+a+person+in+the+context+of+handing+over+an+object\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0028\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Ashgari-Oskoei</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2014</span>. <span class=\"NLM_article-title\">Social roles and baseline proxemic preferences for a domestic service robot</span>. <i>International Journal of Social Robotics</i> 6(4): 469\u201388. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1007/s12369-014-0232-4</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0028&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2Fs12369-014-0232-4\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=K.+L.+Koay&author=D.+S.+Syrdal&author=M.+Ashgari-Oskoei&author=M.+L.+Walters&author=K.+Dautenhahn&title=Social+roles+and+baseline+proxemic+preferences+for+a+domestic+service+robot&doi=10.1007%2Fs12369-014-0232-4\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0029\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Arent</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Malek</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Kreczmer</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_chapter-title\">Companion migration\u2013initial participants\u2019 feedback from a video-based prototyping study</span>. In <i>Mixed reality and human\u2013robot interaction</i>, ed. X. Wang, <span class=\"NLM_fpage\">133</span>\u2013<span class=\"NLM_lpage\">51</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0029&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-94-007-0582-1_8\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=133-51&author=K.+L.+Koay&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=K.+Arent&author=L.+Malek&author=B.+Kreczmer&title=Companion+migration%E2%80%93initial+participants%E2%80%99+feedback+from+a+video-based+prototyping+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0030\"><span><span class=\"hlFld-ContribAuthor\">Koay, <span class=\"NLM_given-names\">K. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">Five weeks in the robot house\u2014Exploratory human\u2013robot interaction trials in a domestic setting</span>. In <i>Advances in computer\u2013human interactions, 2009, ACHI\u201909</i>, <span class=\"NLM_fpage\">219</span>\u2013<span class=\"NLM_lpage\">26</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=219-26&author=K.+L.+Koay&author=D.+S.+Syrdal&author=M.+L.+Walters&author=K.+Dautenhahn&title=Five+weeks+in+the+robot+house%E2%80%94Exploratory+human%E2%80%93robot+interaction+trials+in+a+domestic+setting\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0031\"><span><span class=\"hlFld-ContribAuthor\">Lammer, <span class=\"NLM_given-names\">L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Huber</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Weiss</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Vincze</span>. <span class=\"NLM_year\">2014</span>. <span class=\"NLM_chapter-title\">Mutual care: How older adults react when they should help their care robot</span>. In <i>AISB2014: Proceedings of the 3rd international symposium on new frontiers in human\u2013robot interaction</i>, <span class=\"NLM_publisher-name\">Routledge</span>, April 1\u20134. <span class=\"NLM_publisher-loc\">London, UK</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=L.+Lammer&author=A.+Huber&author=A.+Weiss&author=M.+Vincze&title=Mutual+care%3A+How+older+adults+react+when+they+should+help+their+care+robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0032\"><span><span class=\"hlFld-ContribAuthor\">Latane, <span class=\"NLM_given-names\">B.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Williams</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Harkins</span>. <span class=\"NLM_year\">1979</span>. <span class=\"NLM_article-title\">Many hands make light the work: The causes and consequences of social loafing</span>. <i>Journal of Personality and Social Psychology</i> 37(6): <span class=\"NLM_fpage\">822</span>\u2013<span class=\"NLM_lpage\">32</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1037/0022-3514.37.6.822</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0032&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1037%2F0022-3514.37.6.822\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0032&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1979HH08400002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=37&publication_year=1979&pages=822-32&issue=6&author=B.+Latane&author=K.+Williams&author=S.+Harkins&title=Many+hands+make+light+the+work%3A+The+causes+and+consequences+of+social+loafing&doi=10.1037%2F0022-3514.37.6.822\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0033\"><span><span class=\"hlFld-ContribAuthor\">Loe, <span class=\"NLM_given-names\">M.</span></span> <span class=\"NLM_year\">2010</span>. <span class=\"NLM_article-title\">Doing it my way: Old women, technology and wellbeing</span>. <i>Sociology of Health &amp; Illness</i> 32(2): <span class=\"NLM_fpage\">319</span>\u2013<span class=\"NLM_lpage\">34</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1111/j.1467-9566.2009.01220.x</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0033&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1111%2Fj.1467-9566.2009.01220.x\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0033&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=20149150\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0033&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000274711000011\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=32&publication_year=2010&pages=319-34&issue=2&author=M.+Loe&title=Doing+it+my+way%3A+Old+women%2C+technology+and+wellbeing&doi=10.1111%2Fj.1467-9566.2009.01220.x\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0034\"><span><span class=\"hlFld-ContribAuthor\">Modayil, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Levinson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Harman</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Halper</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kautz</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Integrating sensing and cueing for more e\ufb00ective activity reminders</span>. In <i>AAAI Fall 2008 Symposium on AI in Eldercare: New solutions to old problems</i>, <span class=\"NLM_fpage\">7</span>\u2013<span class=\"NLM_lpage\">9</span>. <span class=\"NLM_publisher-loc\">Menlo Park, CA</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=7-9&author=J.+Modayil&author=R.+Levinson&author=C.+Harman&author=D.+Halper&author=H.+Kautz&title=Integrating+sensing+and+cueing+for+more+e%EF%AC%80ective+activity+reminders\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0035\"><span><span class=\"hlFld-ContribAuthor\">Newell, <span class=\"NLM_given-names\">A. F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Carmichael</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Morgan</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Dickinson</span>. <span class=\"NLM_year\">2006</span>. <span class=\"NLM_article-title\">The use of theatre in requirements gathering and usability studies</span>. <i>Interacting with Computers</i> 18(5): <span class=\"NLM_fpage\">996</span>\u2013<span class=\"NLM_lpage\">1011</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1016/j.intcom.2006.05.003</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0035&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1016%2Fj.intcom.2006.05.003\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0035&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000241345400007\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=18&publication_year=2006&pages=996-1011&issue=5&author=A.+F.+Newell&author=A.+Carmichael&author=M.+Morgan&author=A.+Dickinson&title=The+use+of+theatre+in+requirements+gathering+and+usability+studies&doi=10.1016%2Fj.intcom.2006.05.003\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0036\"><span><span class=\"hlFld-ContribAuthor\">Parlitz, <span class=\"NLM_given-names\">C.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> H\u00e4gele</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Klein</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Seifert</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_article-title\">Care-O-Bot 3\u2014Rationale for human\u2013robot interaction design</span>. <i>Proceedings of 39th International Symposium on Robotics (ISR)</i>, Seoul, Korea, October 15\u201317, <span class=\"NLM_fpage\">275</span>\u2013<span class=\"NLM_lpage\">80</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=275-80&author=C.+Parlitz&author=M.+H%C3%A4gele&author=P.+Klein&author=J.+Seifert&author=K.+Dautenhahn&title=Care-O-Bot+3%E2%80%94Rationale+for+human%E2%80%93robot+interaction+design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0037\"><span><span class=\"hlFld-ContribAuthor\">Payr, <span class=\"NLM_given-names\">S.</span></span> <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Closing and closure in human\u2013companion interactions: Analyzing video data from a field study</span>. In <i>2010 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">476</span>\u2013<span class=\"NLM_lpage\">81</span>. <span class=\"NLM_publisher-loc\">New York, NY</span>: <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0037&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2010.5598625\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=476-81&author=S.+Payr&title=Closing+and+closure+in+human%E2%80%93companion+interactions%3A+Analyzing+video+data+from+a+field+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0038\"><span><span class=\"hlFld-ContribAuthor\">Rubio, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> D\u00edaz</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Mart\u00edn</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Puente</span>. <span class=\"NLM_year\">2004</span>. <span class=\"NLM_article-title\">Evaluation of subjective mental workload: A comparison of SWAT, NASA-TLX, and workload profile methods</span>. <i>Applied Psychology</i> 53(1): <span class=\"NLM_fpage\">61</span>\u2013<span class=\"NLM_lpage\">86</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1111/j.1464-0597.2004.00161.x</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0038&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1111%2Fj.1464-0597.2004.00161.x\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0038&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000188753000004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=53&publication_year=2004&pages=61-86&issue=1&author=S.+Rubio&author=E.+D%C3%ADaz&author=J.+Mart%C3%ADn&author=J.+M.+Puente&title=Evaluation+of+subjective+mental+workload%3A+A+comparison+of+SWAT%2C+NASA-TLX%2C+and+workload+profile+methods&doi=10.1111%2Fj.1464-0597.2004.00161.x\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0039\"><span><span class=\"hlFld-ContribAuthor\">Russo, <span class=\"NLM_given-names\">J. E.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E. J.</span> Johnson</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. L.</span> Stephens</span>. <span class=\"NLM_year\">1989</span>. <span class=\"NLM_article-title\">The validity of verbal protocols</span>. <i>Memory &amp; Cognition</i> 17(6): <span class=\"NLM_fpage\">759</span>\u2013<span class=\"NLM_lpage\">69</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.3758/BF03202637</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0039&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.3758%2FBF03202637\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0039&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=2811673\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0039&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1989AX07500012\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=17&publication_year=1989&pages=759-69&issue=6&author=J.+E.+Russo&author=E.+J.+Johnson&author=D.+L.+Stephens&title=The+validity+of+verbal+protocols&doi=10.3758%2FBF03202637\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0040\"><span><span class=\"hlFld-ContribAuthor\">Schwartz, <span class=\"NLM_given-names\">D.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Wang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Zeitz</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. E.</span> Goss</span>. <span class=\"NLM_year\">1962</span>. <span class=\"NLM_article-title\">Medication errors made by elderly, chronically ill patients</span>. <i>American Journal of Public Health and the Nation's Health</i> 52(12): <span class=\"NLM_fpage\">2018</span>\u2013<span class=\"NLM_lpage\">2029</span> http://dx.doi.org/<span class=\"NLM_pub-id\">10.2105/AJPH.52.12.2018</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0040&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.2105%2FAJPH.52.12.2018\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0040&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=13987359\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0040&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1962B392600018\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=52&publication_year=1962&pages=2018-2029&issue=12&author=D.+Schwartz&author=M.+Wang&author=L.+Zeitz&author=M.+E.+Goss&title=Medication+errors+made+by+elderly%2C+chronically+ill+patients&doi=10.2105%2FAJPH.52.12.2018\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0041\"><span><span class=\"hlFld-ContribAuthor\">Seland, <span class=\"NLM_given-names\">G.</span></span> <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">Empowering end users in design of mobile technology using role play as a method: Reflections on the role-play conduction</span>. In <i>Human centered design</i>, ed. M. Kurosu, <span class=\"NLM_fpage\">912</span>\u2013<span class=\"NLM_lpage\">21</span>. <span class=\"NLM_publisher-loc\">Berlin, Germany:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0041&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2F978-3-642-02806-9_105\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=912-21&author=G.+Seland&title=Empowering+end+users+in+design+of+mobile+technology+using+role+play+as+a+method%3A+Reflections+on+the+role-play+conduction\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0042\"><span><span class=\"hlFld-ContribAuthor\">Sharkey, <span class=\"NLM_given-names\">A.,</span></span> and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Sharkey</span>. <span class=\"NLM_year\">2012</span>. <span class=\"NLM_article-title\">Granny and the robots: Ethical issues in robot care for the elderly</span>. <i>Ethics and Information Technology</i> 14(1): <span class=\"NLM_fpage\">27</span>\u2013<span class=\"NLM_lpage\">40</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1007/s10676-010-9234-6</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0042&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2Fs10676-010-9234-6\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0042&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000301566800004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=14&publication_year=2012&pages=27-40&issue=1&author=A.%2C+Sharkey&author=N.+Sharkey&title=Granny+and+the+robots%3A+Ethical+issues+in+robot+care+for+the+elderly&doi=10.1007%2Fs10676-010-9234-6\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0043\"><span><span class=\"hlFld-ContribAuthor\">Stienstra, <span class=\"NLM_given-names\">J.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Marti</span>. <span class=\"NLM_year\">2012</span>. <span class=\"NLM_chapter-title\">Squeeze me: Gently please</span>. In <i>Proceedings of the 7th Nordic conference on human\u2013computer interaction: Making sense through design</i>, <span class=\"NLM_fpage\">746</span>\u2013<span class=\"NLM_lpage\">50</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0043&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F2399016.2399131\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&pages=746-50&author=J.+Stienstra&author=P.+Marti&title=Squeeze+me%3A+Gently+please\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0044\"><span><span class=\"hlFld-ContribAuthor\">Sung, <span class=\"NLM_given-names\">J.-Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. E.</span> Grinter</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H. I.</span> Christensen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Guo</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Housewives or technophiles?: Understanding domestic robot owners</span>. In <i>2008 3rd ACM/IEEE International Conference on human\u2013robot interaction (HRI)</i>, <span class=\"NLM_fpage\">129</span>\u2013<span class=\"NLM_lpage\">36</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0044&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1349822.1349840\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=129-36&author=J.-Y.+Sung&author=R.+E.+Grinter&author=H.+I.+Christensen&author=L.+Guo&title=Housewives+or+technophiles%3F%3A+Understanding+domestic+robot+owners\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0045\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W. C.</span> Ho</span>. <span class=\"NLM_year\">2014</span>. <span class=\"NLM_article-title\">Views from within a narrative: Evaluating long-term human\u2013robot interaction in a naturalistic environment using open-ended scenarios</span>. <i>Cognitive Computation</i> 6(4): 741\u201359..<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=K.+L.+Koay&author=W.+C.+Ho&title=Views+from+within+a+narrative%3A+Evaluating+long-term+human%E2%80%93robot+interaction+in+a+naturalistic+environment+using+open-ended+scenarios\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0046\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>. <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">The negative attitudes towards robots scale and reactions to robot behaviour in a live human\u2013robot interaction study</span>. <i>New frontiers in human\u2013robot interaction, a symposium at the AISB2009 Convention</i>, <span class=\"NLM_publisher-loc\">Edinburgh, UK</span>, April <span class=\"NLM_fpage\">6</span>\u2013<span class=\"NLM_lpage\">9</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=6-9&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=K.+L.+Koay&author=M.+L.+Walters&title=The+negative+attitudes+towards+robots+scale+and+reactions+to+robot+behaviour+in+a+live+human%E2%80%93robot+interaction+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0047\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N. R.</span> Otero</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_chapter-title\">The theatre methodology for facilitating discussion in human\u2013robot interaction on information disclosure in a home environment</span>. In <i>2011 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">479</span>\u2013<span class=\"NLM_lpage\">84</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0047&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2011.6005247\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=479-84&author=D.+S.+Syrdal&author=K.+Dautenhahn&author=M.+L.+Walters&author=K.+L.+Koay&author=N.+R.+Otero&title=The+theatre+methodology+for+facilitating+discussion+in+human%E2%80%93robot+interaction+on+information+disclosure+in+a+home+environment\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0048\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> G\u00e1csi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Video prototyping of dog-inspired non-verbal a\ufb00ective communication for an appearance constrained robot</span>. In <i>2010 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">632</span>\u2013<span class=\"NLM_lpage\">7</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0048&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2010.5598693\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=632-7&author=D.+S.+Syrdal&author=K.+L.+Koay&author=M.+G%C3%A1csi&author=M.+L.+Walters&author=K.+Dautenhahn&title=Video+prototyping+of+dog-inspired+non-verbal+a%EF%AC%80ective+communication+for+an+appearance+constrained+robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0049\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. L.</span> Walters</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2009</span>. <span class=\"NLM_chapter-title\">The boy-robot should bark!\u2014Children's impressions of agent migration into diverse embodiments</span>. In <i>Proceedings of the new frontiers in human\u2013robot interaction, a symposium at the AISB2009 Convention</i>, <span class=\"NLM_publisher-loc\">Edinburgh, UK</span>, April <span class=\"NLM_fpage\">6</span>\u2013<span class=\"NLM_lpage\">9</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=6-9&author=D.+S.+Syrdal&author=K.+L.+Koay&author=M.+L.+Walters&author=K.+Dautenhahn&title=The+boy-robot+should+bark%21%E2%80%94Children%27s+impressions+of+agent+migration+into+diverse+embodiments\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0050\"><span><span class=\"hlFld-ContribAuthor\">Syrdal, <span class=\"NLM_given-names\">D. S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Otero</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_chapter-title\">Video prototyping in human\u2013robot interaction: Results from a qualitative study</span>. In <i>Proceedings of the 15th European conference on cognitive ergonomics: The ergonomics of cool interaction</i>, <span class=\"NLM_fpage\">29</span>\u2013<span class=\"NLM_lpage\">35</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0050&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1145%2F1473018.1473055\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=29-35&author=D.+S.+Syrdal&author=N.+Otero&author=K.+Dautenhahn&title=Video+prototyping+in+human%E2%80%93robot+interaction%3A+Results+from+a+qualitative+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0051\"><span><span class=\"hlFld-ContribAuthor\">Tapus, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Tapus</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. J.</span> Matari\u0107</span>. <span class=\"NLM_year\">2008</span>. <span class=\"NLM_article-title\">User\u2013robot personality matching and assistive robot behavior adaptation for post-stroke rehabilitation therapy</span>. <i>Intelligent Service Robotics</i> 1(2): <span class=\"NLM_fpage\">169</span>\u2013<span class=\"NLM_lpage\">83</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1007/s11370-008-0017-4</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0051&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1007%2Fs11370-008-0017-4\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=2008&pages=169-83&issue=2&author=A.+Tapus&author=C.+Tapus&author=M.+J.+Matari%C4%87&title=User%E2%80%93robot+personality+matching+and+assistive+robot+behavior+adaptation+for+post-stroke+rehabilitation+therapy&doi=10.1007%2Fs11370-008-0017-4\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0052\"><span><span class=\"hlFld-ContribAuthor\">Walker, <span class=\"NLM_given-names\">J. E.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Howland</span>. <span class=\"NLM_year\">1991</span>. <span class=\"NLM_article-title\">Falls and fear of falling among elderly persons living in the community: Occupational therapy interventions</span>. <i>American Journal of Occupational Therapy</i> 45(2): <span class=\"NLM_fpage\">119</span>\u2013<span class=\"NLM_lpage\">22</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.5014/ajot.45.2.119</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0052&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.5014%2Fajot.45.2.119\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=cit0052&amp;dbid=8&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=2035588\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=cit0052&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=A1991EW43400004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=45&publication_year=1991&pages=119-22&issue=2&author=J.+E.+Walker&author=J.+Howland&title=Falls+and+fear+of+falling+among+elderly+persons+living+in+the+community%3A+Occupational+therapy+interventions&doi=10.5014%2Fajot.45.2.119\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0053\"><span><span class=\"hlFld-ContribAuthor\">Walters, <span class=\"NLM_given-names\">M. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Lohse</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Hanheide</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Wrede</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Koay</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Green</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> H\u00fcttenrauch</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Sagerer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Severinson Eklund</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_article-title\">Evaluating the robot personality and verbal behavior of domestic robots using video-based studies</span>. <i>Advanced Robotics</i> 25(18): <span class=\"NLM_fpage\">2233</span>\u2013<span class=\"NLM_lpage\">54</span>. http://dx.doi.org/<span class=\"NLM_pub-id\">10.1163/016918611X603800</span><span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0053&amp;dbid=20&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1163%2F016918611X603800&amp;tollfreelink=2_18_24537b9100757d6fa3e14b34d82744337f494015f85d55a2df0627a244ec9fc3\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=cit0053&amp;dbid=128&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=000297801000001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=25&publication_year=2011&pages=2233-54&issue=18&author=M.+L.+Walters&author=M.+Lohse&author=M.+Hanheide&author=B.+Wrede&author=D.+S.+Syrdal&author=K.+L.+Koay&author=A.+Green&author=H.+H%C3%BCttenrauch&author=K.+Dautenhahn&author=G.+Sagerer&author=K.+Severinson+Eklund&title=Evaluating+the+robot+personality+and+verbal+behavior+of+domestic+robots+using+video-based+studies&doi=10.1163%2F016918611X603800\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0054\"><span><span class=\"hlFld-ContribAuthor\">Walters, <span class=\"NLM_given-names\">M. L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. A.</span> Oskoei</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. S.</span> Syrdal</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Dautenhahn</span>. <span class=\"NLM_year\">2011</span>. <span class=\"NLM_chapter-title\">A long-term human\u2013robot proxemic study</span>. In <i>2011 IEEE RO-MAN</i>, <span class=\"NLM_fpage\">137</span>\u2013<span class=\"NLM_lpage\">42</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0054&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1109%2FROMAN.2011.6005274\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=137-42&author=M.+L.+Walters&author=M.+A.+Oskoei&author=D.+S.+Syrdal&author=K.+Dautenhahn&title=A+long-term+human%E2%80%93robot+proxemic+study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"cit0055\"><span><span class=\"hlFld-ContribAuthor\">Yagoda, <span class=\"NLM_given-names\">R. E.</span></span> <span class=\"NLM_year\">2010</span>. <span class=\"NLM_chapter-title\">Development of the human robot interaction workload measurement tool (HRI-WM)</span>. <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 54: <span class=\"NLM_fpage\">304</span>\u2013<span class=\"NLM_lpage\">8</span>. <span class=\"NLM_publisher-loc\">New York, NY:</span> <span class=\"NLM_publisher-name\">Routledge</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=cit0055&amp;dbid=16&amp;doi=10.1080%2F01972243.2015.1020212&amp;key=10.1177%2F154193121005400408\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2010&pages=304-8&author=R.+E.+Yagoda&title=Development+of+the+human+robot+interaction+workload+measurement+tool+%28HRI-WM%29\" target=\"_blank\">[Google Scholar]</a></span></span></span></li></ul></div><div class=\"response\"><div class=\"sub-article-title\"></div></div>", "</article>", "</div>", "<div class=\"tab tab-pane\" id=\"relatedContent\">", "</div>", "<div class=\"tab tab-pane \" id=\"metrics-content\">", "<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics\" data-pb-dropzone=\"publicationContentDropZoneMetrics\">", "<div class=\"widget literatumArticleMetricsWidget none  widget-none\" id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"literatumArticleMetricsWidget\" data-ajax-widget-id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"access__limit\" data-pb-dropzone=\"accessLimitPage\">", "</div>", "</div>", "</div>", "</div>", "<input id=\"viewLargeImageCaption\" type=\"hidden\" value=\"View Large Image\" /></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-4 \">", "<div class=\"contents\" data-pb-dropzone=\"contents2\">", "<div class=\"widget general-bookmark-share none  widget-none  widget-compact-all\" id=\"c8494935-e102-4ff5-9395-4ffa44a77f1c\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\">", "<ul>", "<li>", "<div class=\"addthis_toolbox addthis_20x20_style\">", "<div class=\"custom_images\">", "<a class=\"addthis_button_twitter\">", "<span class=\"at-icon-twitter\"></span>", "</a>", "<a class=\"addthis_button_facebook\">", "<span class=\"at-icon-facebook\"></span>", "</a>", "<a class=\"addthis_button_email\">", "<span class=\"at-icon-email\"></span>", "</a>", "<a class=\"addthis_button_none\">", "<span class=\"at-icon-none\"></span>", "</a>", "<a class=\"addthis_button_compact\" tabindex=\"-1\"><span class=\"at-icon-wrapper\"></span>", "<span aria-describedby=\"shareOptions-description\">", "<span class=\"off-screen\" id=\"shareOptions-description\">More Share Options</span>", "</span>", "</a>", "</div>", "</div>", "</li>", "</ul>", "<script type=\"text/javascript\">", "    ", "    var script = document.createElement('script');", "    script.type='text/javascript';", "    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';", "    script.async = true;", "    $('head').append(script)", "</script>", "</div>", "</div>", "</div>", "<div class=\"widget general-html none  widget-none\" id=\"16111d74-c554-42b2-a277-f2727ad2b285\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \">&nbsp;</div>", "</div>", "</div>", "<div class=\"widget layout-tabs none further-fonts collapsed-view further-tab-margin collapsed-sticky widget-none  widget-compact-vertical\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"layout-tabs-dropzone\" data-pb-dropzone=\"layoutTabsDropzone\">", "<div class=\"widget general-html none furtherReadingTitle widget-none  widget-compact-all\" id=\"982b80ad-6fe6-4b98-9675-9b6eef03d365\">", "<div class=\"wrapped \">", "<h2 class=\"widget-header header-none  header-compact-all\">Related research <span class=\"tooltip-collapse\"></span></h2>", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"info hide\">", "<p><b>People also read</b> lists articles that other readers of this article have read.</p>", "<p><b>Recommended articles</b> lists articles that we recommend and is powered by our AI driven recommendation engine.</p>", "<p><b>Cited by</b> lists all citing articles based on Crossref citations.<br />Articles with the Crossref icon will open in a new tab.</p>", "</div></div>", "</div>", "</div>", "</div>", "<div class=\"tabs tabs-widget \" aria-live=\"polite\" aria-atomic=\"true\" aria-relevant=\"additions\">", "<ul class=\"tab-nav\" role=\"tablist\">", "<li class=\"active\" role=\"tab\">", "<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" title=\"show People also read\" class=\"\">People also read</a>", "</li>", "<li class=\"\" role=\"tab\">", "<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\" title=\"show Recommended articles\" class=\"\">Recommended articles</a>", "</li>", "<li class=\"\" role=\"tab\">", "<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" title=\"show Cited by\" class=\"frwidget-tabs--cby\">Cited by</a>", "</li>", "</ul>", "<div class=\"tab-content\">", "<div class=\"tab-pane active\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\">", "<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" data-pb-dropzone-name=\"People also read\">", "<div class=\"widget ajaxCFCRWidget none  widget-none  widget-compact-all\" id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCFCRWidget\" data-ajax-widget-id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\">", "<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-b6de7b7c-de82-45a5-9538-313dd15c6659\" data-pb-dropzone-name=\"Recommended articles\">", "<div class=\"widget ajaxAtmCRWidget none  widget-none  widget-compact-all\" id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxAtmCRWidget\" data-ajax-widget-id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\" data-ajax-spin=\"true\" data-ajax-observe=\"true\"></div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\">", "<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" data-pb-dropzone-name=\"Cited by\">", "<div class=\"widget ajaxCitedByWidget none  widget-none  widget-compact-all\" id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCitedByWidget\" data-ajax-widget-id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "<div class=\"widget pageFooter none  widget-none  widget-compact-all\" id=\"d97c173f-d838-4de1-bbd7-ed69f0d36a91\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><footer class=\"page-footer\">", "<div data-pb-dropzone=\"main\">", "<div class=\"widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all\" id=\"1f15adc0-4a59-4d27-93fe-8cbb14a5108a\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md gutterless \">", "<div class=\"col-md-1-1 fit-padding\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget pbOptimizerWidget none  widget-none  widget-compact-all\" id=\"af788167-0054-4892-bd47-5de7cbd64256\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div data-optimizer data-widget-id=\"af788167-0054-4892-bd47-5de7cbd64256\" id=\"widget-af788167-0054-4892-bd47-5de7cbd64256\" data-observer>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"64a44adf-45ed-4da3-be26-ef25beb9dbee\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"6918e9df-910a-4206-9bd0-1a02bc17f740\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container-fluid\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-1-2 footer_left_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"aa9510dd-52ed-4b74-8211-fb510cd9468e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Information for</h3>", "<ul>", "<li><a href=\"https://authorservices.taylorandfrancis.com/\">Authors</a></li>", "<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/\">Corporate partners</a></li>", "<li><a href=\"https://editorresources.taylorandfrancisgroup.com/\">Editors</a></li>", "<li><a href=\"/page/librarians\">Librarians</a></li>", "<li><a href=\"/societies\">Societies</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-sm-1-2 footer_right_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Open access</h3>", "<ul>", "<li><a href=\"/openaccess\">Overview</a></li>", "<li><a href=\"/openaccess/openjournals\">Open journals</a></li>", "<li><a href=\"/openaccess/openselect\">Open Select</a></li>", "<li><a href=\"/openaccess/dove\">Dove Medical Press</a></li>", "<li><a href=\"/openaccess/f1000\">F1000Research</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"fc564559-f496-499c-87c7-d851f371f061\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container-fluid\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-1-2 footer_left_col\">", " <div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"cdd1a577-15dc-4271-8941-33a105ec6510\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Opportunities</h3>", "<ul>", "<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/marketing/\">Reprints and e-prints</a></li>", "<li><a href=\"https://taylorandfrancis.com/partnership/commercial/advertising-solutions/\" class=\"footer-ad-click\">Advertising solutions</a></li>", "<li><a href=\"https://taylorandfrancis.com/partnership/commercial/accelerated-publication/\">Accelerated publication</a></li>", "<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/purchasing-options/\">Corporate access solutions</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-sm-1-2 footer_right_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"f3fb3d36-db42-4373-9d0e-432958bf2fbc\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Help and information</h3>", "<ul>", "<li><a href=\"https://help.tandfonline.com\">Help and contact</a></li>", "<li><a href=\"https://newsroom.taylorandfrancisgroup.com/\">Newsroom</a></li>", "<li><a href=\"/action/showPublications?pubType=journal\">All journals</a></li>", "<li><a href=\"https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot\">Books</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"b2eecf80-9109-455e-a805-028552718986\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"b997c64c-ce48-41ce-b3d6-9cb2d1c99131\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container-fluid\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-1-2 footer_left_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"914433f6-0ea6-4a47-9781-07564061be86\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-social-label\">", "<h3>Keep up to date</h3>", "</div>", "<div class=\"font-size-correction-sml\">Register to receive personalised research and resources by email</div>", "<div class=\"bs\">", "<div class=\"pull-left links font-size-correction\">", "<a class=\"font-size-correction-link\" href=\"https://taylorandfrancis.formstack.com/forms/tfoguest_signup\"><i class=\"fa fa-envelope-square\" title=\"Register to receive personalised research and resources by email\"></i>Sign me up</a>", "</div></div></div>", "</div>", "</div>", "<div class=\"widget literatumSocialLinks none  widget-none  widget-compact-all\" id=\"3b6a5e53-cd62-452f-adc1-92e187a0849d\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"bs\">", "<div class=\"pull-left links\">", "<a href=\"http://facebook.com/TaylorandFrancisGroup\">", "<i class=\"icon-facebook\" title=\"Taylor and Francis Group Facebook page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"fb-description\">", "<span class=\"off-screen\" id=\"fb-description\">Taylor and Francis Group Facebook page</span>", "</span>", "</a>", "</div>", "<div class=\"pull-left links\">", "<a href=\"https://twitter.com/tandfonline\">", "<i class=\"fa fa-twitter-square\" title=\"Taylor and Francis Group Twitter page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"twitter-description\">", "<span class=\"off-screen\" id=\"twitter-description\">Taylor and Francis Group Twitter page</span>", "</span>", "</a>", "</div>", "<div class=\"pull-left links\">", "<a href=\"http://linkedin.com/company/taylor-&-francis-group\">", "<i class=\"fa fa-linkedin-square\" title=\"Taylor and Francis Group LinkedIn page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"linkedin-description\">", "<span class=\"off-screen\" id=\"linkedin-description\">Taylor and Francis Group Linkedin page</span>", "</span>", "</a>", "</div>", "<div class=\"clearfix\"></div>", "<div class=\"pull-left links\">", "<a href=\"https://www.youtube.com/user/TaylorandFrancis\">", "<i class=\"fa fa-youtube-square\" title=\"Taylor and Francis Group YouTube page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"youtube-description\">", "<span class=\"off-screen\" id=\"youtube-description\">Taylor and Francis Group Youtube page</span>", "</span>", "</a>", "</div>", "<div class=\"pull-left links\">", "<a href=\"http://www.weibo.com/tandfchina\">", "<i class=\"fa fa-weibo\" title=\"Taylor and Francis Group Weibo page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"weibo-description\">", "<span class=\"off-screen\" id=\"weibo-description\">Taylor and Francis Group Weibo page</span>", "</span>", "</a>", "</div>", "<div class=\"clearfix\"></div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-sm-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none  widget-none  widget-compact-horizontal\" id=\"8d803f96-081d-4768-ab7d-280a77af723b\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-3-4 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none footer-info-container widget-none  widget-compact-vertical\" id=\"b247ecb9-84c9-4762-b270-20f8be1f0ae4\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"informa-group-info\">", "<span>Copyright \u00a9 2021 Informa UK Limited</span>", "<span><a href=\"https://informa.com/privacy-policy/\">Privacy policy</a></span>", "<span><a href=\"/cookies\">Cookies</a></span>", "<span><a href=\"/terms-and-conditions\">Terms & conditions</a></span>", "<span><a href=\"/accessibility\">Accessibility</a></span>", "<p>Registered in England & Wales No. 3099067<br />", "5 Howick Place | London | SW1P 1WG</p>", "</div></div>", "</div>", "</div>", " </div>", "</div>", "<div class=\"col-sm-1-4 footer_tandf_logo\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget general-image none  widget-none  widget-compact-vertical\" id=\"b6bde365-079b-454f-94f6-1841291656a1\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><a href=\"http://taylorandfrancis.com/\" title=\"Taylor and Francis Group\">", "<img src=\"/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png\" alt=\"Taylor and Francis Group\" />", "</a></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget cookiePolicy none  widget-none  widget-compact-all\" id=\"cea739ac-da2c-4d77-9cf1-cb3e0da7e31e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"banner\">", "<a href=\"#\" class=\"btn\">Accept</a>", "<p class=\"message\">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href=\"/cookies\">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>", "</div></div>", "</div>", "</div>", "</div>", "</footer></div>", "</div>", "</div>", "</div>", "</div>", "<script type=\"text/javascript\" src=\"/wro/kriw~product.js\"></script>", "<script>", "loadCSS(\"/wro/kriw~lastInBody-css.css\");", "loadCSS(\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\");", "window.scriptSettings=[{js: \"/wro/kriw~jwplayer.js\",selector:'.mediaThumbnailContainer'},", "{js:'/wro/kriw~ajax-widgets.js',css:\"/wro/kriw~ajax-widgets.css\",selector:'.ajaxWidget'},", "{js: '/wro/kriw~loi-api.js',selector:'.toc-fns,.literatumListOfIssuesResponsiveWidget,.literatumListOfIssuesWidget'}", ",{js:\"/wro/kriw~seamless-access-fn.js\",selector: \".seamlessAccess_wrapper,.institutional-login\"}];", "window.addEventListener('load',TandfUtils.scriptLoader);", "</script>", "<noscript>", "    <link rel=\"stylesheet\" href=\"/wro/kriw~lastInBody-css.css\">", "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\">", "</noscript>", "<script defer src=\"https://static.cloudflareinsights.com/beacon.min.js/v64f9daad31f64f81be21cbef6184a5e31634941392597\" integrity=\"sha512-gV/bogrUTVP2N3IzTDKzgP0Js1gg4fbwtYB6ftgLbKQu/V8yH2+lrKCfKHelh4SO3DPzKj4/glTO+tNJGDnb0A==\" data-cf-beacon='{\"rayId\":\"6b66d9e21bdc7986\",\"token\":\"b6951d00f50a499ab38e94f58955e14d\",\"version\":\"2021.11.0\",\"si\":100}' crossorigin=\"anonymous\"></script>", "</body>", "</html>"]}