{"content": "<!DOCTYPE html>\n<html lang=\"en\" class=\"pb-page\" data-request-id=\"47f9a879-9772-47a6-8b7e-7a353d33de60\"><head data-pb-dropzone=\"head\"><meta name=\"pbContext\" content=\";journal:journal:tbit20;requestedJournal:journal:tbit20;page:string:Article/Chapter View;ctype:string:Journal Content;article:article:10.1080/0144929X.2020.1838610;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;subPage:string:Full Text\" />\n<link rel=\"schema.DC\" href=\"http://purl.org/DC/elements/1.0/\" /><meta name=\"citation_journal_title\" content=\"Behaviour &amp; Information Technology\" /><meta name=\"dc.Title\" content=\"Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas\" /><meta name=\"dc.Creator\" content=\"Joni  Salminen\" /><meta name=\"dc.Creator\" content=\"Soon-gyo  Jung\" /><meta name=\"dc.Creator\" content=\"Ahmed Mohamed Sayed  Kamel\" /><meta name=\"dc.Creator\" content=\"Jo\u00e3o M.  Santos\" /><meta name=\"dc.Creator\" content=\"Bernard J.  Jansen\" /><meta name=\"dc.Subject\" content=\"Evaluation; human\u2013computer interaction; user behaviour; human factors; artificially generated facial pictures\" /><meta name=\"dc.Description\" content=\"We conduct two studies to evaluate the suitability of artificially generated facial pictures for use in a customer-facing system using data-driven personas. STUDY 1 investigates the quality of a sa...\" /><meta name=\"Description\" content=\"We conduct two studies to evaluate the suitability of artificially generated facial pictures for use in a customer-facing system using data-driven personas. STUDY 1 investigates the quality of a sa...\" /><meta name=\"dc.Publisher\" content=\"Taylor &amp; Francis\" /><meta name=\"dc.Date\" scheme=\"WTN8601\" content=\"6 Nov 2020\" /><meta name=\"dc.Type\" content=\"research-article\" /><meta name=\"dc.Format\" content=\"text/HTML\" /><meta name=\"dc.Identifier\" scheme=\"publisher-id\" content=\"1838610\" /><meta name=\"dc.Identifier\" scheme=\"doi\" content=\"10.1080/0144929X.2020.1838610\" /><meta name=\"dc.Identifier\" scheme=\"submission-id\" content=\"TBIT-2020-0373.R1\" /><meta name=\"dc.Source\" content=\"https://doi.org/10.1080/0144929X.2020.1838610\" /><meta name=\"dc.Language\" content=\"en\" /><meta name=\"dc.Coverage\" content=\"world\" /><meta name=\"dc.Rights\" content=\"\u00a9 2020 The Author(s). Published by Informa UK Limited, trading as Taylor &amp; Francis Group\" /><meta name=\"keywords\" content=\"Evaluation,human\u2013computer interaction,user behaviour,human factors,artificially generated facial pictures\" /><meta name=\"citation_fulltext_world_readable\" content=\"\" />\n<link rel=\"meta\" type=\"application/atom+xml\" href=\"https://doi.org/10.1080%2F0144929X.2020.1838610\" />\n<link rel=\"meta\" type=\"application/rdf+json\" href=\"https://doi.org/10.1080%2F0144929X.2020.1838610\" />\n<link rel=\"meta\" type=\"application/unixref+xml\" href=\"https://doi.org/10.1080%2F0144929X.2020.1838610\" />\n<title>Full article: Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</title>\n<meta charset=\"UTF-8\">\n<meta name=\"robots\" content=\"noarchive\" />\n\n<meta property=\"og:title\" content=\"Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas\" />\n<meta property=\"og:type\" content=\"article\" />\n<meta property=\"og:url\" content=\"https://www.tandfonline.com/doi/abs/10.1080/0144929X.2020.1838610\" />\n<meta property=\"og:image\" content=\"https://www.tandfonline.com/doi/cover-img/10.1080/tbit20\" />\n<meta property=\"og:site_name\" content=\"Taylor & Francis\" />\n<meta property=\"og:description\" content=\"(2020). Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas. Behaviour &amp; Information Technology. Ahead of Print.\" />\n<meta name=\"twitter:card\" content=\"summary_large_image\">\n<meta name=\"twitter:site\" content=\"@tandfonline\">\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n<link href=\"//connect.facebook.net\" rel=\"preconnect\" />\n<link href=\"//go.taylorandfrancis.com\" rel=\"preconnect\" />\n<link href=\"//pi.pardot.com\" rel=\"preconnect\" />\n<link href=\"//static.hotjar.com\" rel=\"preconnect\" />\n<link href=\"//cdn.pbgrd.com\" rel=\"preconnect\" />\n<link href=\"//f1-eu.readspeaker.com\" rel=\"preconnect\" />\n<link href=\"//www.googleadservices.com\" rel=\"preconnect\" />\n<link href=\"https://m.addthis.com\" rel=\"preconnect\" />\n<link href=\"https://wl.figshare.com\" rel=\"preconnect\" />\n<link href=\"https://pagead2.googlesyndication.com\" rel=\"preconnect\" />\n<link href=\"https://www.googletagmanager.com\" rel=\"preconnect\" />\n<link href=\"https://www.google-analytics.com\" rel=\"preconnect\" />\n<link href=\"https://fonts.googleapis.com\" rel=\"preconnect\" />\n<link href=\"https://fonts.gstatic.com\" rel=\"preconnect\" />\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/css/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/icomoon/icomoon.woff?g276mb\" type=\"font/woff\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-regular.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">\n<link type=\"text/css\" rel=\"stylesheet\" href=\"/wro/kriw~product.css\">\n<link rel=\"stylesheet\" type=\"text/css\" href=\"/pb/css/t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" id=\"pb-css\" data-pb-css-id=\"t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" />\n<script type=\"text/javascript\" src=\"//cdn.pbgrd.com/core-tandf.js\" async></script>\n<script data-ad-client=\"ca-pub-5143040550582507\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\" async></script>\n\n<script>\n    (function(h,o,t,j,a,r){\n        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};\n        h._hjSettings={hjid:864760,hjsv:6};\n        a=o.getElementsByTagName('head')[0];\n        r=o.createElement('script');r.async=1;\n        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;\n        a.appendChild(r);\n    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');\n</script>\n<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>\n<script type=\"text/javascript\">\n        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};\n    </script>\n<script type=\"application/javascript\" src=\"//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr\" id=\"read-speaker\" async></script>\n<script>var tandfData={\"search\":{\"cbRec\":8},\"seamlessAccess\":{\"apiUrl\":\"https://service.seamlessaccess.org/ps/\",\"context\":\"seamlessaccess.org\"},\"identity\":{\"isSpv\":false,\"isAuthenticated\":false},\"pubCount\":{\"citedCount\":1},\"actionLog\":{\"eventGroupKey\":\"d77116a5-639a-4772-974b-29cb497068d5\"}};</script>\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n<link rel=\"canonical\" href=\"https://www.tandfonline.com/doi/full/10.1080/0144929X.2020.1838610\" />\n</head>\n<body class=\"pb-ui\">\n\n<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>\n\n<noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>\n\n\n<script type=\"text/javascript\" src=\"/wro/kriw~jquery-3.5.0.js\"></script>\n<div class=\"skipContent off-screen\"><a href=\"#top-content-scroll\" class=\"skipToContent\" title=\"Skip to Main Content\" tabIndex=\"0\">Skip to Main Content</a></div>\n<script type=\"text/javascript\">(function(e){var t=e.getElementsByClassName(\"skipToContent\");t.length>0&&(t[0].onclick=function(){var t=e.getElementById(\"top-content-scroll\");null==t&&(t=e.getElementsByClassName(\"top-content-scroll\").item(0)),t.setAttribute(\"tabindex\",\"0\"),t.focus()})})(document);</script>\n<div id=\"pb-page-content\" data-ng-non-bindable>\n<div data-pb-dropzone=\"main\" data-pb-dropzone-name=\"Main\">\n\n<div class=\"widget pageHeader none  widget-none  widget-compact-all\" id=\"a4d4fdd3-c594-4d68-9f06-b69b8b37ed56\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><header class=\"page-header\" aria-label=\"Main Banner\">\n<div data-pb-dropzone=\"main\">\n<div class=\"widget responsive-layout none header-top widget-none  widget-compact-all\" id=\"036fa949-dc25-4ffe-9df0-d7daefee281b\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-xs  \">\n<div class=\"col-xs-1-6 header-index\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal\" id=\"e817489e-2520-418b-a731-b62e247e74df\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\" title=\"Taylor and Francis Online\">\n<img src=\"/pb-assets/Global/tfo_logo-1444989687640.png\" alt=\"Taylor and Francis Online\" />\n</a></div>\n</div>\n</div>\n<div class=\"widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal\" id=\"b3fe8380-8b88-4558-b004-6485d3aea155\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\">\n<img src=\"/pb-assets/Global/tfo_logo_sm-1459688573210.png\" />\n</a></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-xs-5-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget layout-inline-content alignRight  widget-none  widget-compact-all\" id=\"a8a37801-55c7-4566-bdef-e4e738967e38\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">\n<div class=\"widget layout-inline-content none customLoginBar widget-none\" id=\"fbe90803-b9c8-4bef-9365-cb53cc4bfa0e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">\n<div class=\"widget literatumInstitutionBanner none bannerWidth widget-none\" id=\"3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"institution-image hidden-xs logout-institution-image\">\n</div></div>\n</div>\n</div>\n<div class=\"widget literatumNavigationLoginBar none  widget-none  widget-compact-all\" id=\"1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"loginBar not-logged-in\">\n<span class=\"icon-user\"></span>\n<a href=\"/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F0144929X.2020.1838610\" class=\"sign-in-link\">\nLog in\n</a>\n<span class=\"loginSeprator\">&nbsp;|&nbsp;</span>\n<a href=\"/action/registration?redirectUri=%2F\" class=\"register-link\">\nRegister\n</a>\n</div></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget eCommerceCartIndicatorWidget none literatumCartLink widget-none\" id=\"9de10bb5-08af-48bc-b9f6-3f6433229f3e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><a href=\"/action/showCart?FlowID=1\" class=\"cartLabel\">\n<span class=\"hidden-xs hidden-sm visible-tl-inline-block\">Cart</span>\n<span class=\"cartItems\" data-id=\"cart-size\" role=\"status\">\n</span>\n</a></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none breadcrumbs-container widget-none  widget-compact-all\" id=\"64c16283-4b04-4d90-ac0f-4db85fcd0cf5\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget literatumBreadcrumbs none breadcrumbs-widget widget-none  widget-compact-all\" id=\"b1c121c1-cbbd-4241-8774-7120f3a783e8\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\">\n<nav aria-label=\"Breadcrumb\">\n<ol class=\"breadcrumbs\">\n<li class=\"\">\n<a href=\"/\" class=\"bc-click\">\nHome\n</a>\n\n</li>\n<li class=\"\">\n<a href=\"/action/showPublications?pubType=journal\" class=\"bc-click\">\nAll Journals\n</a>\n</li>\n<li class=\"\">\n<a href=\"/toc/tbit20/current\" class=\"bc-click\">\nBehaviour & Information Technology\n</a>\n</li>\n<li class=\"\">\n <a href=\"/loi/tbit20\" class=\"bc-click\">\nList of Issues\n</a>\n</li>\n<li class=\"\">\n<a href=\"/toc/tbit20/0/0\" class=\"bc-click\">\nLatest Articles\n</a>\n</li>\n<li class=\"\">\n<a href=\"#\" class=\"bc-click\" aria-current=\"page\">\nUsing artificially generated pictures in ....\n</a>\n</li>\n</ol>\n</nav>\n<script type=\"application/ld+json\">\n    {\n        \"@context\": \"https://schema.org\",\n        \"@type\": \"BreadcrumbList\",\n        \"itemListElement\":\n        [{\n            \"@type\": \"ListItem\",\n            \"position\": \"1\",\n            \"name\": \"Home\"\n            ,\"item\": \"https://www.tandfonline.com/\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"2\",\n            \"name\": \"All Journals\"\n            ,\"item\": \"https://www.tandfonline.com/action/showPublications?pubType=journal\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"3\",\n            \"name\": \"Behaviour & Information Technology\"\n            ,\"item\": \"https://www.tandfonline.com/toc/tbit20/current\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"4\",\n            \"name\": \"List of Issues\"\n            ,\"item\": \"https://www.tandfonline.com/loi/tbit20\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"5\",\n            \"name\": \"Latest Articles\"\n            ,\"item\": \"https://www.tandfonline.com/toc/tbit20/0/0\"\n        },\n        {\n            \"@type\": \"ListItem\",\n            \"position\": \"6\",\n            \"name\": \"Using artificially generated pictures in .... \"\n            \n        }\n        ]\n    }\n</script></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</header></div>\n</div>\n</div>\n<div data-widget-def=\"pageBody\" data-widget-id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\" role=\"main\">\n<div class=\"widget pageBody none  widget-none  widget-compact-all\" id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\">\n<div class=\"page-body pagefulltext\">\n<div data-pb-dropzone=\"main\">\n<div class=\"widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all\" id=\"1728e801-36cd-4288-9f53-392bad29506a\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md gutterless \">\n<div class=\"col-md-5-12 search_container \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget quickSearchWidget none search-customize-width widget-none  widget-compact-all\" id=\"d46e3260-1f5c-4802-821a-28a03a699c82\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"quickSearchFormContainer \">\n<form action=\"/action/doSearch\" name=\"quickSearch\" class=\"quickSearchForm \" title=\"Quick Search\" role=\"search\" method=\"get\" onsubmit=\"appendSearchFilters(this)\" aria-label=\"Quick Search\"><span class=\"simpleSearchBoxContainer\">\n<input name=\"AllField\" class=\"searchText main-search-field autocomplete\" value=\"\" type=\"search\" id=\"searchText\" title=\"Type search term here\" aria-label=\"Search\" placeholder=\"Enter keywords, authors, DOI, ORCID etc\" autocomplete=\"off\" data-history-items-conf=\"3\" data-publication-titles-conf=\"3\" data-publication-items-conf=\"3\" data-topics-conf=\"3\" data-contributors-conf=\"3\" data-fuzzy-suggester=\"false\" data-auto-complete-target=\"title-auto-complete\" />\n</span>\n<span class=\"searchDropDownDivRight\">\n<label for=\"searchInSelector\" class=\"visuallyhidden\">Search in:</label>\n<select id=\"searchInSelector\" name=\"SeriesKey\" class=\"js__searchInSelector\">\n<option value=\"tbit20\" id=\"thisJournal\" data-search-in=\"thisJournal\">\nThis Journal\n</option>\n<option value=\"\" data-search-in=\"default\">\nAnywhere\n</option>\n</select>\n</span>\n<span class=\"quick-search-btn\">\n<input class=\"mainSearchButton searchButtons pointer\" title=\"Search\" role=\"button\" type=\"submit\" value=\"\" aria-label=\"Search\" />\n </span></form>\n</div>\n<div class=\"advancedSearchLinkDropZone\" data-pb-dropzone=\"advancedSearchLinkDropZone\">\n<div class=\"widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all\" id=\"323e2a31-1c81-4995-bd17-8e149458c214\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><a href=\"/search/advanced\" class=\"advSearchArticle\">Advanced search</a></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-7-12 serNav_container\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget literatumSeriesNavigation none  widget-none\" id=\"7730bfe1-9fca-4cf4-a6d6-2a0148105437\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"issueSerialNavigation journal\">\n<div class=\"cover\">\n<img src=\"/action/showCoverImage?journalCode=tbit20\" alt=\"Publication Cover\" width=\"90\" height=\"120\" />\n</div>\n<div class=\"info \">\n<div class=\"title-container\">\n<h1 class=\"journal-heading\">\n<a href=\"/toc/tbit20/current\">\nBehaviour &amp; Information Technology\n</a>\n</h1>\n<span class=\"issue-heading\">\n<a href=\"/toc/tbit20/0/0\">Latest Articles</a>\n</span>\n</div>\n<div class=\"seriesNavDropZone\" data-pb-dropzone=\"seriesNavDropZone\">\n\n<div class=\"widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal\" id=\"753455df-1eeb-47ca-bdc9-e19022075973\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"serial-action\">\n<a href=\"http://mc.manuscriptcentral.com/tbit\" class=\"green submitAnArticle\"><span>Submit an article</span></a>\n<a href=\"/toc/tbit20/current\" class=\"jHomepage\"><span>Journal homepage</span></a>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n\n<div class=\"widget responsive-layout none  widget-none  widget-compact-vertical\" id=\"e42aea8f-434a-4d39-aaef-f56af3ff00dc\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all\" id=\"6aacf107-e82d-494d-a14c-0c00bba52560\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"accessLogo\">\n<div>\n<img class=\"accessIconLocation\" src=\"/pb-assets/3rdPartyLogos/accessOA-1452596421933.png\" alt=\"Open access\" />\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n\n<div class=\"widget responsive-layout none publicationContentHeader widget-none  widget-compact-all\" id=\"63f402e4-3498-4709-8d7d-ee8e69f93467\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical\" id=\"5afd8b6d-7e09-43ff-8ad6-afa3764e543c\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"articleMetricsContainer\">\n<div class=\"content compactView\">\n<div class=\"section\">\n<div class=\"value\">\n949\n</div>\n<div class=\"title\">\nViews\n</div>\n</div>\n<div class=\"section\">\n<div class=\"value\">\n1\n</div>\n<div class=\"title\">\nCrossRef citations to date\n</div>\n</div>\n<div class=\"section score\">\n<div class=\"altmetric-score true\">\n<div class=\"value\" data-doi=\"10.1080/0144929X.2020.1838610\">\n<span class=\"metrics-score\">0</span>\n</div>\n<div class=\"title\">\nAltmetric\n</div>\n</div>\n</div>\n<script>tandfData.altmetric={key:'be0ef6915d1b2200a248b7195d01ef22'}</script>\n<script src=\"/wro/kriw~altmetric.js\" async></script>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-2-3 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all\" id=\"fa57727f-b942-4eb8-9ed2-ecfe11ac03f5\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div id=\"read-speaker-container\" style=\"display: block; height: 40px\">\n<div id=\"readspeaker_button1\" class=\"rs_skip rsbtn rs_preserve\">\n<a href=\"//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F0144929X.2020.1838610\" rel=\"nofollow\" class=\"rsbtn_play\" accesskey=\"L\" title=\"Listen to this page using ReadSpeaker webReader\" style=\"border-radius: 0 11.4px 11.4px 2px;\">\n<span class=\"rsbtn_left rsimg rspart\"><span class=\"rsbtn_text\"><span>Listen</span></span></span>\n<span class=\"rsbtn_right rsimg rsplay rspart\"></span>\n</a>\n</div>\n</div>\n<div class=\"toc-heading\">Research Article</div>\n<h1><span class=\"NLM_article-title hlFld-title\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span></h1><div class=\"literatumAuthors\"><div class=\"publicationContentAuthors\"><div class=\"hlFld-ContribAuthor\"><span class=\"NLM_contrib-group\"><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Salminen%2C+Joni\">Joni Salminen</a><span class=\"overlay\">a Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar;b Turku School of Economics at the University of Turku, Turku, Finland</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Jung%2C+Soon-gyo\">Soon-gyo Jung</a><span class=\"overlay\">a Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\">Ahmed Mohamed Sayed Kamel</a><span class=\"overlay\">c Department of Clinical Pharmacy, Cairo University, Giza, Egypt</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Santos%2C+Jo%C3%A3o+M\">Jo\u00e3o M. Santos</a><span class=\"overlay\">d Instituto Universit\u00e1rio de Lisboa (ISCTE-IUL), Lisbon, Portugal</span></div> &amp; </span><span class=\"contribDegrees corresponding \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Jansen%2C+Bernard+J\">Bernard J. Jansen<i class=\"fa fa-envelope\" aria-hidden=\"true\" style=\"margin-left: 0.5em;\"></i></a><span class=\"overlay\">a Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar<span class=\"corr-sec\"><span class=\"heading\">Correspondence</span><span class=\"corr-email\"><i class=\"fa fa-envelope\" style=\"color: #10147E; padding-right: 7px\" aria-hidden=\"true\"></i><a href=\"mailto:jjansen@acm.org\">jjansen@acm.org</a></span><br /></span></span></div></span></span></div></div></div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none  widget-none  widget-compact-all\" id=\"5f562208-b1d5-4e5a-81c7-356431240f04\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">\n<div class=\"row row-md gutterless \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget layout-inline-content none  widget-none  widget-compact-all\" id=\"87ac5840-18fa-4a14-8eca-065b90ede3d7\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">\n\n\n<div class=\"widget literatumContentItemHistory none  widget-none  widget-compact-all\" id=\"32bf868e-52ce-411a-9dc3-717743aad997\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div>Received 16 Apr 2020</div><div>Accepted 13 Oct 2020</div><div>Published online: 06 Nov 2020</div></div>\n</div>\n</div>\n<div class=\"widget literatumArticleToolsWidget none  widget-none  widget-compact-all\" id=\"ed673666-7b5d-470e-bd33-c5c679d996cb\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"articleTools\">\n<ul class=\"linkList\">\n<li class=\"downloadCitations\">\n<a href=\"/action/showCitFormats?doi=10.1080%2F0144929X.2020.1838610&area=0000000000000001\"><i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>Download citation</a>\n</li>\n<li class=\"dx-doi\">\n<a href=\"https://doi.org/10.1080/0144929X.2020.1838610\"><i class=\"fa fa-external-link-square\" style=\"margin: 0 0.25rem 0 0\" aria-hidden=\"true\"></i>https://doi.org/10.1080/0144929X.2020.1838610</a>\n</li>\n<script src=\"/wro/kriw~crossmark.js\" async></script>\n<li class=\"cross_mark\">\n<a class=\"cross_mark--link\" data-doi=\"10.1080/0144929X.2020.1838610\" data-target=\"crossmark\" href=\"#\">\n<img src=\"/templates/jsp/images/CROSSMARK_Color_horizontal.svg\" alt=\"CrossMark Logo\" width=\"100\" height=\"22px\" />\n<span aria-describedby=\"crossMark-description\"><span class=\"off-screen\" id=\"crossMark-description\">CrossMark</span></span>\n</a>\n</li>\n</ul>\n</div></div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents2\">\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none publicationContentBody widget-none\" id=\"f4a74f7a-9ba2-4605-86b1-8094cb1f01de\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-6 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget sectionsNavigation none  widget-none\" id=\"f15bd2de-bb18-4067-8ab9-03ea3be30bf7\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"sections-nav\" role=\"navigation\" aria-label=\"Article Navigation\"><span class=\"title\">In this article<a href=\"#\" class=\"close\" tabindex=\"-1\"><span aria-label=\"close button for mobile\"><span class=\"off-screen\" id=\"close-description\">Close</span></span></a></span><ul class=\"sections-list\"><li><a href=\"#abstract\">ABSTRACT</a></li><li><span class=\"sub-art-heading\"><a href=\"#_i3\">1. Introduction</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i6\">2. Related literature</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i11\">3. Methodology</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i16\">4. STUDY 1: crowdsourced evaluation</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i23\">5. STUDY 2: effects on persona perceptions</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i34\">6. Discussion</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i39\">7. Conclusion</a></span><ul class=\"sub-art-titles\"></ul></li><li><a href=\"#coi-statement\">Disclosure statement</a></li><li><a href=\"#inline_frontnotes\">Footnotes</a></li><li><a href=\"#references-Section\">References</a></li></ul></div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-7-12 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n\n<div class=\"widget responsive-layout none rs_readArea widget-none  widget-compact-all\" id=\"9751b4f9-64b9-44c0-955b-f75246902839\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-1 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n\n<div class=\"widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all\" id=\"d29f04e9-776c-4996-a0d8-931023161e00\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">\n    MathJax.Hub.Config({\n        \"HTML-CSS\": {scale: 70, linebreaks: {automatic: true, width: \"container\"}},\n        SVG: {linebreaks: {automatic: true, width: \"25%\"}},\n        menuSettings: {zoom: \"Click\"},\n\n        /* This is necessary to lazy loading. */\n        skipStartupTypeset: true\n    });\n</script>\n<script type=\"application/javascript\" async src=\"/wro/kriw~mathjax.js\"></script>\n<div class=\"articleMeta ja\">\n<div class=\"tocHeading\">\n<h2>Research Article</h2>\n</div>\n<div class=\"hlFld-Title\">\n<div class=\"publicationContentTitle\">\n<h1 class=\"chaptertitle\">\nUsing artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas\n</h1>\n</div>\n</div>\n<div class=\"copyrightStatement\">\n</div>\n<div class=\"articleMetaDrop publicationContentDropZone\" data-pb-dropzone=\"articleMetaDropZone\">\n</div>\n<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone1\" data-pb-dropzone=\"articleMetaDropZone1\">\n</div>\n<div class=\"copyrightline\">\n</div>\n<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone2\" data-pb-dropzone=\"articleMetaDropZone2\">\n</div>\n</div>\n<div class=\"publication-tabs ja publication-tabs-dropdown\">\n<div class=\"tabs tabs-widget\">\n<ul class=\"tab-nav\" role=\"tablist\">\n<li class=\"active\" role=\"tab\">\n<a href=\"/doi/full/10.1080/0144929X.2020.1838610?scroll=top&amp;needAccess=true\" class=\"show-full\">\n<i class=\"fa fa-file-text\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">\nFull Article\n</span>\n</a>\n</li>\n<li role=\"tab\">\n<a href=\"/doi/figure/10.1080/0144929X.2020.1838610?scroll=top&amp;needAccess=true\" class=\"show-figure\">\n<i class=\"fa fa-image\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">Figures & data</span>\n</a>\n</li>\n<li role=\"tab\">\n <a href=\"/doi/ref/10.1080/0144929X.2020.1838610?scroll=top\" class=\"show-references\">\n<i class=\"fa fa-book\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">References</span>\n</a>\n</li>\n<li class=\"citedbyTab \" role=\"tab\">\n<a href=\"/doi/citedby/10.1080/0144929X.2020.1838610?scroll=top&amp;needAccess=true\">\n<i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">\nCitations\n</span>\n</a>\n</li>\n<li role=\"tab\" class=\"metrics-tab\">\n<a href=\"#metrics-content\" class=\"show-metrics\">\n<i class=\"fa fa-bar-chart\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">Metrics</span>\n</a>\n</li>\n<li role=\"tab\" class=\"licencing-tab \">\n<a href=\"/action/showCopyRight?scroll=top&amp;doi=10.1080%2F0144929X.2020.1838610\" class=\"show-copyright\">\n<i class=\"fa fa-copyright\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">Licensing</span>\n</a>\n</li>\n<li role=\"tab\" class=\"permissions-tab \">\n<a href=\"/doi/abs/10.1080/0144929X.2020.1838610?tab=permissions&amp;scroll=top\" class=\"show-permissions\">\n<i class=\"fa fa-print\" aria-hidden=\"true\"></i>\n<span class=\"nav-data\">\nReprints & Permissions</span></a>\n</li>\n<li class=\"pdf-tab \" role=\"tab\">\n<a href=\"/doi/pdf/10.1080/0144929X.2020.1838610?needAccess=true\" class=\"show-pdf\" role=\"button\" target=\"_blank\">\n<span class=\"nav-data\">\nPDF\n</span>\n</a>\n</li>\n<li class=\"epub-tab \" role=\"tab\">\n<a href=\"/doi/epub/10.1080/0144929X.2020.1838610?needAccess=true\" class=\"show-epub noBefore oneButton oneButtonDesktop \" role=\"button\" target=\"_blank\">EPUB</a>\n</li>\n</ul>\n<div class=\"tab-content \">\n<a id=\"top-content-scroll\"></a>\n<div class=\"tab tab-pane active\">\n<article class=\"article\">\n<p class=\"fulltext\"></p><div class=\"hlFld-Abstract\"><p class=\"fulltext\"></p><div class=\"sectionInfo abstractSectionHeading\"><h2 id=\"abstract\" class=\"section-heading-2\">ABSTRACT<div id=\"mathJaxToggle\" class=\"hideElement\"><label for=\"mathJaxToggle\"><span>Formulae display:</span><input type=\"checkbox\" id=\"mathJaxToggleCheck\" name=\"mathJaxToggleCheck\" /></label><span class=\"mathJaxLogo\"><img src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/templates/jsp/_style2/_tandf/pb2/images/math-jax.gif\"}' alt=\"MathJax Logo\" /><span class=\"qMrk\">?</span><span class=\"auPopUp hideElement\"><span class=\"pointyEdge\"></span>Mathematical formulae have been encoded as MathML and are displayed in this HTML version using MathJax in order to improve their display. Uncheck the box to turn MathJax off. This feature requires Javascript. Click on a formula to zoom.</span></span></div><span class=\"math-settings hidden-lg\"></span></h2></div><div class=\"abstractSection abstractInFull\"><p class=\"summary-title\"><b>ABSTRACT</b></p><p>We conduct two studies to evaluate the suitability of artificially generated facial pictures for use in a customer-facing system using data-driven personas. STUDY 1 investigates the quality of a sample of 1,000 artificially generated facial pictures. Obtaining 6,812 crowd judgments, we find that 90% of the images are rated medium quality or better. STUDY 2 examines the application of artificially generated facial pictures in data-driven personas using an experimental setting where the high-quality pictures are implemented in persona profiles. Based on 496 participants using 4 persona treatments (2\u2009\u00d7\u20092 research design), findings of Bayesian analysis show that using the artificial pictures in persona profiles did not decrease the scores for Authenticity, Clarity, Empathy, and Willingness to Use of the data-driven personas.</p></div></div><div class=\"abstractKeywords\"><div class=\"hlFld-KeywordText\"><div><p class=\"kwd-title\" aria-label=\"Keywords\">KEYWORDS: </p><a href=\"/keyword/Evaluation\" class=\"kwd-btn keyword-click\" role=\"button\">Evaluation</a><a href=\"/keyword/Human%E2%80%93computer+Interaction\" class=\"kwd-btn keyword-click\" role=\"button\">human\u2013computer interaction</a><a href=\"/keyword/User+Behaviour\" class=\"kwd-btn keyword-click\" role=\"button\">user behaviour</a><a href=\"/keyword/Human+Factors\" class=\"kwd-btn keyword-click\" role=\"button\">human factors</a><a href=\"/keyword/Artificially+Generated+Facial+Pictures\" class=\"kwd-btn keyword-click\" role=\"button\">artificially generated facial pictures</a></div></div></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneAbs\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div></div><div class=\"hlFld-Fulltext\"><div id=\"S001\" class=\"NLM_sec NLM_sec-type_intro NLM_sec_level_1\"><h2 id=\"_i3\" class=\"section-heading-2\">1. Introduction</h2><p>There is tremendous research interest concerning artificial image generation (AIG). The state-of-the-art studies in this field use <i>Generative Adversarial Networks</i> (GANs) (Goodfellow et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0037\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>) and Conditional GANs (Lu, Tai, and Tang <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0064\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) to generate images that are promised to be photorealistic and easily deployable. GANs have been applied, for example, to automatically create art (Tan et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0099\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), cartoons (Liu et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0061\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), medical images (Nie et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0068\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), and facial pictures (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), the latter including transformations such as increasing/decreasing a person's age or altering their gender (Antipov, Baccouche, and Dugelay <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0006\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Choi et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0020\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Isola et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0048\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>).</p><p>Due its low cost, AIG provides novel opportunities for a wide range of applications, including health-care (Nie et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0068\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), advertising (Neumann, Pyromallis, and Alexander <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0067\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), and user analytics for human computer interaction (HCI) and design purposes (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). However, despite the far-reaching interest in AIG among academia and across industries, there is scant research on <i>evaluating the suitability of the generated images for practical use in deployed systems</i>. This means that the quality and impact of the artificial images on user perceptions are often neglected, lacking user studies of their deployment in real systems. This area of evaluation is an overlooked but critical area of research, as it is the \u2018final step\u2019 of deployment that actually determines if the quality of the AIG is good enough, as prior work has shown the impact that pictures can have on real systems (King, Lazard, and White <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0056\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>). Therefore, the impact of AIG on user experience (UX) and design applications is a largely unaddressed field of study, although with work in related areas of empathy (Weiss and Cohen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0100\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). For example, Weiss and Cohen (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0100\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) that aspects of empathy with subjects in videos is complex in terms of encouraging or discouraging engagement with the content.</p><p>Most typically, artificial pictures are evaluated using <i>technical metrics</i> (Yuan et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0104\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>) that are abstract and do not reflect user perceptions or UX. An example is the <i>Fr\u00e8chet inception distance</i> (FID) (Heusel et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0041\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) that measures the similarity of two image distributions (i.e. the generated set and the training set). While metrics such as FID are without question necessary for measuring the <i>technical quality</i> of the generated images (Zhao et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0107\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>), we argue there is also a substantial need for evaluating the <i>user experience</i> of the pictures for real-world systems and applications.</p><p>In this regard, the user study tradition from HCI is helpful \u2013 in addition to technical metrics, <i>user-centric metrics</i> gauging UX and user perceptions (Ashraf, Jaafar, and Sulaiman <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0008\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Brauner et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0015\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) can be deployed. The potential impact of AIG is transformational, including domains of public relations, marketing, advertising, ecommerce sites, retail brochures, chatbots, virtual agents, design, and others. The use of artificially generated facial images is generally free of copyright restrictions and can allow for a wide range of demographic diversity (age, gender, ethnicity). Nonetheless, these benefits only hold if the pictures are \u2018good enough\u2019 for real applications. Given the multiple application areas of AIG, the results of an evaluation study measuring the impact of artificial facial pictures on UX is of immediate interest for researchers and practitioners alike.</p><p>To address the call for user studies concerning AIG, we carry out two evaluation studies: (a) one addressing the overall <i>perceived quality</i> of artificial pictures among crowd workers, and (b) another addressing user perceptions when implementing the pictures for data-driven personas (DDPs). Our research question is: <i>Are artificially generated facial pictures \u2018good enough\u2019 for a system requiring substantial images of people?</i></p><p>DDPs are personas imaginary people representing real user segments, as defined traditionally in HCI (Cooper <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0023\" data-refLink=\"_i41 _i42\" href=\"#\">2004</a></span>) created from social media and Web analytics data (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0004\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>, <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>). Although the DDP process may vary system to system, most will have the six major steps shown in <a href=\"#F0001\">Figure 1</a>. <div class=\"figure figureViewer\" id=\"F0001\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 1. </span> Data-driven persona development approach. Six-step process common for most DDP methods.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0001image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0001_ob.jpg\"}' width=\"500\" height=\"37\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0001\"><p><span class=\"captionLabel\">Figure 1. </span> Data-driven persona development approach. Six-step process common for most DDP methods.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0001\"></div></p><p>The advantage of DDPs, relative to traditional personas (Brangier and Bornet <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0014\" data-refLink=\"_i41 _i42\" href=\"#\">2011</a></span>) that are manually created and typically include 3\u20137 personas per set (Hong et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0044\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), is that one can create hundreds of DDPs from the data to reflect different behavioural and demographic nuances in the underlying user population (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0082\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>). For example, a news organisation distributing its contents in social media platforms to audiences originating from dozens of countries can have dozens of audience segments relevant for different decision-making scenarios in other geographic areas (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0093\" data-refLink=\"_i41 _i42\" href=\"#\">2018e</a></span>). DDPs summarise these segments into easily approachable human profiles (see <a href=\"#F0002\">Figure 2</a> for example) that can be used within the organisation to understand the persona's needs (Nielsen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0069\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) and communicate (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0090\" data-refLink=\"_i41 _i42\" href=\"#\">2018d</a></span>) about these needs as a part of user-centric decision making (Idoughi, Seffah, and Kolski <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0046\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). <div class=\"figure figureViewer\" id=\"F0002\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 2. </span> Example of DDP. The persona has a picture (stock photo in this example), name, age, text description, topics of interest, quotes, most viewed contents, and audience size. The picture is purchased and downloaded manually from an online photobank; the practical goal of this research is to replace manual photo curation through automatic image generation.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0002image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0002_oc.jpg\"}' width=\"500\" height=\"428\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0002\"><p><span class=\"captionLabel\">Figure 2. </span> Example of DDP. The persona has a picture (stock photo in this example), name, age, text description, topics of interest, quotes, most viewed contents, and audience size. The picture is purchased and downloaded manually from an online photobank; the practical goal of this research is to replace manual photo curation through automatic image generation.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0002\"></div></p><p>One of the important issues for automatically creating DDPs from data is the availability of persona pictures \u2013 since DDP systems can create dozens of personas in near real time, there is a need for an inventory of pictures to use when rendering the personas for end users to view and interact with. Conceptually, this leads to a need for an AIG module that creates suitable persona pictures on demand (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). However, prior to engaging in system development, there is a need for ensuring that artificially generated pictures are not detrimental to user perceptions of the personas, or otherwise, one risks futile efforts with immature technology. In a sense, therefore, the question of picture quality for DDPs is also a question of feasibility study (of implementation).</p><p>Note that pictures constitute an essential element of the persona profile (Baxter, Courage, and Caine <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0012\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>; Nielsen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0070\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>). Pictures are instrumental for the persona to appear believable, and they have been found impactful for central persona perceptions, such as empathy (Probster, Haque, and Marsden <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0076\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>). Therefore, DDPs require these pictures in order to realise the many benefits associated with the use of personas in the HCI literature (Long <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0063\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>; Nielsen and Storgaard Hansen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0072\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>).</p><p>To evaluate the <i>quality</i>, we first generate a sample of 1,000 artificial facial pictures using a state-of-the-art generator. To evaluate this sample, we then obtain 6,812 judgments from crowdworkers. To evaluate <i>user perceptions</i>, we conduct a 2\u2009\u00d7\u20092 experiment with DDPs with a real/artificial picture. For measurement of user perceptions, we deploy the Persona Perception Scale (PPS) instrument (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>) to gauge the impact of artificial pictures on the DDPs\u2019 <i>authenticity</i> and <i>clarity</i>, as well as the sense of <i>empathy</i>, and <i>willingness to use</i> among the online pool of respondents.</p><p>Thus, our research goal is to evaluate artificially generated pictures across multiple dimensions for deployment in DDPs. Note that our goal is <i>not</i> to make a technical AIG contribution. Rather, we apply a pre-existing method for persona profiles and then evaluate the results for user perceptions. So, our contribution is in the area of practical design and implementation of AIG.</p><p>Note also that even though we focus on DDPs in this research, many other domains and use cases have similar needs in terms of requiring large collections of diverse facial images, including HCI and human-robot interaction such as avatars (Ablanedo et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0001\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; \u015eeng\u00fcn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0094\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>; Seng\u00fcn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0095\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>), robots (dos Santos et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0028\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>; Duffy <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0029\" data-refLink=\"_i41 _i42\" href=\"#\">2003</a></span>; Edwards et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0030\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>; Holz, Dragone, and O\u2019Hare <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0043\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>), and chatbots (Araujo <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0007\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Go and Shyam Sundar <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0036\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Shmueli-Scheuer et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0097\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0108\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). Thus, our evaluation study has a cross-sectional value for other design purposes where artificial facial pictures would be useful.</p></div><div id=\"S002\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i6\" class=\"section-heading-2\">2. Related literature</h2><div id=\"S002-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i7\">2.1. Lack of evaluation studies for artificial pictures</h3><p>To quantify the need for evaluation studies of AIG in real systems, we carried out a scoping review (Bazzano et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0013\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) by extracting information from 20 research articles that generate artificial facial pictures. The articles were retrieved via Google Scholar using relevant search phrases (\u2018automatic image generation\u2009+\u2009faces\u2019, \u2018facial image creation\u2019, \u2018artificial picture generation\u2009+\u2009face\u2019, etc.) and focusing on peer-reviewed conference/journal articles published between 2015 and 2019. The list of articles, along with the extracted evaluation methods, is provided in <i>Supplementary Material</i>.</p><p>Results show that evaluation methods in these articles almost always contain one or more technical metrics (90%, <i>N</i>\u2009=\u200918) and always a short, subjective evaluation by the authors (100%, <i>N</i>\u2009=\u200920), in the line of \u2018manual inspection revealed some errors but generally good quality\u2019 (not an actual quote). Among the 20 articles, less than half (45%, <i>N</i>\u2009=\u20099) measured actual human perceptions (typically using crowdsourced ratings). More importantly, none of the articles provided an evaluation study that would implement the generated pictures into a real system or application. The results of this scoping review thus show a general lack of user studies for practical evaluation of AIG in real systems or use cases (0% of the research we could locate did so).</p><p>As stated, the evaluation of AIG focuses on technical metrics (Gao et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0034\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>) of image generation (e.g. inception score (Dey et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0024\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Di and Patel <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0025\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Salimans et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0080\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>; Yin et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0103\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), FID (Dey et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0024\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Lin et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0060\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), Euclidean distance (Gecer et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0035\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), cosine similarity (Dey et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0024\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), reconstruction errors (Chen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0019\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Lee et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0058\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), or accuracy of face recognition (Di, Sindagi, and Patel <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0026\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Liu et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0062\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>)). Many of the technical metrics are said to have various strengths and weaknesses (Barratt and Sharma <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0010\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Shmelkov, Schmid, and Alahari <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0096\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Zhang et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0106\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>). The main weakness is that they do not capture user perceptions or UX ramifications of the pictures in real applications. This is because the technical metrics are not directly related to end-user experience when the user is observing the pictures within the <i>context</i> of their intended use (e.g. as part of DDPs).</p><p>Human evaluation studies, on the other hand, tend to focus on comparing the outputs of different algorithms, again ignoring the importance of context on the evaluation results. Typically, participants are asked to rank pictures produced using different algorithms from best to worst (Li et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0059\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Liu et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0062\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0109\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>) or rate the pictures by user perception metrics, such as realism, overall quality, and identity (Yin et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0103\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0109\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>). For example, Li et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0059\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) recruited 84 volunteers to rank three generated images out of 10 non-makeup and 20 makeup test images based on quality, realism, and makeup style similarity. Lee et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0058\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) employed a similar approach by asking users which image is more realistic out of samples created using different generation methods. Similarly, Choi et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0020\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) asked crowd workers in Amazon Mechanical Turk (AMT) to rank the generated images based on realism, quality of attribute transfer (hair colour, gender, or age), and preservation of the person's original identity. The participants were shown four images at a time, generated using different methods. Zhang et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0106\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) conducted a two-alternative forced choice (2AFC) test by asking AMT participants which of the provided pictures is more similar to a reference picture.</p><p>On rarer instances, user perception metrics, such as realism, overall quality, and identity have been deployed. For example, Zhou et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0108\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) evaluated the quality of their generated results by asking if the participants consider the generated faces as realistic (\u2018yes\u2019 or \u2018no\u2019). In their study, 88.4% of the pictures were considered realistic. Iizuka, Simo-Serra, and Ishikawa (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0047\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) recruited ten volunteers to evaluate the \u2018naturalness\u2019 of the generated pictures; the volunteers were asked to guess if a picture was real or generated. Overall, 77% of the generated pictures were deemed to be real. Yin et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0103\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) asked students to compare 100 generated pictures with original pictures along with three criteria: (1) <i>saliency</i> (the degree of the attributes that has been changed in the picture), (2) <i>quality</i> (the overall quality of the picture), and (3) <i>identity</i> (if the generated and the original picture are the same person). Their AIG method achieved an average quality rating of 4.20 out of 5. While these studies are closer to the realm of UX, we could not locate previous research that would (a) <i>investigate the effect of artificial pictures on UX of a real system</i>, or (b) <i>evaluate the impact of using artificially generated pictures on user perceptions</i>. However, evaluating AIG approaches for user perceptions and UX in real systems, is crucial for determining the success of AIG in real usage contexts for design, HCI, and various other areas of application (\u00d6zmen and Yucel <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0073\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>).</p></div><div id=\"S002-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i8\">2.2. Data-driven persona development</h3><p>A persona is a fictive person that describes a user or customer segment (Cooper <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0022\" data-refLink=\"_i41 _i42\" href=\"#\">1999</a></span>). Originating from HCI, personas are used in various domains, such as user experience/design (Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>), marketing (Jenkinson <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0052\" data-refLink=\"_i41 _i42\" href=\"#\">1994</a></span>), and online analytics (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0082\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>) to increase the empathy by designers, software developers, marketers (etc.) toward the users or customers of a product (Dong, Kelkar, and Braun <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0027\" data-refLink=\"_i41 _i42\" href=\"#\">2007</a></span>). Personas make it possible for decision makers to see use cases \u2018through the eyes of the user\u2019 (Goodwin <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0038\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>) and facilitate communication between team members through shared mental models (Pruitt and Adlin <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0077\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>). Researchers are increasingly developing methodologies for DDPs (McGinn and Kotamraju <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0066\" data-refLink=\"_i41 _i42\" href=\"#\">2008</a></span>; Zhang, Brown, and Shankar <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0105\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>) and automatic persona generation (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0004\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>; An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>), mainly due to the increase in the availability of online user data and to increase the robustness of personas given the alternative forms of user understanding (Jansen, Salminen, and Jung <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0049\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>). DDPs typically leverage quantitative social media and online analytics data to create personas that represent users or customers of a specific channel<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0001\" href=\"#\" data-refLink=\"fn\"><sup>1</sup></a></span> (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0092\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>). Regarding the development of DDPs, for the generated pictures to be useful for personas, they need to be \u2018taken for real\u2019, meaning that they do not hinder the user perceptions of the personas (e.g. not reduce the persona's authenticity).</p></div><div id=\"S002-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i9\">2.3. Persona user perceptions</h3><p>Evaluation of user perceptions has been noted as a major concern of personas. Scholars have observed that personas need justification, mainly for their accuracy and usefulness in real organisations and usage scenarios (Chapman and Milham <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0018\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>; Friess <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0033\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>; Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). Prior research typically examines persona user perceptions via case studies (Faily and Flechais <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0032\" data-refLink=\"_i41 _i42\" href=\"#\">2011</a></span>; Jansen, Van Mechelen, and Slegers <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0050\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Nielsen and Storgaard Hansen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0072\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>), ethnography (Friess <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0033\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>), usability standards (Long <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0063\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>), or using statistical evaluation (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>; Brickey, Walczak, and Burgess <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0016\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>; Zhang, Brown, and Shankar <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0105\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>). For example, Friess (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0033\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>) investigated the adoption of personas among designers. Long (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0063\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>) measured the effectiveness of using personas as a design tool, using Nielsen's usability heuristics. Nielsen et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0071\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) analyze the match between journalists\u2019 preconceptions and personas created from the audience data, whereas Chapman et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0017\" data-refLink=\"_i41 _i42\" href=\"#\">2008</a></span>) evaluate personas as quantitative information. While these evaluation approaches are interesting, survey methods provide a lucrative alternative for understanding how end users perceive personas. Survey research typically measures perceptions as latent constructs, apt for measurement of attitudes and perceptions that cannot be directly observed (Barrett <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0011\" data-refLink=\"_i41 _i42\" href=\"#\">2007</a></span>). This approach seems intuitively compatible with personas, as researchers have reported several attitudinal perceptions concerning personas (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0085\" data-refLink=\"_i41 _i42\" href=\"#\">2019c</a></span>). These are captured in the PPS survey instrument (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0088\" data-refLink=\"_i41 _i42\" href=\"#\">2019f</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0091\" data-refLink=\"_i41 _i42\" href=\"#\">2019g</a></span>) that includes eight constructs and twenty-eight items to measure user perceptions of personas. We deploy this instrument in this research, as it covers essential user perceptions in the persona context.</p></div><div id=\"S002-S2004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i10\">2.4. Hypotheses</h3><p>Following prior persona research, we formulate the following hypotheses to test persona user perceptions. <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">H01: Using artificial pictures does not decrease the authenticity of the persona. HCI research has shown that authenticity (or <i>credibility</i>, <i>believability</i>) is a crucial issue for persona acceptance in real organisations \u2014 if the personas come across as \u2018fake\u2019, decision makers are unlikely to adopt them for use (Chapman and Milham <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0018\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>; Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). This is especially relevant for our context because personas already are fictitious people describing real user groups (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>), so we need to ensure that enhancing these fictitious people with artificially generated pictures does not further risk the perception of realism.</p></li><li><p class=\"inline\">H02: Using artificial pictures does not decrease the clarity of the persona profile. For personas to be useful, they should not be abstract or misleading (Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). HCI researchers have found that personas with inconsistent information make end users of personas confused (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0090\" data-refLink=\"_i41 _i42\" href=\"#\">2018d</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0084\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>). Again, we need to ensure that artificial pictures do not make persona profiles more \u2018messy\u2019 or unclear for the end users.</p></li><li><p class=\"inline\">H03: Using artificial pictures does not decrease empathy towards the persona. Empathy is considered, among HCI scholars, as a key advantage of personas compared to other forms of presenting user data (Cooper <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0022\" data-refLink=\"_i41 _i42\" href=\"#\">1999</a></span>; Nielsen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0069\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). The generated personas need to \u2018resonate\u2019 with end users to make a real impact. Therefore, to be successful, artificial pictures should not reduce the sense of empathy towards the persona.</p></li><li><p class=\"inline\">H04: Using artificial pictures does not decrease the willingness to use the persona. Willingness to use (WTU) is a crucial construct for the adoption of personas for practical decision making (R\u00f6nkk\u00f6 <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0078\" data-refLink=\"_i41 _i42\" href=\"#\">2005</a></span>; R\u00f6nkk\u00f6 et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0079\" data-refLink=\"_i41 _i42\" href=\"#\">2004</a></span>). HCI research has shown that if persona users do not show a willingness to learn more about the persona for their task at hand, persona creation risks remaining a futile exercise (R\u00f6nkk\u00f6 et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0079\" data-refLink=\"_i41 _i42\" href=\"#\">2004</a></span>).</p></li></ul></p><p>Overall, ranking high on these perceptions is considered positive (desirable) within the HCI literature. This leads to defining the \u2018good enough\u2019 quality of artificial pictures in the DDP context such that <i>a \u2018good enough\u2019 picture quality does not decrease (a) the authenticity (i.e. the persona is still considered as \u2018real\u2019 as with real photographs), (b) clarity of the persona profile, (c) the sense of empathy felt toward the persona, or (d) the willingness to learn more about the persona.</i> In other words, it is the design goal of replacing real photographs with artificial pictures in the context of personas, with the concept being transferrable to other domains.</p></div></div><div id=\"S003\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i11\" class=\"section-heading-2\">3. Methodology</h2><div id=\"S003-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i12\">3.1. Overview of evaluation steps</h3><p>Our evaluation of picture quality consists of two separate studies: (1) crowdsourced evaluation study of AIG quality, and (2) user study measuring the perceptions of an online panel concerning personas with artificially generated pictures. The latter study tests if DDPs are perceived differently when using artificial pictures, while addressing the hypotheses presented in the previous section.</p></div><div id=\"S003-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i13\">3.2. Research context</h3><p>Our research context is a DDP system: <i>Automatic Persona Generation</i> (APG<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0002\" href=\"#\" data-refLink=\"fn\"><sup>2</sup></a></span>). As a DDP system, APG requires thousands of realistic facial pictures to produce a wide range of believable persona profiles for client organisations (Pruitt and Adlin <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0077\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>) covering a wide range of ages and ethnicities. An overview of the typical DDP development process is presented in <a href=\"#F0003\">Figure 3</a>. <div class=\"figure figureViewer\" id=\"F0003\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 3. </span> APG data and processing flowchart from server configuration to data collection and persona generation.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0003image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0003_oc.jpg\"}' width=\"500\" height=\"172\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0003\"><p><span class=\"captionLabel\">Figure 3. </span> APG data and processing flowchart from server configuration to data collection and persona generation.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0003\"></div></p><p>A practical limitation of APG is the need for manually acquiring facial pictures for the persona profiles (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). Because the pictures for APG are acquired from online stock photo banks (e.g. iStockPhoto, 123rf.com, etc.), manual effort is required to curate a large number of pictures. A large number of pictures is needed because APG can generate thousands of personas for client organisations \u2013 for each persona, a unique facial picture is required. Organisations over a lengthy period can have dozens of unique personas. Using stock photo banks also involves a financial cost (ranging from $1 to $20 USD per picture), making picture curation both time-consuming and costly. Given the goal of fully automated persona generation (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>), there is a practical need for automatic image generation.</p><p>Thus, we evaluate the automatically generated facial pictures for use in APG (Jung et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0053\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>; Jung et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0054\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>). APG generates personas from online analytics and social media data (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0086\" data-refLink=\"_i41 _i42\" href=\"#\">2019d</a></span>). <a href=\"#F0002\">Figure 2</a> shows an example of a persona generated using the system. The practical purpose of automatically generated images is to replace the manual curation of persona profile pictures, saving time and money. Note that the cost and effort are not unique problems of APG, but generalise to all similar images systems, as the pictures need to be provided for each new persona generated.</p></div><div id=\"S003-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i15\">3.3. Deploying StyleGAN for persona pictures</h3><p>For AIG, we utilise a pre-trained version of <i>StyleGAN</i> (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), a state-of-the-art generator that represents a leap towards photorealistic facial pictures and can be freely accessed on GitHub.<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0003\" href=\"#\" data-refLink=\"fn\"><sup>3</sup></a></span> StyleGAN was chosen for this research because (a) it is a leap toward generating photorealistic facial images, especially relative to the previous state-of-art, (b) the trained model is publicly available, and (c) its deployment is robust for possible use in real systems. StyleGAN generated the images, so this is a back end process.</p><p>We use a pretrained model from the creators of StyleGAN (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). This model was trained on CelebA-HQ and FFHQ datasets using eight Tesla V100\u2005GPUs. It is implemented in TensorFlow,<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0004\" href=\"#\" data-refLink=\"fn\"><sup>4</sup></a></span> an open-source machine learning library and is available in a GitHub repository.<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0005\" href=\"#\" data-refLink=\"fn\"><sup>5</sup></a></span> We access this pre-trained model via the GitHub repository that contains the model and the required source code to run it.</p><p>Our goal is to use this pre-trained model to generate a sample of 1,000 realistic facial pictures. The method of applying the published code to generate the pictures is straightforward. We provide the exact steps below to facilitate replication studies: <div class=\"quote\"><p>\u2022 <b>Step 1:</b> Import the required Python packages (<i>os</i>, <i>pickle</i>, <i>numpy</i>, from <i>PIL</i>: <i>Image</i>, <i>dnnlib</i>).</p></div> <div class=\"quote\"><p>\u2022 <b>Step 2:</b> Define the parameters and paths</p></div> <div class=\"quote\"><p>\u2022 <b>Step 3:</b> Initialize the environment and load the pretrained StyleGAN model.</p></div> <div class=\"quote\"><p>\u2022 <b>Step 4:</b> Set random states and generate new random input. Randomization is needed because the model always generates the same face for a particular input vector. To generate unique images, a unique set of input arrays should be provided. This is done by setting a random state equal to the current number of iterations, which allows us to have unique images and reproducible results at the same time.</p></div> <div class=\"quote\"><p>\u2022 <b>Step 5:</b> Generate images using the random input array created in the previous step.</p></div> <div class=\"quote\"><p>\u2022 <b>Steps 6:</b> Save the generated images as files to the output folder. We use the resolution of 1024\u2009\u00d7\u20091024 pixels. Other available resolutions are 512\u2009\u00d7\u2009512\u2005px and 256\u2009\u00d7\u2009256\u2005px.</p></div></p><p>The above steps with the mentioned parameters enable us to generate artificial pictures with similar quality to those in the StyleGAN research paper (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). For replicability, we are sharing the Python code we used for AIG in <i>Supplementary Material</i>.</p></div></div><div id=\"S004\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i16\" class=\"section-heading-2\">4. STUDY 1: crowdsourced evaluation</h2><div id=\"S004-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i17\">4.1. Method</h3><p>We evaluate the human-perceived quality of 1,000 generated facial pictures. To facilitate comparison with prior work using human evaluation for artificial pictures (Choi et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0020\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Song et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0098\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Zhang et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0106\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), we opt for crowdsourcing, using Figure Eight to collect the ratings. This platform has been widely used for gathering manually annotated training data (Alam, Ofli, and Imran <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0002\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) and ratings (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0081\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>) in various subdomains of computer science. The pictures were shown in the full 1024\u2009\u00d7\u20091024 pixels format to provide the crowd raters enough detail for a valid evaluation. The following task description was provided to the crowd raters, including the quality criteria and examples:</p><p><i>You are shown a facial picture of a person. Look at the picture and choose how well it represents a real person. The options:</i> <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">5: Perfect\u2014the picture is indistinguishable from a real person.</p></li><li><p class=\"inline\">4: High quality\u2014the picture has minor defects, but overall it's pretty close to a real person.</p></li><li><p class=\"inline\">3: Medium quality\u2014the picture has some flaws that suggest it's not a real person.</p></li><li><p class=\"inline\">2: Low quality\u2014the picture has severe malformations or defects that instantly show it's a fake picture.</p></li><li><p class=\"inline\">1: Unusable\u2014the picture does not represent a person at all.</p></li></ul></p><p>We also clarified to the participants that the use case is to find realistic pictures specifically for persona profiles, explaining that these are descriptive people of some user segment. Additionally, we indicated in the title that the task is to evaluate <i>artificial</i> pictures of people, to manage the expectations of the crowd raters accordingly (Pitk\u00e4nen and Salminen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0075\" data-refLink=\"_i41 _i42\" href=\"#\">2013</a></span>). Other than the persona aspect, these are similar to guidelines used in prior work to facilitate image comparisons.</p><p>Following the quality control guidelines for crowdsourcing by Huang, Weber, and Vieweg (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0045\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>) and Alonso (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0003\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>), we implemented suitable parameters in the Figure Eight platform. We also enabled <i>Dynamic judgments</i>, meaning the platform automatically collects more ratings when there is a higher disagreement among the raters. Based on the results of a pilot study with 100 pictures, not used in the final research, we set the maximum number of ratings to 5 and confidence goal to 0.65. The default number of raters was three, so the platform only went to 5 raters if a 0.65 confidence was not achieved.<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0006\" href=\"#\" data-refLink=\"fn\"><sup>6</sup></a></span></p></div><div id=\"S004-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i18\">4.2. Results</h3><p>We spent $266.98 USD to obtain 6,812 crowdsourced image ratings. This was the number of evaluations from trusted contributors, not including the test questions. Note that if the accuracy of a crowd rater's ratings relative to the test questions falls below the minimum accuracy threshold (in our case, 80%), the rater is disqualified, and the evaluations become untrusted. There were 423 untrusted judgments (6% of the total submitted ratings), i.e. ratings coming from contributors that continuously fail to correctly rate the test pictures. Thus, 94% of the total ratings were deemed trustworthy. The majority label for each rated picture is assigned by comparing the confidence-adjusted ratings of each available class, calculated as follows: <span class=\"NLM_disp-formula-image disp-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_um0001.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_um0001.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula disp-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>Confidenc</mtext></mstyle><msub><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>e</mtext></mstyle><mrow><mrow><mi mathvariant=\"normal\">class</mi></mrow></mrow></msub><mo>=</mo><mstyle displaystyle=\"true\" scriptlevel=\"0\"><mrow><mfrac><mrow><msubsup><mrow><mo movablelimits=\"false\">\u2211</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>\u2061</mo><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>trus</mtext></mstyle><msub><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>t</mtext></mstyle><mrow><mrow><mi mathvariant=\"normal\">class</mi></mrow></mrow></msub></mrow><mrow><msubsup><mrow><mo movablelimits=\"false\">\u2211</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>\u2061</mo><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>trus</mtext></mstyle><msub><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>t</mtext></mstyle><mrow><mrow><mi mathvariant=\"normal\">all</mi></mrow></mrow></msub></mrow></mfrac></mrow><mo>,</mo><mspace width=\"thickmathspace\"></mspace></mstyle></math></span>where the confidence score of the class is given by the sum of the trust scores from all <i>n</i> raters of that picture. The trust score is based on a crowdworker's historical accuracy (relative to test questions) on all the jobs he/she has participated in. For example, if the confidence score of \u2018perfect\u2019 is 0.66 and \u2018medium quality\u2019 is 0.72, then the chosen majority label is \u2018medium quality\u2019 (0.72\u2009&gt;\u20090.66).</p><p>The results (see <a class=\"ref showTableEventRef\" data-ID=\"T0001\">Table 1</a>) show \u2018High quality\u2019 as the most frequent class. Sixty percent (60%) of the generated pictures are rated as either \u2018Perfect\u2019 or \u2018High quality\u2019. The average quality score was 3.7 out of 5 (SD\u2009=\u20090.91) when calculated from majority votes and 3.8 when calculated from all the ratings. 9.9% of the pictures were rated as \u2018Low quality\u2019, and none was rated as \u2018Unusable\u2019. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 1. The results of crowd evaluation based on a majority vote of the picture quality. Most frequent class bolded. Example facial image from each of the 5 classes shown for comparison.</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"false\" id=\"T0001-table-wrapper\"><a data-id=\"T0001\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p></div><div id=\"S004-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i21\">4.3. Reliability analysis</h3><p>To assess the reliability of the crowd ratings, we measured the interrater agreement of the quality ratings among crowdworkers. For this, we used two metrics: <i>Gwet's AC1</i> (AC1) and <i>percentage agreement</i> (PA). Using AC1 is appropriate when the outcome is ordinal, the number of ratings varies across items (Gwet <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0039\" data-refLink=\"_i41 _i42\" href=\"#\">2008</a></span>) and where the Kappa metric is low despite a high level of agreement (Banerjee et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0009\" data-refLink=\"_i41 _i42\" href=\"#\">1999</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0081\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>). Because of these properties, we chose AC1 with ordinal weights as the interrater agreement metric. In addition, PA was calculated as a simple baseline measure. Standard errors were used to construct the 95% confidence interval (CI) for AC1. For PA, 95% CI was calculated using 100 bootstrapped samples.</p><p>Results (see <a class=\"ref showTableEventRef\" data-ID=\"T0002\">Table 2</a>) show a high PA agreement (86.2%). The interrater reliability was 0.627, in the range of <i>good</i> (i.e. 0.6\u22120.8) (Wongpakaran et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0101\" data-refLink=\"_i41 _i42\" href=\"#\">2013</a></span>). The results were statistically significant (<i>p</i>\u2009&lt;\u20090.001), with the probability of observing such results by chance is less than 0.1%. Therefore, the crowd ratings can be considered to have satisfactory internal validity. However, the quality of some pictures is more easily agreed upon than others. When stratified, the overall agreement and AC1 were similar across <i>low</i>, <i>moderate</i>, and <i>high</i> quality labels (PA \u223c 85%, and AC1 \u223c 0.75). However, the agreement was lower when the picture was rated perfect (PA\u2009=\u200976.7%, AC1\u2009=\u20090.498). This implies that \u2018perfect\u2019 is more difficult to determine than the other rating labels. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 2. Agreement metrics for the crowdsourced ratings showing satisfactory internal validity.</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"T0002-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=T0002&amp;doi=10.1080%2F0144929X.2020.1838610&amp;downloadType=CSV\">CSV</a><a data-id=\"T0002\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p></div></div><div id=\"S005\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i23\" class=\"section-heading-2\">5. STUDY 2: effects on persona perceptions</h2><div id=\"S005-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i24\">5.1. Experiment design</h3><p>We created two base personas using the APG (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>) methodology described previously; one male and one female. We leave the evaluation of other genders for future research. The experiment variable is the use of an artificial image in the persona profile. The other elements of the persona profiles are identical between the two treatments. For this, we manipulated the base personas by introducing either (a) <i>a real photograph of a person</i> or (b) a <i>demographically matching artificial picture</i> (see <a href=\"#F0004\">Figure 4</a>). <div class=\"figure figureViewer\" id=\"F0004\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 4. </span> Artificial male picture [A], Real male picture [B], Artificial female picture [C], and Real female picture [D]. Among the male/female personas, all other content in the persona profile was the same except the picture that alternated between Artificial and Real. Pictures of the full persona profiles are provided in Supplementary Material.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0004image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0004_oc.jpg\"}' width=\"500\" height=\"163\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0004\"><p><span class=\"captionLabel\">Figure 4. </span> Artificial male picture [A], Real male picture [B], Artificial female picture [C], and Real female picture [D]. Among the male/female personas, all other content in the persona profile was the same except the picture that alternated between Artificial and Real. Pictures of the full persona profiles are provided in Supplementary Material.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0004\"></div></p><p>The demographic match was determined manually by two researchers who judged that the chosen pictures were similar for gender, age, and race. Using a modified Delphi method, a seed image of either a real or article picture was select using the meta-data attributes of gender, age, and race. The researchers independently selected matching images for each. The two researchers then jointed selected the mutually agreed upon image for the treatments. The artificial pictures were chosen from the ones rated \u2018perfect\u2019 by the crowd raters. The real photos were sourced from online repositories, with Creative Commons license.</p><p>In total, four persona treatments were created: <i>Male Persona with Real Picture</i> (MPR), <i>Male Persona with Artificial Picture</i> (MPA), <i>Female Persona with Real Picture</i> (FPR), and <i>Female Persona with Artificial Picture</i> (FPA). The created personas were mixed into four sequences: <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\"><b>Sequence 1:</b> MPR \u2192 FPA</p></li><li><p class=\"inline\"><b>Sequence 2:</b> MPA \u2192 FPR</p></li><li><p class=\"inline\"><b>Sequence 3:</b> FPR \u2192 MPA</p></li><li><p class=\"inline\"><b>Sequence 4:</b> FPA \u2192 MPR</p></li></ul></p><p>Each participant was randomly assigned to one of the sequences. To counterbalance the dataset, we ensured an even number of participants (<i>N</i>\u2009=\u2009520/4\u2009=\u2009130) for each sequence. Technically, the participants self-selected the sequence, as each participant could only take one survey. The participants were excluded from answering in more than one survey based on their (anonymous) Respondent ID. The gender distribution for each of the four sequences, as shown: S1 (<i>M</i>: 41.5% <i>F</i>: 58.5%), S2 (<i>M</i>: 39.0% <i>F</i>: 61.0%), S3 (<i>M</i>: 39.8% <i>F</i>:60.2%), S4 (<i>M</i>: 34.5% <i>F</i>: 64.5%).</p></div><div id=\"S005-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i26\">5.2. Recruitment of participants</h3><p>We created a survey for each sequence. In each survey, we (a) explain to participants what the research is about and what personas are (\u2018<i>a persona is defined as a fictive person describing a specific customer group</i>\u2019). Then, we (b) show an example persona with explanations of the content, and (c) explain the task scenario (\u2018<i>Imagine that you are creating a YouTube video for the target group that the persona you will be shown next describes</i>\u2019.). After this, (d) the participants are shown one of the four treatments, asked to review the information carefully, and complete the PPS questionnaire.</p><p>In total, 520 participants were recruited using Prolific, an online survey platform often applied in social science research (Palan and Schitter <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0074\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>). Prolific was chosen for this evaluation step (as opposed to previously used Figure Eight), as it provides background information of the participants (e.g. gender, age) that can be deployed for further analyses. The average age of the participants was 35 years old (SD\u2009=\u20097.2), with 59.1% being female, overall. The nationality of the participants was the United Kingdom, they had at least an undergraduate degree, and none were students. We verified the quality of the answers using an attention check question (\u2018<i>It's important that you pay attention to this study. Please select \u201cSlightly agree\u201d\u2019</i>.). Out of 520 answers, 19 (3.7%) failed the attention check; these answers were removed. In addition, five answers were timed out by the Prolific platform. Therefore, we ended up with 496 qualified participants (95.4% of the total).</p></div><div id=\"S005-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i27\">5.3. Measurement</h3><p>The perceptions are measured using the PPS (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>), a survey instrument measuring what individuals think about specific personas (see <a class=\"ref showTableEventRef\" data-ID=\"T0003\">Table 3</a>). The PPS has previously deployed in several persona experiments (see [Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0088\" data-refLink=\"_i41 _i42\" href=\"#\">2019f</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0091\" data-refLink=\"_i41 _i42\" href=\"#\">2019g</a></span>]). Note that the <i>authenticity</i> construct is similar to constructs in earlier artificial image evaluation \u2013 specifically to <i>realism</i> (Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0109\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>) and <i>naturalness</i> (Iizuka, Simo-Serra, and Ishikawa <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0047\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>). However, the other constructs expand the perceptions typically used for image evaluation. In this sense, the hypotheses (a) add novelty to the measurement of user perceptions regarding the employment of artificial images in a real system output, and (b) are relevant for the design and use of personas. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 3. Survey statements. The participants answered using a 7-point Likert scale, ranging from Strongly Disagree to Strongly Agree. The statements were validated in (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>). WTU \u2013 willingness to use.</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"T0003-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=T0003&amp;doi=10.1080%2F0144929X.2020.1838610&amp;downloadType=CSV\">CSV</a><a data-id=\"T0003\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p></div><div id=\"S005-S2004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i29\">5.4. Analysis procedure</h3><p>The participants were grouped based on the persona presented (either the male or the female one), and whether the persona picture was artificial or real. The data was re-arranged to disentangle the gender of the persona, leading to one male-persona dataset (with a \u2018real\u2019 and an \u2018artificial\u2019 group), and a female-persona dataset with similar groups. This allowed the usage of a standard MANOVA (Hair et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0040\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>) to determine whether the measurements differed across artificial and real pictures; both genders were analysed independently.</p><p>To enhance the robustness of the findings, Bayesian independent samples tests were used to estimate Bayesian Factors (BF), comparing the likelihoods between the null and alternative hypotheses (Lee <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0057\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>). A Na\u00efve Bayes approach was employed with regards to priors.</p></div><div id=\"S005-S2005\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i30\">5.5. Results</h3><p><b>Male persona:</b> Beginning with the multivariate tests, no significant effects were registered for Type of Picture (Pillai's Trace\u2009=\u20090.017, <i>F</i>(5, 476)\u2009=\u20091.651, <span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0001.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0001.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup><mo>=</mo><mn>0.017</mn></math></span>, <i>p</i>\u2009=\u20090.145), indicating that none of the measurements differed across male real and artificial pictures for the persona profile. Nevertheless, we proceeded with an analysis of univariate tests, which confirmed that none of the measurements differed across types of pictures. The univariate differences for between-subjects are summarised in <a class=\"ref showTableEventRef\" data-ID=\"T0004\">Table 4</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 4. Univariate tests for between-subjects effects (df(error)\u2009=\u20091(480)).</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"false\" id=\"T0004-table-wrapper\"><a data-id=\"T0004\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p><p>The lack of differences in scale ratings (see <a href=\"#F0005\">Figure 5</a>) also indicates that the use of real or artificial pictures results in no differences for <i>authenticity</i>, <i>clarity</i>, <i>empathy</i>, or <i>willingness to use</i> for the male persona. <div class=\"figure figureViewer\" id=\"F0005\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 5. </span> Means of the scale variables for the <i>male</i> persona. Error bars indicate standard error.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0005image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0005_oc.jpg\"}' width=\"500\" height=\"252\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0005\"><p><span class=\"captionLabel\">Figure 5. </span> Means of the scale variables for the <i>male</i> persona. Error bars indicate standard error.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0005\"></div></p><p>The Bayesian analysis on the male persona indicates strong lack of evidence for differences regarding <i>clarity</i> (BF\u2009=\u200913.856; <i>F</i>(1, 480)\u2009=\u20090.002; <i>p</i>\u2009=\u20090.965) and <i>willingness to use</i> (BF\u2009=\u200910.030; <i>F</i>(1, 480)\u2009=\u20090.658; <i>p</i>\u2009=\u20090.418), and moderate lack of evidence for differences regarding <i>authenticity</i> (BF\u2009=\u20095.126; <i>F</i>(1, 480)\u2009=\u20092.023; <i>p</i>\u2009=\u20090.156) and <i>empathy</i> (BF\u2009=\u20095.040; <i>F</i>(1, 480)\u2009=\u20092.057; <i>p</i>\u2009=\u20090.152) (Jeffreys <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0051\" data-refLink=\"_i41 _i42\" href=\"#\">1998</a></span>).</p><p><b>Female persona:</b> Beginning with the multivariate tests, unlike with the male persona, significant effects were registered for Type of Picture (Pillai's Trace\u2009=\u20090.051, <i>F</i>(5, 476)\u2009=\u20095.081, <span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0004.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0004.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup><mo>=</mo><mn>0.051</mn></math></span>, <i>p</i>\u2009&lt;\u20090.001), indicating that at least one of the measurements differed between real and artificial pictures for the female persona. Thus, we proceeded with univariate testing to determine which of the measurements exhibited differences across picture type (see <a class=\"ref showTableEventRef\" data-ID=\"T0004\">Table 4</a>). <i>Authenticity</i> had significant differences across types of picture (BF\u2009=\u20090.032; <i>F</i>(1, 480)\u2009=\u200912.479, <i>p</i>\u2009&lt;\u20090.001). Artificial female pictures were perceived as more <i>authentic</i> (<i>M</i>\u2009=\u20095.075, SD\u2009=\u20091.016) than real pictures (<i>M</i>\u2009=\u20094.711, SD\u2009=\u20091.235). None of the other measurements differed across types of pictures. <a href=\"#F0006\">Figure 6</a> illustrates the comparison between the two groups for the female persona. <div class=\"figure figureViewer\" id=\"F0006\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 6. </span> Means of the scale variables for the <i>female</i> persona. Error bars indicate standard error.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0006image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0006_oc.jpg\"}' width=\"500\" height=\"252\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0006\"><p><span class=\"captionLabel\">Figure 6. </span> Means of the scale variables for the <i>female</i> persona. Error bars indicate standard error.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0006\"></div></p><p>This was corroborated by the Bayesian Factors that indicate that strong lack of evidence regarding differences for <i>clarity</i> (BF\u2009=\u200913.865; <i>F</i>(1, 480)\u2009=\u20090.001, <i>p</i>\u2009=\u20090.980) and <i>willingness to use</i> (BF\u2009=\u200913.828; <i>F</i>(1, 480)\u2009=\u20090.006, <i>p</i>\u2009=\u20090.938), and moderate lack of evidence for <i>empathy</i> (BF\u2009=\u20098.290; <i>F</i>(1, 480)\u2009=\u20091.045, <i>p</i>\u2009=\u20090.307) (Jeffreys <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0051\" data-refLink=\"_i41 _i42\" href=\"#\">1998</a></span>).</p><p>Finally, as one the statements in PPS specifically dealt with the picture of the persona (Item 3: \u2018<i>The picture of the persona looks authentic</i>\u2019.), we inspected the mean scores of this statement separately. In line with our other findings, the artificial female persona picture is in fact considered to be more authentic than the real photograph (M<sub>FPA</sub>\u2009=\u20095.74 vs. M<sub>FPR</sub>\u2009=\u20094.89). This difference is statistically significant (<i>t</i>(480)\u2009=\u20096.896, <i>p</i>\u2009&lt;\u20090.001). For the male persona, differences are minimal (M<sub>MPA</sub>\u2009=\u20095.11 vs. M<sub>MPR</sub>\u2009=\u20095.14) and not statistically significant (<i>t</i>(480)\u2009=\u2009\u22120.187, <i>p</i>\u2009=\u20090.851).</p><p>In summary, for H01, there were no significant differences in the perceptions for the male persona; however, for the female persona, artificial pictures actually <i>increased</i> the perceived authenticity. For the other perceptions, there was no significant change when replacing the real photo with the artificially generated picture. Therefore, <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">There was no evidence that using artificial pictures decrease the perceived authenticity of the persona (H01: supported).</p></li><li><p class=\"inline\">There was no evidence that using artificial pictures decrease the clarity of the persona profile (H02: supported).</p></li><li><p class=\"inline\">There was no evidence that using artificial pictures decrease empathy towards the persona (H03: supported).</p></li><li><p class=\"inline\">There was no evidence that using artificial pictures decrease the willingness to use the persona (H04).</p></li></ul></p></div></div><div id=\"S006\" class=\"NLM_sec NLM_sec-type_discussion NLM_sec_level_1\"><h2 id=\"_i34\" class=\"section-heading-2\">6. Discussion</h2><div id=\"S006-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i35\">6.1. Can artificial pictures be used for DDPs?</h3><p>Our analysis focuses on a timely problem in a relevant, yet underexplored area. However, it is one of increasing importance in a media rich online environment (Church, Iyer, and Zhao <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0021\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). The impact of artificial facial pictures on user perceptions has not been studied thoroughly in previous HCI design literature. The lack of applied user studies is understandable given that until recently, the generated facial pictures were not close to realistic, so the research focus was on improving algorithms. However, as the quality of the facial pictures improves, the focus ought to shift towards evaluation studies in real-world use cases, systems, and applications. As there is a lack of literature in this regard, the research presented here contains a step forward in analysing the use of artificially facial generated pictures in real systems.</p><p>In terms of results, the crowd evaluation suggests that more than half of the artificial pictures are considered as either <i>perfect</i> or <i>high quality</i>. The ratio of \u2018perfect and high-quality\u2019 pictures to the rest is around 1.5, implying that most of the pictures are satisfactory according to the guidelines we provided. The persona perception analysis shows that the use of artificial pictures vs. real pictures in persona profiles does not reduce the authenticity of the persona or people's willingness to use the persona, two crucial concerns of persona applicability. Therefore, we find the state-of-the-art of AIG satisfactory for a persona and most likely for other systems requiring the substantial use of facial images. So, it is possible to replace the need for manually retrieving pictures from online photo banks with a process of automatically generated pictures.</p></div><div id=\"S006-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i36\">6.2. Gender differences in perception</h3><p>Regarding the female persona with an artificial picture being perceived as more authentic, we surmise that there might be a \u2018stock photo\u2019 effect involved, rather than a gender effect. This proposition is backed up by previous findings of stock photos being perceived differently by individuals than non-stock photos (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0088\" data-refLink=\"_i41 _i42\" href=\"#\">2019f</a></span>). Visually, to the respondents, the real photo chosen for the female persona appears different from the one chosen for the male persona (see <a href=\"#F0004\">Figure 4</a>). It is difficult to explain or quantify why this is. We interpret this finding such that the choice of pictures for a persona profile, and perhaps other system contexts, is a delicate matter; even small nuances can affect user perceptions.</p><p>This interpretation is generally in line with previous HCI research regarding the foundational impact of photos in persona profiles (Hill et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0042\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0090\" data-refLink=\"_i41 _i42\" href=\"#\">2018d</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0084\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>). Possibly, stock photos can appear, at times, less realistic than photos of \u2018real people\u2019 because they are \u2018too shiny, too perfect\u2019 (or \u2018too smiling\u2019 [Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0087\" data-refLink=\"_i41 _i42\" href=\"#\">2019e</a></span>]). Thus, <i>if the generator's outputs are closer to real people than stock photos in their appearance</i>, it is possible that these pictures are deemed more realistic than stock photos. However, this does not explain why the effect was found for the female persona and not for the male one. The only way to establish if there is a gender effect that influences perceptions of stock photos is to conduct repeated experiments with stock photos of different people. In addition to repeated experiments, for future research, the gender difference suggests another variable to consider: \u2018the degree of photo-editing\u2019 or \u2018shiny factor\u2019 (i.e. how polished the stock photo is and how this affects persona perceptions). The proper adjustment of this variable is best ensured via a manipulation check.</p></div><div id=\"S006-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i37\">6.3. Wider implications for HCI and system implementation</h3><p>In a broader context, the manuscript contributes to evaluating a machine learning tool for UX/UI design work. For the use of artificial images, guidelines are provided. <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">Solutions for Mitigating Subjectivity: The results indicate that evaluating the quality of artificial facial pictures contains a moderate to a high level of subjectivity, making reliable evaluation for production systems costlier. We hypothesise that there will always be some degree of subjectivity, as individuals vary in their ability to pay attention to details. This can be partially remedied by <i>choosing the pictures with the highest agreement between the raters</i>, or <i>using a binary rating scale</i> (i.e. \u2018good enough\u2019 vs. \u2018not good enough\u2019) as the agreement is generally easier to obtain with fewer classes (Alonso <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0003\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>). The observed \u2018disagreement\u2019 may be partly fallacious because people might agree whether a picture is either usable (4 or 5) or non-usable (1 or 2), but the exact agreement between 4 or 5, for example, is lower. As stated, for practical purposes, it does not matter if a picture is \u2018Perfect\u2019 or \u2018High quality\u2019, as both classes are decent, at least for this use case.</p></li><li><p class=\"inline\">Handling of Borderline Cases. Regarding pictures to use in a production system, we recommend a <i>borderline principle</i>: if in doubt of the picture quality, reject it. The marginal cost of generating new pictures is diminishingly low but showing a low-quality picture decreases user experience, sometimes drastically. For this reason, the economics of automatic image generation are in favour of rejecting borderline images more than letting through distorted images. However, rejecting borderline images does increase the total cost of evaluation because to obtain <i>n</i> useful pictures, one now has to obtain <i>n</i>\u2009\u00d7\u2009(1\u2009+\u2009false positive rate) ratings, which is (<i>n</i>\u2009\u00d7\u2009(1\u2009+\u2009false positive rate) \u2013 <i>n</i>) / <i>n</i> ratings more than <i>n</i> ratings. Additionally, as we have shown, the higher the disagreement among the crowd raters, the more ratings required.</p></li><li><p class=\"inline\">Final Choice for Human. In evaluating the suitability of artificial pictures for use in real applications, domain expertise is needed because, irrespective of quality guidelines, the crowd may have different quality standards than domain experts. For example, the crowd can be used to filter out low-quality photos, but the \u2018better\u2019 quality photos should be evaluated specifically by domain experts, as different domains likely have different quality standards. For personas, the pictures need to be of high quality, but when implementing them for the system, they are cropped into a smaller resolution that helps obfuscate minor errors.</p></li></ul></p></div><div id=\"S006-S2004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i38\">6.4. Future research avenues</h3><p>The following avenues for future research are proposed. <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">Suitability in Other Domains. For example, <i>how do quality standards and requirements by users and organizations differ across domains and use cases? How well are artificial (\u2018fake\u2019) pictures detected by end users, such as consumers and voters?</i> This research ties in with the nascent field of \u2018deep fakes\u2019 (Yang, Li, and Lyu <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0102\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), i.e. images and videos purposefully manipulated for a political or commercial agenda. To this end, future studies could investigate the wider impact of using AI-generated images for profile pictures on sharing, economy platforms, or social media and news sites, and how that impact user perceptions, such as trust. Another interesting domain for suitability studies includes marketing, as facial pictures are widely deployed to advertise products such as fashion and luxury items.</p></li><li><p class=\"inline\">Algorithmic Bias. It would be important to investigate if the generated pictures involve an algorithmic bias \u2013 given that the training data may be biased, it would be worthwhile to analyze how diverse the generated pictures for different ethnicities, ages, and genders. Regarding persona perceptions, the race could be a confounding factor in our research and should be analysed separately in future research. A related question is: does the picture quality vary by demographic factors such as gender and race? Studies on algorithmic bias have been carried out within the HCI community (Eslami et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0031\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0084\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>) and should be extended to this context.</p></li><li><p class=\"inline\">Demographically Conditional Images. For future development, we envision a system that automatically generates persona-specific pictures based on specific features/attributes of the personas \u2013 this would enable \u2018on-demand\u2019 picture creation for new personas generated by APG, whereas currently, the pictures need to manually tagged for age, gender, and country.</p></li></ul></p></div></div><div id=\"S007\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i39\" class=\"section-heading-2\">7. Conclusion</h2><p>Our research goal was to evaluate the applicability of artificial pictures for personas along two dimensions: their quality and their impact on user perceptions. We found that more than half of the pictures were rated as perfect or high quality, with none as unusable. Moreover, the use of artificial pictures did not decrease the perceptions of personas that are found important in the HCI literature. These results can be considered as a vote of confidence for the current state of technology concerning the automatic generation of facial pictures and their use in data-driven persona profiles.</p></div></div><script type=\"text/javascript\">\r\n                        window.figureViewer={doi:'10.1080/0144929X.2020.1838610',path:'/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527',figures:[{i:'F0001',g:[{m:'tbit_a_1838610_f0001_ob.jpg',l:'tbit_a_1838610_f0001_ob.jpeg',size:'45 KB'}]}\r\n                            ,{i:'F0002',g:[{m:'tbit_a_1838610_f0002_oc.jpg',l:'tbit_a_1838610_f0002_oc.jpeg',size:'374 KB'}]}\r\n                            ,{i:'F0003',g:[{m:'tbit_a_1838610_f0003_oc.jpg',l:'tbit_a_1838610_f0003_oc.jpeg',size:'113 KB'}]}\r\n                            ,{i:'F0004',g:[{m:'tbit_a_1838610_f0004_oc.jpg',l:'tbit_a_1838610_f0004_oc.jpeg',size:'140 KB'}]}\r\n                            ,{i:'F0005',g:[{m:'tbit_a_1838610_f0005_oc.jpg',l:'tbit_a_1838610_f0005_oc.jpeg',size:'42 KB'}]}\r\n                            ,{i:'F0006',g:[{m:'tbit_a_1838610_f0006_oc.jpg',l:'tbit_a_1838610_f0006_oc.jpeg',size:'43 KB'}]}\r\n                            ]}</script><script type=\"text/javascript\">window.tableViewer={doi:'10.1080/0144929X.2020.1838610',path:'/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527',tables:[{i:'T0001'},{i:'T0002'},{i:'T0003'},{i:'T0004'}]}</script><script type=\"text/javascript\">window.tableIDIndexMap = {\"id\":-1};window.tableIDIndexMap['T0001'] = 1; window.tableIDIndexMap['T0002'] = 2; window.tableIDIndexMap['T0003'] = 3; window.tableIDIndexMap['T0004'] = 4; </script><div id=\"table-content-T0001\" class=\"hidden\"><table class=\"table frame_bottom\"><div class=\"caption\"><b>Table 1. The results of crowd evaluation based on a majority vote of the picture quality. Most frequent class bolded. Example facial image from each of the 5 classes shown for comparison.</b></div><colgroup><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\" class=\"last\"><td align=\"center\" class=\" align_center last\"><span class=\"NLM_inline-graphic\"><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_ilg0001.gif\" alt=\"\" /></span></td></tr></tbody></table></div><div id=\"table-content-T0002\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><b>Table 2. Agreement metrics for the crowdsourced ratings showing satisfactory internal validity.</b></div><colgroup><col /><col /><col /><col /><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">Measure</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">Value</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">SE</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">95% CI</th><th align=\"center\" valign=\"bottom\" class=\" align_center last\"><i>P</i></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">PA</td><td align=\"char\" char=\".\" class=\" align_char\">86.2%</td><td align=\"char\" char=\".\" class=\" align_char\">0.6%</td><td align=\"left\" class=\" align_left\">85.27%, 87.2%</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">AC1</td><td align=\"char\" char=\".\" class=\" align_char\">0.627</td><td align=\"char\" char=\".\" class=\" align_char\">0.017</td><td align=\"left\" class=\" align_left\">0.59, 0.66</td><td align=\"left\" class=\" align_left last\">&lt;0.001</td></tr></tbody></table></div><div id=\"table-content-T0003\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><b>Table 3. Survey statements. The participants answered using a 7-point Likert scale, ranging from Strongly Disagree to Strongly Agree. The statements were validated in (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>). WTU \u2013 willingness to use.</b></div><colgroup><col /><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">Perception</th><th align=\"center\" valign=\"bottom\" class=\" align_center last\">Statements</th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Authenticity</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">The persona seems like a real person.</p></li><li><p class=\"inline\">I have met people like this persona.</p></li><li><p class=\"inline\">The picture of the persona looks authentic.</p></li><li><p class=\"inline\">The persona seems to have a personality.</p></li></ul></td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Clarity</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">The information about the persona is well presented.</p></li><li><p class=\"inline\">The text in the persona profile is clear enough to read.</p></li><li><p class=\"inline\">The information in the persona profile is easy to understand.</p></li></ul></td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Empathy</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">I feel like I understand this persona.</p></li><li><p class=\"inline\">I feel strong ties to this persona.</p></li><li><p class=\"inline\">I can imagine a day in the life of this persona.</p></li></ul></td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Willingness To Use</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">I would like to know more about this persona.</p></li><li><p class=\"inline\">This persona would improve my ability to make decisions about the customers it describes.</p></li><li><p class=\"inline\">I would make use of this persona in my task [of creating a YouTube video].</p></li></ul></td></tr></tbody></table></div><div id=\"table-content-T0004\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><b>Table 4. Univariate tests for between-subjects effects (df(error)\u2009=\u20091(480)).</b></div><colgroup><col /><col /><col /><col /><col /><col /><col /><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">\u00a0</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">\u00a0</th><th colspan=\"3\" align=\"center\" valign=\"bottom\" class=\" align_center\"><i>Male</i> persona</th><th colspan=\"3\" align=\"center\" valign=\"bottom\" class=\" align_center last\"><i>Female</i> persona</th></tr><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">Independent variable</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">Dependent variable</th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><i>F</i></th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0002.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0002.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup></math></span></th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><i>p</i>-value</th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><i>F</i></th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0003.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0003.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup></math></span></th><th align=\"center\" valign=\"bottom\" class=\" align_center last\"><i>p</i>- value</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"4\" align=\"left\" class=\" align_left\">Type of Picture (real or artificial)</td><td align=\"left\" class=\" align_left\">Authenticity</td><td align=\"char\" char=\".\" class=\" align_char\">2.023</td><td align=\"char\" char=\".\" class=\" align_char\">0.004</td><td align=\"char\" char=\".\" class=\" align_char\">0.156</td><td align=\"char\" char=\".\" class=\" align_char\">12.479</td><td align=\"char\" char=\".\" class=\" align_char\">0.025</td><td align=\"char\" char=\".\" class=\" align_char last\">&lt;0.001</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Clarity</td><td align=\"char\" char=\".\" class=\" align_char\">0.002</td><td align=\"char\" char=\".\" class=\" align_char\">&lt;0.001</td><td align=\"char\" char=\".\" class=\" align_char\">0.965</td><td align=\"char\" char=\".\" class=\" align_char\">0.001</td><td align=\"char\" char=\".\" class=\" align_char\">&lt;0.001</td><td align=\"char\" char=\".\" class=\" align_char last\">0.980</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Empathy</td><td align=\"char\" char=\".\" class=\" align_char\">2.057</td><td align=\"char\" char=\".\" class=\" align_char\">0.004</td><td align=\"char\" char=\".\" class=\" align_char\">0.152</td><td align=\"char\" char=\".\" class=\" align_char\">1.045</td><td align=\"char\" char=\".\" class=\" align_char\">0.002</td><td align=\"char\" char=\".\" class=\" align_char last\">0.307</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">WTU</td><td align=\"char\" char=\".\" class=\" align_char\">0.658</td><td align=\"char\" char=\".\" class=\" align_char\">0.001</td><td align=\"char\" char=\".\" class=\" align_char\">0.418</td><td align=\"char\" char=\".\" class=\" align_char\">0.006</td><td align=\"char\" char=\".\" class=\" align_char\">&lt;0.001</td><td align=\"char\" char=\".\" class=\" align_char last\">0.938</td></tr></tbody></table></div><div id=\"coi-statement\" class=\"NLM_sec\"><div id=\"S008\" class=\"NLM_sec NLM_sec-type_COI-statement NLM_sec_level_1\"><h2 id=\"_i41\" class=\"section-heading-2\">Disclosure statement</h2><p>No potential conflict of interest was reported by the author(s).</p></div></div><a id=\"inline_frontnotes\"></a><h2>Notes</h2><div class=\"summation-section\"><a id=\"EN0001\"></a><p>1 A demo of the system can be accessed at <a class=\"ext-link\" href=\"https://persona.qcri.org\" target=\"_blank\">https://persona.qcri.org</a></p><a id=\"EN0002\"></a><p>2 <a class=\"ext-link\" href=\"https://persona.qcri.org\" target=\"_blank\">https://persona.qcri.org</a></p><a id=\"EN0003\"></a><p>3 <a class=\"ext-link\" href=\"https://github.com/NVlabs/stylegan\" target=\"_blank\">https://github.com/NVlabs/stylegan</a></p><a id=\"EN0004\"></a><p>4 <a class=\"ext-link\" href=\"https://www.tensorflow.org/\" target=\"_blank\">https://www.tensorflow.org/</a></p><a id=\"EN0005\"></a><p>5 <a class=\"ext-link\" href=\"https://github.com/NVlabs/stylegan\" target=\"_blank\">https://github.com/NVlabs/stylegan</a></p><a id=\"EN0006\"></a><p>6 Confidence is defined as agreement adjusted by trust score of each rater.</p></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneFull\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"c8b0dea6-9842-46af-b708-142fe9107344\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"c8b0dea6-9842-46af-b708-142fe9107344\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div></div><div id=\"references-Section\"><h2 id=\"figures\">References</h2><ul class=\"references numeric-ordered-list\"><li id=\"CIT0001\"><span><span class=\"hlFld-ContribAuthor\">Ablanedo, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Fairchild</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Griffith</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Rodeheffer</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Is This Person Real? Avatar Stylization and Its Influence on Human Perception in a Counseling Training Environment</span>.\u201d In: Chen J., Fragomeni G. (eds). In <i>International Conference on Virtual, Augmented and Mixed Reality</i>, <span class=\"NLM_fpage\">279</span>\u2013<span class=\"NLM_lpage\">289</span>. Lecture Notes in Computer Science, vol 10909. Springer, Cham. doi:<span class=\"NLM_pub-id\">10.1007/978-3-319-91581-4_20</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0001&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-319-91581-4_20\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=279-289&author=J.+Ablanedo&author=E.+Fairchild&author=T.+Griffith&author=C.+Rodeheffer&title=Is+This+Person+Real%3F+Avatar+Stylization+and+Its+Influence+on+Human+Perception+in+a+Counseling+Training+Environment\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0002\"><span><span class=\"hlFld-ContribAuthor\">Alam, <span class=\"NLM_given-names\">F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Ofli</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Imran</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Crisismmd: Multimodal Twitter Datasets from Natural Disasters</span>.\u201d In <i>Twelfth International AAAI Conference on Web and Social Media</i>. AAAI. Palo Alto, California, USA.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&author=F.+Alam&author=F.+Ofli&author=M.+Imran&title=Crisismmd%3A+Multimodal+Twitter+Datasets+from+Natural+Disasters\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0003\"><span><span class=\"hlFld-ContribAuthor\">Alonso, <span class=\"NLM_given-names\">O.</span></span> <span class=\"NLM_year\">2015</span>. \u201c<span class=\"NLM_chapter-title\">Practical Lessons for Gathering Quality Labels at Scale</span>.\u201d In <i>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</i>, <span class=\"NLM_fpage\">1089</span>\u2013<span class=\"NLM_lpage\">1092</span>. doi:<span class=\"NLM_pub-id\">10.1145/2766462.2776778</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0003&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2766462.2776778\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&pages=1089-1092&author=O.+Alonso&title=Practical+Lessons+for+Gathering+Quality+Labels+at+Scale\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0004\"><span><span class=\"hlFld-ContribAuthor\">An, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018a</span>. \u201c<span class=\"NLM_article-title\">Customer Segmentation Using Online Platforms: Isolating Behavioral and Demographic Segments for Persona Creation via Aggregated User Data</span>.\u201d <i>Social Network Analysis and Mining</i> 8 (1). doi:<span class=\"NLM_pub-id\">10.1007/s13278-018-0531-0</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0004&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2Fs13278-018-0531-0\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0004&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000442734100002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=8&publication_year=2018a&issue=1&author=J.+An&author=H.+Kwak&author=S.+Jung&author=J.+Salminen&author=B.+J.+Jansen&title=Customer+Segmentation+Using+Online+Platforms%3A+Isolating+Behavioral+and+Demographic+Segments+for+Persona+Creation+via+Aggregated+User+Data&doi=10.1007%2Fs13278-018-0531-0\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0005\"><span><span class=\"hlFld-ContribAuthor\">An, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018b</span>. \u201c<span class=\"NLM_article-title\">Imaginary People Representing Real Numbers: Generating Personas From Online Social Media Data</span>.\u201d <i>ACM Transactions on the Web (TWEB)</i> 12 (4): Article No. 27. doi:<span class=\"NLM_pub-id\">10.1145/3265986</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0005&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3265986\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0005&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000457144000007\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=12&publication_year=2018b&issue=4&author=J.+An&author=H.+Kwak&author=J.+Salminen&author=S.+Jung&author=B.+J.+Jansen&title=Imaginary+People+Representing+Real+Numbers%3A+Generating+Personas+From+Online+Social+Media+Data&doi=10.1145%2F3265986\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0006\"><span><span class=\"hlFld-ContribAuthor\">Antipov, <span class=\"NLM_given-names\">G.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Baccouche</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-L.</span> Dugelay</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Face Aging with Conditional Generative Adversarial Networks</span>.\u201d In <i>2017 IEEE International Conference on Image Processing (ICIP)</i>, IEEE, Beijing, <span class=\"NLM_fpage\">2089</span>\u2013<span class=\"NLM_lpage\">2093</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0006&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICIP.2017.8296650\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=2089-2093&author=G.+Antipov&author=M.+Baccouche&author=J.-L.+Dugelay&title=Face+Aging+with+Conditional+Generative+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0007\"><span><span class=\"hlFld-ContribAuthor\">Araujo, <span class=\"NLM_given-names\">T.</span></span> <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_article-title\">Living up to the Chatbot Hype: The Influence of Anthropomorphic Design Cues and Communicative Agency Framing on Conversational Agent and Company Perceptions</span>.\u201d <i>Computers in Human Behavior</i> 85: <span class=\"NLM_fpage\">183</span>\u2013<span class=\"NLM_lpage\">189</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.chb.2018.03.051</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0007&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.chb.2018.03.051\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0007&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000435622000018\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=85&publication_year=2018&pages=183-189&author=T.+Araujo&title=Living+up+to+the+Chatbot+Hype%3A+The+Influence+of+Anthropomorphic+Design+Cues+and+Communicative+Agency+Framing+on+Conversational+Agent+and+Company+Perceptions&doi=10.1016%2Fj.chb.2018.03.051\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0008\"><span><span class=\"hlFld-ContribAuthor\">Ashraf, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N. I.</span> Jaafar</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Sulaiman</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">System- vs. Consumer-Generated Recommendations: Affective and Social-Psychological Effects on Purchase Intention</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1259</span>\u2013<span class=\"NLM_lpage\">1272</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1583285</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0008&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1583285&amp;tollfreelink=2_18_1f2362c1c7976a48b7fe16c48cce10bbce95e9161acf376d5828e9ae7c174016\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0008&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200006\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1259-1272&issue=12&author=M.+Ashraf&author=N.+I.+Jaafar&author=A.+Sulaiman&title=System-+vs.+Consumer-Generated+Recommendations%3A+Affective+and+Social-Psychological+Effects+on+Purchase+Intention&doi=10.1080%2F0144929X.2019.1583285\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0009\"><span><span class=\"hlFld-ContribAuthor\">Banerjee, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Capozzoli</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> McSweeney</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Sinha</span>. <span class=\"NLM_year\">1999</span>. \u201c<span class=\"NLM_article-title\">Beyond Kappa: A Review of Interrater Agreement Measures</span>.\u201d <i>Canadian Journal of Statistics</i> 27 (1): <span class=\"NLM_fpage\">3</span>\u2013<span class=\"NLM_lpage\">23</span>. doi:<span class=\"NLM_pub-id\">10.2307/3315487</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0009&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.2307%2F3315487\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0009&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000080854000002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=27&publication_year=1999&pages=3-23&issue=1&author=M.+Banerjee&author=M.+Capozzoli&author=L.+McSweeney&author=D.+Sinha&title=Beyond+Kappa%3A+A+Review+of+Interrater+Agreement+Measures&doi=10.2307%2F3315487\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0010\"><span><span class=\"hlFld-ContribAuthor\">Barratt, <span class=\"NLM_given-names\">S.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Sharma</span>. <span class=\"NLM_year\">2018</span>. A Note on the Inception Score. <i>ArXiv:1801.01973 [Cs, Stat]</i>. <a class=\"ext-link\" href=\"http://arxiv.org/abs/1801.01973\" target=\"_blank\">http://arxiv.org/abs/1801.01973</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Barratt%2C+S.%2C+and+R.+Sharma.+2018.+A+Note+on+the+Inception+Score.+ArXiv%3A1801.01973+%5BCs%2C+Stat%5D.+http%3A%2F%2Farxiv.org%2Fabs%2F1801.01973.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0011\"><span><span class=\"hlFld-ContribAuthor\">Barrett, <span class=\"NLM_given-names\">P.</span></span> <span class=\"NLM_year\">2007</span>. \u201c<span class=\"NLM_article-title\">Structural Equation Modelling: Adjudging Model fit</span>.\u201d <i>Personality and Individual Differences</i> 42 (5): <span class=\"NLM_fpage\">815</span>\u2013<span class=\"NLM_lpage\">824</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0011&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.paid.2006.09.018\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0011&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000245028700002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=42&publication_year=2007&pages=815-824&issue=5&author=P.+Barrett&title=Structural+Equation+Modelling%3A+Adjudging+Model+fit\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0012\"><span><span class=\"hlFld-ContribAuthor\">Baxter, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Courage</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Caine</span>. <span class=\"NLM_year\">2015</span>. <i>Understanding Your Users: A Practical Guide to User Requirements Methods, Tools, and Techniques</i>. <span class=\"NLM_edition\">2nd ed.</span> Burlington, Massachusetts. <span class=\"NLM_publisher-name\">Morgan Kaufmann</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&author=K.+Baxter&author=C.+Courage&author=K.+Caine&title=Understanding+Your+Users%3A+A+Practical+Guide+to+User+Requirements+Methods%2C+Tools%2C+and+Techniques\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0013\"><span><span class=\"hlFld-ContribAuthor\">Bazzano, <span class=\"NLM_given-names\">A. N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Martin</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Hicks</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Faughnan</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Murphy</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_article-title\">Human-centred Design in Global Health: A Scoping Review of Applications and Contexts</span>.\u201d <i>PloS One</i> 12 (11).<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0013&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1371%2Fjournal.pone.0186744\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0013&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000414229700029\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=12&publication_year=2017&issue=11&author=A.+N.+Bazzano&author=J.+Martin&author=E.+Hicks&author=M.+Faughnan&author=L.+Murphy&title=Human-centred+Design+in+Global+Health%3A+A+Scoping+Review+of+Applications+and+Contexts\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0014\"><span><span class=\"hlFld-ContribAuthor\">Brangier, <span class=\"NLM_given-names\">E.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Bornet</span>. <span class=\"NLM_year\">2011</span>. \u201c<span class=\"NLM_chapter-title\">Persona: A Method to Produce Representations Focused on Consumers\u2019 Needs</span>.\u201d Eds: Waldemar Karwowski, Marcelo M. Soares, Neville A. Stanton. In <i>Human Factors and Ergonomics in Consumer Product Design</i>, <span class=\"NLM_fpage\">37</span>\u2013<span class=\"NLM_lpage\">61</span>. <span class=\"NLM_publisher-name\">Taylor and Francis</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0014&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1201%2Fb10950-5\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=37-61&author=E.+Brangier&author=C.+Bornet&title=Persona%3A+A+Method+to+Produce+Representations+Focused+on+Consumers%E2%80%99+Needs\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0015\"><span><span class=\"hlFld-ContribAuthor\">Brauner, <span class=\"NLM_given-names\">P.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Philipsen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. C.</span> Valdez</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Ziefle</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">What Happens when Decision Support Systems Fail? \u2014 The Importance of Usability on Performance in Erroneous Systems</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1225</span>\u2013<span class=\"NLM_lpage\">1242</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1581258</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0015&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1581258&amp;tollfreelink=2_18_672e5a20433733ce1bcda6b8d9b63cce32368d15868ac8dbf217bb04c4dc0f81\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0015&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1225-1242&issue=12&author=P.+Brauner&author=R.+Philipsen&author=A.+C.+Valdez&author=M.+Ziefle&title=What+Happens+when+Decision+Support+Systems+Fail%3F+%E2%80%94+The+Importance+of+Usability+on+Performance+in+Erroneous+Systems&doi=10.1080%2F0144929X.2019.1581258\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0016\"><span><span class=\"hlFld-ContribAuthor\">Brickey, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Walczak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Burgess</span>. <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_article-title\">Comparing Semi-Automated Clustering Methods for Persona Development</span>.\u201d <i>IEEE Transactions on Software Engineering</i> 38 (3): <span class=\"NLM_fpage\">537</span>\u2013<span class=\"NLM_lpage\">546</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0016&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FTSE.2011.60\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0016&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000304414400003\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2012&pages=537-546&issue=3&author=J.+Brickey&author=S.+Walczak&author=T.+Burgess&title=Comparing+Semi-Automated+Clustering+Methods+for+Persona+Development\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0017\"><span><span class=\"hlFld-ContribAuthor\">Chapman, <span class=\"NLM_given-names\">C. N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Love</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. P.</span> Milham</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> ElRif</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. L.</span> Alford</span>. <span class=\"NLM_year\">2008</span>. \u201c<span class=\"NLM_article-title\">Quantitative Evaluation of Personas as Information</span>.\u201d <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 52 (16): <span class=\"NLM_fpage\">1107</span>\u2013<span class=\"NLM_lpage\">1111</span>. doi:<span class=\"NLM_pub-id\">10.1177/154193120805201602</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0017&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1177%2F154193120805201602\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=52&publication_year=2008&pages=1107-1111&issue=16&author=C.+N.+Chapman&author=E.+Love&author=R.+P.+Milham&author=P.+ElRif&author=J.+L.+Alford&title=Quantitative+Evaluation+of+Personas+as+Information&doi=10.1177%2F154193120805201602\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0018\"><span><span class=\"hlFld-ContribAuthor\">Chapman, <span class=\"NLM_given-names\">C. N.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. P.</span> Milham</span>. <span class=\"NLM_year\">2006</span>. \u201c<span class=\"NLM_article-title\">The Personas\u2019 New Clothes: Methodological and Practical Arguments against a Popular Method</span>.\u201d <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 50 (5): <span class=\"NLM_fpage\">634</span>\u2013<span class=\"NLM_lpage\">636</span>. doi:<span class=\"NLM_pub-id\">10.1177/154193120605000503</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0018&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1177%2F154193120605000503\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=50&publication_year=2006&pages=634-636&issue=5&author=C.+N.+Chapman&author=R.+P.+Milham&title=The+Personas%E2%80%99+New+Clothes%3A+Methodological+and+Practical+Arguments+against+a+Popular+Method&doi=10.1177%2F154193120605000503\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0019\"><span><span class=\"hlFld-ContribAuthor\">Chen, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Chen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Zhang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Mitchell</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Yu</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">Photo-Realistic Facial Details Synthesis from Single Image</span>.\u201d In <i>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</i>, <span class=\"NLM_fpage\">9428</span>\u2013<span class=\"NLM_lpage\">9438</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICCV.2019.00952</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0019&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICCV.2019.00952\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=9428-9438&author=A.+Chen&author=Z.+Chen&author=G.+Zhang&author=K.+Mitchell&author=J.+Yu&title=Photo-Realistic+Facial+Details+Synthesis+from+Single+Image\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0020\"><span><span class=\"hlFld-ContribAuthor\">Choi, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Choi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Kim</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-W.</span> Ha</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Kim</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Choo</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Stargan: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</span>.\u201d In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, IEEE, Salt Lake City, UT. <span class=\"NLM_fpage\">8789</span>\u2013<span class=\"NLM_lpage\">8797</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0020&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2018.00916\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=8789-8797&author=Y.+Choi&author=M.+Choi&author=M.+Kim&author=J.-W.+Ha&author=S.+Kim&author=J.+Choo&title=Stargan%3A+Unified+Generative+Adversarial+Networks+for+Multi-Domain+Image-to-Image+Translation\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0021\"><span><span class=\"hlFld-ContribAuthor\">Church, <span class=\"NLM_given-names\">E. M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Iyer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Zhao</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Pictures Tell a Story: Antecedents of Rich-Media Curation in Social Network Sites</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (4): <span class=\"NLM_fpage\">361</span>\u2013<span class=\"NLM_lpage\">374</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2018.1535620</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0021&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2018.1535620&amp;tollfreelink=2_18_b111162a921aac579d26b7781fec171ea9e245375dab467468b2077a78dd5de4\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0021&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000460627500004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=361-374&issue=4&author=E.+M.+Church&author=L.+Iyer&author=X.+Zhao&title=Pictures+Tell+a+Story%3A+Antecedents+of+Rich-Media+Curation+in+Social+Network+Sites&doi=10.1080%2F0144929X.2018.1535620\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0022\"><span><span class=\"hlFld-ContribAuthor\">Cooper, <span class=\"NLM_given-names\">A.</span></span> <span class=\"NLM_year\">1999</span>. <i>The Inmates Are Running the Asylum: Why High Tech Products Drive Us Crazy and How to Restore the Sanity</i>. <span class=\"NLM_edition\">1st ed</span>. <span class=\"NLM_publisher-name\">Carmel, Indiana. Sams \u2013 Pearson Education</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0022&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-322-99786-9_1\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=1999&author=A.+Cooper&title=The+Inmates+Are+Running+the+Asylum%3A+Why+High+Tech+Products+Drive+Us+Crazy+and+How+to+Restore+the+Sanity\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0023\"><span><span class=\"hlFld-ContribAuthor\">Cooper, <span class=\"NLM_given-names\">A.</span></span> <span class=\"NLM_year\">2004</span>. <i>The Inmates Are Running the Asylum: Why High Tech Products Drive Us Crazy and How to Restore the Sanity</i>. <span class=\"NLM_edition\">2nd ed.</span> <span class=\"NLM_publisher-name\">Carmel, Indiana. Pearson Higher Education</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&author=A.+Cooper&title=The+Inmates+Are+Running+the+Asylum%3A+Why+High+Tech+Products+Drive+Us+Crazy+and+How+to+Restore+the+Sanity\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0024\"><span><span class=\"hlFld-ContribAuthor\">Dey, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Juefei-Xu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. N.</span> Boddeti</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Savvides</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">RankGAN: A Maximum Margin Ranking GAN for Generating Faces</span>.\u201d In <i>Computer Vision \u2013 ACCV 2018. (Vol. 11363)</i>. <span class=\"NLM_publisher-loc\">Cham</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-20893-6_1</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0024&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-20893-6_1\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&author=R.+Dey&author=F.+Juefei-Xu&author=V.+N.+Boddeti&author=M.+Savvides&title=RankGAN%3A+A+Maximum+Margin+Ranking+GAN+for+Generating+Faces\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0025\"><span><span class=\"hlFld-ContribAuthor\">Di, <span class=\"NLM_given-names\">X.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. M.</span> Patel</span>. <span class=\"NLM_year\">2017</span>. \u201cFace Synthesis from Visual Attributes via Sketch Using Conditional VAEs and GANs.\u201d <i>ArXiv:1801.00077 [Cs]</i>. <a class=\"ext-link\" href=\"http://arxiv.org/abs/1801.00077\" target=\"_blank\">http://arxiv.org/abs/1801.00077</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Di%2C+X.%2C+and+V.+M.+Patel.+2017.+%E2%80%9CFace+Synthesis+from+Visual+Attributes+via+Sketch+Using+Conditional+VAEs+and+GANs.%E2%80%9D+ArXiv%3A1801.00077+%5BCs%5D.+http%3A%2F%2Farxiv.org%2Fabs%2F1801.00077.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0026\"><span><span class=\"hlFld-ContribAuthor\">Di, <span class=\"NLM_given-names\">X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. A.</span> Sindagi</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. M.</span> Patel</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks</span>.\u201d In <i>2018 24th International Conference on Pattern Recognition (ICPR)</i>, <span class=\"NLM_fpage\">1079</span>\u2013<span class=\"NLM_lpage\">1084</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICPR.2018.8545081</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0026&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICPR.2018.8545081\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=1079-1084&author=X.+Di&author=V.+A.+Sindagi&author=V.+M.+Patel&title=GP-GAN%3A+Gender+Preserving+GAN+for+Synthesizing+Faces+from+Landmarks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0027\"><span><span class=\"hlFld-ContribAuthor\">Dong, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Kelkar</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Braun</span>. <span class=\"NLM_year\">2007</span>. \u201c<span class=\"NLM_chapter-title\">Getting the Most Out of Personas for Product Usability Enhancements</span>.\u201d In <i>Usability and Internationalization. HCI and Culture</i>, <span class=\"NLM_fpage\">291</span>\u2013<span class=\"NLM_lpage\">296</span>. <a class=\"ext-link\" href=\"http://www.springerlink.com/index/C0U2718G14HG1263.pdf\" target=\"_blank\">http://www.springerlink.com/index/C0U2718G14HG1263.pdf</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0027&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-540-73287-7_36\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=291-296&author=J.+Dong&author=K.+Kelkar&author=K.+Braun&title=Getting+the+Most+Out+of+Personas+for+Product+Usability+Enhancements\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0028\"><span><span class=\"hlFld-ContribAuthor\">dos Santos, <span class=\"NLM_given-names\">T. F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. G.</span> de Castro</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. A.</span> Masiero</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P. T. A.</span> Junior</span>. <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_chapter-title\">Behavioral Persona for Human-Robot Interaction: A Study Based on Pet Robot</span> Kurosu M. (eds) In. <i>International Conference on Human-Computer Interaction</i>, <span class=\"NLM_fpage\">687</span>\u2013<span class=\"NLM_lpage\">696</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0028&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-319-07230-2_65\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=687-696&author=T.+F.+dos+Santos&author=D.+G.+de+Castro&author=A.+A.+Masiero&author=P.+T.+A.+Junior&title=Behavioral+Persona+for+Human-Robot+Interaction%3A+A+Study+Based+on+Pet+Robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0029\"><span><span class=\"hlFld-ContribAuthor\">Duffy, <span class=\"NLM_given-names\">B. R.</span></span> <span class=\"NLM_year\">2003</span>. \u201c<span class=\"NLM_article-title\">Anthropomorphism and the Social Robot</span>.\u201d <i>Robotics and Autonomous Systems</i> 42 (3\u20134): <span class=\"NLM_fpage\">177</span>\u2013<span class=\"NLM_lpage\">190</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0029&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2FS0921-8890%2802%2900374-3\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0029&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000181591200004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=42&publication_year=2003&pages=177-190&issue=3%E2%80%934&author=B.+R.+Duffy&title=Anthropomorphism+and+the+Social+Robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0030\"><span><span class=\"hlFld-ContribAuthor\">Edwards, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Edwards</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P. R.</span> Spence</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Harris</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Gambino</span>. <span class=\"NLM_year\">2016</span>. \u201c<span class=\"NLM_article-title\">Robots in the Classroom: Differences in Students\u2019 Perceptions of Credibility and Learning Between \u2018Teacher as Robot\u2019 and \u2018Robot as Teacher\u2019</span>.\u201d <i>Computers in Human Behavior</i> 65: <span class=\"NLM_fpage\">627</span>\u2013<span class=\"NLM_lpage\">634</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.chb.2016.06.005</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0030&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.chb.2016.06.005\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0030&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000386986000067\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=65&publication_year=2016&pages=627-634&author=A.+Edwards&author=C.+Edwards&author=P.+R.+Spence&author=C.+Harris&author=A.+Gambino&title=Robots+in+the+Classroom%3A+Differences+in+Students%E2%80%99+Perceptions+of+Credibility+and+Learning+Between+%E2%80%98Teacher+as+Robot%E2%80%99+and+%E2%80%98Robot+as+Teacher%E2%80%99&doi=10.1016%2Fj.chb.2016.06.005\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0031\"><span><span class=\"hlFld-ContribAuthor\">Eslami, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. R.</span> Krishna Kumaran</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Sandvig</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Karahalios</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Communicating Algorithmic Process in Online Behavioral Advertising</span>.\u201d In <i>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">New York, NY, USA, Paper 432</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0031&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3173574.3174006\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=New+York%2C+NY%2C+USA%2C+Paper+432&author=M.+Eslami&author=S.+R.+Krishna+Kumaran&author=C.+Sandvig&author=K.+Karahalios&title=Communicating+Algorithmic+Process+in+Online+Behavioral+Advertising\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0032\"><span><span class=\"hlFld-ContribAuthor\">Faily, <span class=\"NLM_given-names\">S.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Flechais</span>. <span class=\"NLM_year\">2011</span>. \u201c<span class=\"NLM_chapter-title\">Persona Cases: A Technique for Grounding Personas</span>.\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">2267</span>\u2013<span class=\"NLM_lpage\">2270</span>. doi:<span class=\"NLM_pub-id\">10.1145/1978942.1979274</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0032&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F1978942.1979274\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=2267-2270&author=S.+Faily&author=I.+Flechais&title=Persona+Cases%3A+A+Technique+for+Grounding+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0033\"><span><span class=\"hlFld-ContribAuthor\">Friess, <span class=\"NLM_given-names\">E.</span></span> <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_chapter-title\">Personas and Decision Making in the Design Process: An Ethnographic Case Study</span>.\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">1209</span>\u2013<span class=\"NLM_lpage\">1218</span>. doi:<span class=\"NLM_pub-id\">10.1145/2207676.2208572</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0033&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2207676.2208572\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&pages=1209-1218&author=E.+Friess&title=Personas+and+Decision+Making+in+the+Design+Process%3A+An+Ethnographic+Case+Study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0034\"><span><span class=\"hlFld-ContribAuthor\">Gao, <span class=\"NLM_given-names\">F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Zhu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Jiang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Niu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Han</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Yu</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Incremental Focal Loss GANs</span>.\u201d <i>Information Processing &amp; Management</i> 57 (3): <span class=\"NLM_fpage\">102192</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ipm.2019.102192</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0034&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ipm.2019.102192\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0034&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000528550100024\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=57&publication_year=2020&pages=102192&issue=3&author=F.+Gao&author=J.+Zhu&author=H.+Jiang&author=Z.+Niu&author=W.+Han&author=J.+Yu&title=Incremental+Focal+Loss+GANs&doi=10.1016%2Fj.ipm.2019.102192\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0035\"><span><span class=\"hlFld-ContribAuthor\">Gecer, <span class=\"NLM_given-names\">B.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Bhattarai</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Kittler</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.-K.</span> Kim</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model</span>.\u201d In <i>Computer Vision \u2013 ECCV 2018. ECCV 2018 (Vol. 11215)</i>, <span class=\"NLM_fpage\">230</span>\u2013<span class=\"NLM_lpage\">248</span>. <span class=\"NLM_publisher-loc\">Cham</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-01252-6_14</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0035&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-01252-6_14\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=230-248&author=B.+Gecer&author=B.+Bhattarai&author=J.+Kittler&author=T.-K.+Kim&title=Semi-supervised+Adversarial+Learning+to+Generate+Photorealistic+Face+Images+of+New+Identities+from+3D+Morphable+Model\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0036\"><span><span class=\"hlFld-ContribAuthor\">Go, <span class=\"NLM_given-names\">E.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Shyam Sundar</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Humanizing Chatbots: The Effects of Visual, Identity and Conversational Cues on Humanness Perceptions</span>.\u201d <i>Computers in Human Behavior</i>. doi:<span class=\"NLM_pub-id\">10.1016/j.chb.2019.01.020</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0036&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.chb.2019.01.020\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0036&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000469154400030\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&author=E.+Go&author=S.+Shyam+Sundar&title=Humanizing+Chatbots%3A+The+Effects+of+Visual%2C+Identity+and+Conversational+Cues+on+Humanness+Perceptions&doi=10.1016%2Fj.chb.2019.01.020\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0037\"><span><span class=\"hlFld-ContribAuthor\">Goodfellow, <span class=\"NLM_given-names\">I.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Pouget-Abadie</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Mirza</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Xu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Warde-Farley</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Ozair</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Courville</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Bengio</span>. <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_chapter-title\">Generative Adversarial Nets</span>.\u201d In <i>Advances in Neural Information Processing Systems 27</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Ghahramani</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Welling</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Cortes</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N. D.</span> Lawrence</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. Q.</span> Weinberger</span>, <span class=\"NLM_fpage\">2672</span>\u2013<span class=\"NLM_lpage\">2680</span>. <span class=\"NLM_publisher-name\">Curran Associates, Inc</span>. <a class=\"ext-link\" href=\"http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\" target=\"_blank\">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=2672-2680&author=I.+Goodfellow&author=J.+Pouget-Abadie&author=M.+Mirza&author=B.+Xu&author=D.+Warde-Farley&author=S.+Ozair&author=A.+Courville&author=Y.+Bengio&title=Generative+Adversarial+Nets\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0038\"><span><span class=\"hlFld-ContribAuthor\">Goodwin, <span class=\"NLM_given-names\">K.</span></span> <span class=\"NLM_year\">2009</span>. <i>Designing for the Digital Age: How to Create Human-Centered Products and Services</i>. <span class=\"NLM_edition\">New York, New York. 1st ed.</span> <span class=\"NLM_publisher-name\">Wiley</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&author=K.+Goodwin&title=Designing+for+the+Digital+Age%3A+How+to+Create+Human-Centered+Products+and+Services\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0039\"><span><span class=\"hlFld-ContribAuthor\">Gwet, <span class=\"NLM_given-names\">K. L.</span></span> <span class=\"NLM_year\">2008</span>. \u201c<span class=\"NLM_article-title\">Computing Inter-Rater Reliability and Its Variance in the Presence of High Agreement</span>.\u201d <i>British Journal of Mathematical and Statistical Psychology</i> 61 (1): <span class=\"NLM_fpage\">29</span>\u2013<span class=\"NLM_lpage\">48</span>. doi:<span class=\"NLM_pub-id\">10.1348/000711006X126600</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0039&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1348%2F000711006X126600\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0039&amp;dbid=8&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=18482474\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=CIT0039&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000256524900002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=61&publication_year=2008&pages=29-48&issue=1&author=K.+L.+Gwet&title=Computing+Inter-Rater+Reliability+and+Its+Variance+in+the+Presence+of+High+Agreement&doi=10.1348%2F000711006X126600\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0040\"><span><span class=\"hlFld-ContribAuthor\">Hair, <span class=\"NLM_given-names\">J. F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W. C.</span> Black</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Babin</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. E.</span> Anderson</span>. <span class=\"NLM_year\">2009</span>. <i>Multivariate Data Analysis</i>. <span class=\"NLM_edition\">7th ed.</span> New York, New York. <span class=\"NLM_publisher-name\">Pearson</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&author=J.+F.+Hair&author=W.+C.+Black&author=B.+J.+Babin&author=R.+E.+Anderson&title=Multivariate+Data+Analysis\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0041\"><span><span class=\"hlFld-ContribAuthor\">Heusel, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Ramsauer</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Unterthiner</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Nessler</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Hochreiter</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_article-title\">Gans Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</span>.\u201d <i>Advances in Neural Information Processing Systems</i>, <span class=\"NLM_fpage\">6626</span>\u2013<span class=\"NLM_lpage\">6637</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=6626-6637&author=M.+Heusel&author=H.+Ramsauer&author=T.+Unterthiner&author=B.+Nessler&author=S.+Hochreiter&title=Gans+Trained+by+a+Two+Time-Scale+Update+Rule+Converge+to+a+Local+Nash+Equilibrium\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0042\"><span><span class=\"hlFld-ContribAuthor\">Hill, <span class=\"NLM_given-names\">C. G.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Haag</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Oleson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Mendez</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Marsden</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Sarma</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Burnett</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Gender-Inclusiveness Personas vs. Stereotyping: Can We Have It Both Ways?</span>\u201d In <i>Proceedings of the 2017 CHI Conference</i>, <span class=\"NLM_fpage\">6658</span>\u2013<span class=\"NLM_lpage\">6671</span>. doi:<span class=\"NLM_pub-id\">10.1145/3025453.3025609</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0042&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3025453.3025609\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=6658-6671&author=C.+G.+Hill&author=M.+Haag&author=A.+Oleson&author=C.+Mendez&author=N.+Marsden&author=A.+Sarma&author=M.+Burnett&title=Gender-Inclusiveness+Personas+vs.+Stereotyping%3A+Can+We+Have+It+Both+Ways%3F\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0043\"><span><span class=\"hlFld-ContribAuthor\">Holz, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Dragone</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G. M.</span> O\u2019Hare</span>. <span class=\"NLM_year\">2009</span>. \u201c<span class=\"NLM_article-title\">Where Robots and Virtual Agents Meet</span>.\u201d <i>International Journal of Social Robotics</i> 1 (1): <span class=\"NLM_fpage\">83</span>\u2013<span class=\"NLM_lpage\">93</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0043&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2Fs12369-008-0002-2\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0043&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000208892900008\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=2009&pages=83-93&issue=1&author=T.+Holz&author=M.+Dragone&author=G.+M.+O%E2%80%99Hare&title=Where+Robots+and+Virtual+Agents+Meet\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0044\"><span><span class=\"hlFld-ContribAuthor\">Hong, <span class=\"NLM_given-names\">B. B.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Bohemia</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Neubauer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Santamaria</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Designing for Users: The Global Studio</span>.\u201d In <i>DS 93: Proceedings of the 20th International Conference on Engineering and Product Design Education (E&amp;PDE 2018), Dyson School of Engineering, Imperial College, London. 6th-7th September 2018</i>, <span class=\"NLM_fpage\">738</span>\u2013<span class=\"NLM_lpage\">743</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=738-743&author=B.+B.+Hong&author=E.+Bohemia&author=R.+Neubauer&author=L.+Santamaria&title=Designing+for+Users%3A+The+Global+Studio\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0045\"><span><span class=\"hlFld-ContribAuthor\">Huang, <span class=\"NLM_given-names\">W.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Weber</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Vieweg</span>. <span class=\"NLM_year\">2014</span>. \u201cInferring Nationalities of Twitter Users and Studying Inter-National Linking.\u201d <i>ACM HyperText Conference</i>. <a class=\"ext-link\" href=\"https://works.bepress.com/vieweg/18/\" target=\"_blank\">https://works.bepress.com/vieweg/18/</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Huang%2C+W.%2C+I.+Weber%2C+and+S.+Vieweg.+2014.+%E2%80%9CInferring+Nationalities+of+Twitter+Users+and+Studying+Inter-National+Linking.%E2%80%9D+ACM+HyperText+Conference.+https%3A%2F%2Fworks.bepress.com%2Fvieweg%2F18%2F.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0046\"><span><span class=\"hlFld-ContribAuthor\">Idoughi, <span class=\"NLM_given-names\">D.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Seffah</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Kolski</span>. <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_article-title\">Adding User Experience into the Interactive Service Design Loop: A Persona-Based Approach</span>.\u201d <i>Behaviour &amp; Information Technology</i> 31 (3): <span class=\"NLM_fpage\">287</span>\u2013<span class=\"NLM_lpage\">303</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2011.563799</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0046&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2011.563799&amp;tollfreelink=2_18_e0030495c235aaeaf3bc43e532c419a5c00a4f3b3ea53f90f2c9d504a60f19db\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0046&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000300849400008\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=31&publication_year=2012&pages=287-303&issue=3&author=D.+Idoughi&author=A.+Seffah&author=C.+Kolski&title=Adding+User+Experience+into+the+Interactive+Service+Design+Loop%3A+A+Persona-Based+Approach&doi=10.1080%2F0144929X.2011.563799\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0047\"><span><span class=\"hlFld-ContribAuthor\">Iizuka, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Simo-Serra</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Ishikawa</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_article-title\">Globally and Locally Consistent Image Completion</span>.\u201d <i>ACM Transactions on Graphics</i> 36 (4): <span class=\"NLM_fpage\">107:1</span>\u2013<span class=\"NLM_lpage\">107:14</span>. doi:<span class=\"NLM_pub-id\">10.1145/3072959.3073659</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0047&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3072959.3073659\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0047&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000406432100075\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=36&publication_year=2017&pages=107%3A1-107%3A14&issue=4&author=S.+Iizuka&author=E.+Simo-Serra&author=H.+Ishikawa&title=Globally+and+Locally+Consistent+Image+Completion&doi=10.1145%2F3072959.3073659\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0048\"><span><span class=\"hlFld-ContribAuthor\">Isola, <span class=\"NLM_given-names\">P.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-Y.</span> Zhu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Zhou</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. A.</span> Efros</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Image-to-Image Translation with Conditional Adversarial Networks</span>.\u201d In <i>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, <span class=\"NLM_fpage\">IEEE, Honolulu, HI. 5967</span>\u2013<span class=\"NLM_lpage\">5976</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0048&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2017.632\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=IEEE%2C+Honolulu%2C+HI.+5967-5976&author=P.+Isola&author=J.-Y.+Zhu&author=T.+Zhou&author=A.+A.+Efros&title=Image-to-Image+Translation+with+Conditional+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0049\"><span><span class=\"hlFld-ContribAuthor\">Jansen, <span class=\"NLM_given-names\">B. J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. O.</span> Salminen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Data-Driven Personas for Enhanced User Understanding: Combining Empathy with Rationality for Better Insights to Analytics</span>.\u201d <i>Data and Information Management</i> 4 (1): <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">17</span>. doi:<span class=\"NLM_pub-id\">10.2478/dim-2020-0005</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0049&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.2478%2Fdim-2020-0005\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=4&publication_year=2020&pages=1-17&issue=1&author=B.+J.+Jansen&author=J.+O.+Salminen&author=S.-G.+Jung&title=Data-Driven+Personas+for+Enhanced+User+Understanding%3A+Combining+Empathy+with+Rationality+for+Better+Insights+to+Analytics&doi=10.2478%2Fdim-2020-0005\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0050\"><span><span class=\"hlFld-ContribAuthor\">Jansen, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Van Mechelen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Slegers</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Personas and Behavioral Theories: A Case Study Using Self-Determination Theory to Construct Overweight Personas</span>.\u201d In <i>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">2127</span>\u2013<span class=\"NLM_lpage\">2136</span>. doi:<span class=\"NLM_pub-id\">10.1145/3025453.3026003</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0050&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3025453.3026003\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=2127-2136&author=A.+Jansen&author=M.+Van+Mechelen&author=K.+Slegers&title=Personas+and+Behavioral+Theories%3A+A+Case+Study+Using+Self-Determination+Theory+to+Construct+Overweight+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0051\"><span><span class=\"hlFld-ContribAuthor\">Jeffreys, <span class=\"NLM_given-names\">H.</span></span> <span class=\"NLM_year\">1998</span>. <i>Theory of Probability</i>. <span class=\"NLM_edition\">3rd ed.</span> <span class=\"NLM_publisher-name\">Oxford: Oxford University Press</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=1998&author=H.+Jeffreys&title=Theory+of+Probability\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0052\"><span><span class=\"hlFld-ContribAuthor\">Jenkinson, <span class=\"NLM_given-names\">A.</span></span> <span class=\"NLM_year\">1994</span>. \u201c<span class=\"NLM_article-title\">Beyond Segmentation</span>.\u201d <i>Journal of Targeting, Measurement and Analysis for Marketing</i> 3 (1): <span class=\"NLM_fpage\">60</span>\u2013<span class=\"NLM_lpage\">72</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=3&publication_year=1994&pages=60-72&issue=1&author=A.+Jenkinson&title=Beyond+Segmentation\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0053\"><span><span class=\"hlFld-ContribAuthor\">Jung, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018a</span>. \u201c<span class=\"NLM_chapter-title\">Automatically Conceptualizing Social Media Analytics Data via Personas</span>.\u201d In <i>Proceedings of the International AAAI Conference on Web and Social Media (ICWSM 2018), June 25</i>. International AAAI Conference on Web and Social Media (ICWSM 2018), San Francisco, California, USA.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018a&author=S.+Jung&author=J.+Salminen&author=J.+An&author=H.+Kwak&author=B.+J.+Jansen&title=Automatically+Conceptualizing+Social+Media+Analytics+Data+via+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0054\"><span><span class=\"hlFld-ContribAuthor\">Jung, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018b</span>. \u201c<span class=\"NLM_chapter-title\">Automatic Persona Generation (APG): A Rationale and Demonstration</span>.\u201d In <i>Proceedings of the ACM, 2018 Conference on Human Information Interaction &amp; Retrieval</i>, ACM, New Brunswick, NJ., <span class=\"NLM_fpage\">321</span>\u2013<span class=\"NLM_lpage\">324</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0054&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3176349.3176893\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018b&pages=321-324&author=S.+Jung&author=J.+Salminen&author=H.+Kwak&author=J.+An&author=B.+J.+Jansen&title=Automatic+Persona+Generation+%28APG%29%3A+A+Rationale+and+Demonstration\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0055\"><span><span class=\"hlFld-ContribAuthor\">Karras, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Laine</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Aila</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">A Style-Based Generator Architecture for Generative Adversarial Networks</span>.\u201d In <i>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, <span class=\"NLM_fpage\">4396</span>\u2013<span class=\"NLM_lpage\">4405</span>. doi:<span class=\"NLM_pub-id\">10.1109/CVPR.2019.00453</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0055&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2019.00453\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=4396-4405&author=T.+Karras&author=S.+Laine&author=T.+Aila&title=A+Style-Based+Generator+Architecture+for+Generative+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0056\"><span><span class=\"hlFld-ContribAuthor\">King, <span class=\"NLM_given-names\">A. J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. J.</span> Lazard</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. R.</span> White</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">The Influence of Visual Complexity on Initial User Impressions: Testing the Persuasive Model of Web Design</span>.\u201d <i>Behaviour &amp; Information Technology</i> 39 (5): <span class=\"NLM_fpage\">497</span>\u2013<span class=\"NLM_lpage\">510</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1602167</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0056&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1602167&amp;tollfreelink=2_18_ba37b055a7024e5d28beab14ac2ff075170172009f68d0a1577d775393707d08\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0056&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000465783000001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=39&publication_year=2020&pages=497-510&issue=5&author=A.+J.+King&author=A.+J.+Lazard&author=S.+R.+White&title=The+Influence+of+Visual+Complexity+on+Initial+User+Impressions%3A+Testing+the+Persuasive+Model+of+Web+Design&doi=10.1080%2F0144929X.2019.1602167\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0057\"><span><span class=\"hlFld-ContribAuthor\">Lee, <span class=\"NLM_given-names\">M. D.</span></span> <span class=\"NLM_year\">2014</span>. <i>Bayesian Cognitive Modeling: A Practical Course</i>.<span class=\"NLM_publisher-name\">Cambridge: Cambridge University Press</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=M.+D.+Lee&title=Bayesian+Cognitive+Modeling%3A+A+Practical+Course\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0058\"><span><span class=\"hlFld-ContribAuthor\">Lee, <span class=\"NLM_given-names\">H.-Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.-Y.</span> Tseng</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-B.</span> Huang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. K.</span> Singh</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.-H.</span> Yang</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Diverse Image-to-Image Translation via Disentangled Representations</span>.\u201d In <i>Computer Vision \u2013 ECCV 2018. ECCV 2018 (Vol. 11205)</i>. <span class=\"NLM_publisher-loc\">Cham</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-01246-5_3</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0058&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-01246-5_3\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&author=H.-Y.+Lee&author=H.-Y.+Tseng&author=J.-B.+Huang&author=M.+K.+Singh&author=M.-H.+Yang&title=Diverse+Image-to-Image+Translation+via+Disentangled+Representations\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0059\"><span><span class=\"hlFld-ContribAuthor\">Li, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Qian</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Dong</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Liu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Q.</span> Yan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Zhu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Lin</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">BeautyGAN: Instance-Level Facial Makeup Transfer with Deep Generative Adversarial Network</span>.\u201d In <i>Proceedings of the 26th ACM International Conference on Multimedia</i>, <span class=\"NLM_fpage\">645</span>\u2013<span class=\"NLM_lpage\">653</span>. doi:<span class=\"NLM_pub-id\">10.1145/3240508.3240618</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0059&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3240508.3240618\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=645-653&author=T.+Li&author=R.+Qian&author=C.+Dong&author=S.+Liu&author=Q.+Yan&author=W.+Zhu&author=L.+Lin&title=BeautyGAN%3A+Instance-Level+Facial+Makeup+Transfer+with+Deep+Generative+Adversarial+Network\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0060\"><span><span class=\"hlFld-ContribAuthor\">Lin, <span class=\"NLM_given-names\">C. H.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.-C.</span> Chang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.-S.</span> Chen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.-C.</span> Juan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Wei</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.-T.</span> Chen</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">COCO-GAN: Generation by Parts via Conditional Coordinating</span>.\u201d In <i>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</i>, <span class=\"NLM_fpage\">4511</span>\u2013<span class=\"NLM_lpage\">4520</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICCV.2019.00461</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0060&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICCV.2019.00461\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=4511-4520&author=C.+H.+Lin&author=C.-C.+Chang&author=Y.-S.+Chen&author=D.-C.+Juan&author=W.+Wei&author=H.-T.+Chen&title=COCO-GAN%3A+Generation+by+Parts+via+Conditional+Coordinating\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0061\"><span><span class=\"hlFld-ContribAuthor\">Liu, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Qin</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Wan</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Luo</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_article-title\">Auto-painter: Cartoon Image Generation from Sketch by Using Conditional Wasserstein Generative Adversarial Networks</span>.\u201d <i>Neurocomputing</i> 311: <span class=\"NLM_fpage\">78</span>\u2013<span class=\"NLM_lpage\">87</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.neucom.2018.05.045</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0061&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.neucom.2018.05.045\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0061&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000438313100008\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=311&publication_year=2018&pages=78-87&author=Y.+Liu&author=Z.+Qin&author=T.+Wan&author=Z.+Luo&title=Auto-painter%3A+Cartoon+Image+Generation+from+Sketch+by+Using+Conditional+Wasserstein+Generative+Adversarial+Networks&doi=10.1016%2Fj.neucom.2018.05.045\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0062\"><span><span class=\"hlFld-ContribAuthor\">Liu, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Sun</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Zhu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Bao</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Wang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Shu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Yan</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Face Aging with Contextual Generative Adversarial Nets</span>.\u201d In <i>Proceedings of the 25th ACM International Conference on Multimedia</i>, <span class=\"NLM_fpage\">82</span>\u2013<span class=\"NLM_lpage\">90</span>. doi:<span class=\"NLM_pub-id\">10.1145/3123266.3123431</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0062&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3123266.3123431\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=82-90&author=S.+Liu&author=Y.+Sun&author=D.+Zhu&author=R.+Bao&author=W.+Wang&author=X.+Shu&author=S.+Yan&title=Face+Aging+with+Contextual+Generative+Adversarial+Nets\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0063\"><span><span class=\"hlFld-ContribAuthor\">Long, <span class=\"NLM_given-names\">F.</span></span> <span class=\"NLM_year\">2009</span>. \u201c<span class=\"NLM_chapter-title\">Real or Imaginary: The Effectiveness of Using Personas in Product Design</span>.\u201d In <i>Proceedings of the Irish Ergonomics Society Annual Conference</i>, <span class=\"NLM_fpage\">Dublin, IR., 14</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=Dublin%2C+IR.%2C+14&author=F.+Long&title=Real+or+Imaginary%3A+The+Effectiveness+of+Using+Personas+in+Product+Design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0064\"><span><span class=\"hlFld-ContribAuthor\">Lu, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.-W.</span> Tai</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.-K.</span> Tang</span>. <span class=\"NLM_year\">2017</span>. \u201cConditional Cyclegan for Attribute Guided Face Image Generation.\u201d <i>ArXiv Preprint ArXiv:1705.09966</i>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Lu%2C+Y.%2C+Y.-W.+Tai%2C+and+C.-K.+Tang.+2017.+%E2%80%9CConditional+Cyclegan+for+Attribute+Guided+Face+Image+Generation.%E2%80%9D+ArXiv+Preprint+ArXiv%3A1705.09966.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0065\"><span><span class=\"hlFld-ContribAuthor\">Matthews, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Judge</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Whittaker</span>. <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_chapter-title\">How Do Designers and User Experience Professionals Actually Perceive and Use Personas?</span>\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">1219</span>\u2013<span class=\"NLM_lpage\">1228</span>. doi:<span class=\"NLM_pub-id\">10.1145/2207676.2208573</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0065&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2207676.2208573\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&pages=1219-1228&author=T.+Matthews&author=T.+Judge&author=S.+Whittaker&title=How+Do+Designers+and+User+Experience+Professionals+Actually+Perceive+and+Use+Personas%3F\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0066\"><span><span class=\"hlFld-ContribAuthor\">McGinn, <span class=\"NLM_given-names\">J. J.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Kotamraju</span>. <span class=\"NLM_year\">2008</span>. \u201c<span class=\"NLM_chapter-title\">Data-Driven Persona Development</span>.\u201d In <i>ACM Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">ACM, New York, NY, USA, 1521</span>\u2013<span class=\"NLM_lpage\">1524</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0066&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F1357054.1357292\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=ACM%2C+New+York%2C+NY%2C+USA%2C+1521-1524&author=J.+J.+McGinn&author=N.+Kotamraju&title=Data-Driven+Persona+Development\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0067\"><span><span class=\"hlFld-ContribAuthor\">Neumann, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Pyromallis</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Alexander</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Evolution of Images with Diversity and Constraints Using a Generative Adversarial Network</span>.\u201dCheng L., Leung A., Ozawa S. (eds). In <i>International Conference on Neural Information Processing</i>, <span class=\"NLM_fpage\">452</span>\u2013<span class=\"NLM_lpage\">465</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0067&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-04224-0_39\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=452-465&author=A.+Neumann&author=C.+Pyromallis&author=B.+Alexander&title=Evolution+of+Images+with+Diversity+and+Constraints+Using+a+Generative+Adversarial+Network\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0068\"><span><span class=\"hlFld-ContribAuthor\">Nie, <span class=\"NLM_given-names\">D.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Trullo</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Lian</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Petitjean</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Ruan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Q.</span> Wang</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Shen</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Medical Image Synthesis with Context-Aware Generative Adversarial Networks</span>.\u201dDescoteaux M., Maier-Hein L., Franz A., Jannin P., Collins D., Duchesne S. (eds). In <i>International Conference on Medical Image Computing and Computer-Assisted Intervention</i>, <span class=\"NLM_fpage\">417</span>\u2013<span class=\"NLM_lpage\">425</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0068&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-319-66179-7_48\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=417-425&author=D.+Nie&author=R.+Trullo&author=J.+Lian&author=C.+Petitjean&author=S.+Ruan&author=Q.+Wang&author=D.+Shen&title=Medical+Image+Synthesis+with+Context-Aware+Generative+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0069\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span> <span class=\"NLM_year\">2019</span>. <i>Personas\u2014User Focused Design</i>. <span class=\"NLM_edition\">2nd ed.</span> <span class=\"NLM_publisher-name\">Springer</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0069&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-1-4471-7427-1\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&author=L.+Nielsen&title=Personas%E2%80%94User+Focused+Design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0070\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. S.</span> Hansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Stage</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Billestrup</span>. <span class=\"NLM_year\">2015</span>. \u201c<span class=\"NLM_article-title\">A Template for Design Personas: Analysis of 47 Persona Descriptions from Danish Industries and Organizations</span>.\u201d <i>International Journal of Sociotechnology and Knowledge Development</i> 7 (1): <span class=\"NLM_fpage\">45</span>\u2013<span class=\"NLM_lpage\">61</span>. doi:<span class=\"NLM_pub-id\">10.4018/ijskd.2015010104</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0070&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.4018%2Fijskd.2015010104\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=7&publication_year=2015&pages=45-61&issue=1&author=L.+Nielsen&author=K.+S.+Hansen&author=J.+Stage&author=J.+Billestrup&title=A+Template+for+Design+Personas%3A+Analysis+of+47+Persona+Descriptions+from+Danish+Industries+and+Organizations&doi=10.4018%2Fijskd.2015010104\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0071\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Who Are Your Users?: Comparing Media Professionals\u2019 Preconception of Users to Data-Driven Personas</span>.\u201d In <i>Proceedings of the 29th Australian Conference on Computer-Human Interaction</i>, <span class=\"NLM_fpage\">602</span>\u2013<span class=\"NLM_lpage\">606</span>. doi:<span class=\"NLM_pub-id\">10.1145/3152771.3156178</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0071&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3152771.3156178\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=602-606&author=L.+Nielsen&author=S.-G.+Jung&author=J.+An&author=J.+Salminen&author=H.+Kwak&author=B.+J.+Jansen&title=Who+Are+Your+Users%3F%3A+Comparing+Media+Professionals%E2%80%99+Preconception+of+Users+to+Data-Driven+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0072\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Storgaard Hansen</span>. <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_chapter-title\">Personas Is Applicable: A Study on the Use of Personas in Denmark</span>.\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">ACM, New York, NY.. 1665</span>\u2013<span class=\"NLM_lpage\">1674</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0072&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2556288.2557080\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=ACM%2C+New+York%2C+NY..+1665-1674&author=L.+Nielsen&author=K.+Storgaard+Hansen&title=Personas+Is+Applicable%3A+A+Study+on+the+Use+of+Personas+in+Denmark\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0073\"><span><span class=\"hlFld-ContribAuthor\">\u00d6zmen, <span class=\"NLM_given-names\">M. U.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Yucel</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Handling of Online Information by Users: Evidence from TED Talks</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1309</span>\u2013<span class=\"NLM_lpage\">1323</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1584244</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0073&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1584244&amp;tollfreelink=2_18_a5e4c4277f85c350fbaa76e5abe368745e9ba9adacefeaf6da7827e143b803d5\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0073&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200009\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1309-1323&issue=12&author=M.+U.+%C3%96zmen&author=E.+Yucel&title=Handling+of+Online+Information+by+Users%3A+Evidence+from+TED+Talks&doi=10.1080%2F0144929X.2019.1584244\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0074\"><span><span class=\"hlFld-ContribAuthor\">Palan, <span class=\"NLM_given-names\">S.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Schitter</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_article-title\">Prolific. Ac\u2014A Subject Pool for Online Experiments</span>.\u201d <i>Journal of Behavioral and Experimental Finance</i> 17: <span class=\"NLM_fpage\">22</span>\u2013<span class=\"NLM_lpage\">27</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0074&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.jbef.2017.12.004\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0074&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000427997000004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=17&publication_year=2018&pages=22-27&author=S.+Palan&author=C.+Schitter&title=Prolific.+Ac%E2%80%94A+Subject+Pool+for+Online+Experiments\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0075\"><span><span class=\"hlFld-ContribAuthor\">Pitk\u00e4nen, <span class=\"NLM_given-names\">L.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>. <span class=\"NLM_year\">2013</span>. \u201c<span class=\"NLM_chapter-title\">Managing the Crowd: A Study on Videography Application</span>.\u201d In <i>Proceedings of Applied Business and Entrepreneurship Association International (ABEAI), November</i>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&author=L.+Pitk%C3%A4nen&author=J.+Salminen&title=Managing+the+Crowd%3A+A+Study+on+Videography+Application\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0076\"><span><span class=\"hlFld-ContribAuthor\">Probster, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. E.</span> Haque</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Marsden</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Perceptions of Personas: The Role of Instructions</span>.\u201d In <i>2018 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)</i>, <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">8</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICE.2018.8436339</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0076&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICE.2018.8436339\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=1-8&author=M.+Probster&author=M.+E.+Haque&author=N.+Marsden&title=Perceptions+of+Personas%3A+The+Role+of+Instructions\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0077\"><span><span class=\"hlFld-ContribAuthor\">Pruitt, <span class=\"NLM_given-names\">J.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Adlin</span>. <span class=\"NLM_year\">2006</span>. <i>The Persona Lifecycle: Keeping People in Mind Throughout Product Design</i>. <span class=\"NLM_edition\">1st ed.</span> <span class=\"NLM_publisher-name\">Morgan Kaufmann</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&author=J.+Pruitt&author=T.+Adlin&title=The+Persona+Lifecycle%3A+Keeping+People+in+Mind+Throughout+Product+Design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0078\"><span><span class=\"hlFld-ContribAuthor\">R\u00f6nkk\u00f6, <span class=\"NLM_given-names\">K.</span></span> <span class=\"NLM_year\">2005</span>. \u201c<span class=\"NLM_chapter-title\">An Empirical Study Demonstrating How Different Design Constraints, Project Organization and Contexts Limited the Utility of Personas</span>.\u201d In <i>Proceedings of the Proceedings of the 38th Annual Hawaii International Conference on System Sciences \u2013 Volume 08</i>. doi:<span class=\"NLM_pub-id\">10.1109/HICSS.2005.85</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0078&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FHICSS.2005.85\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&author=K.+R%C3%B6nkk%C3%B6&title=An+Empirical+Study+Demonstrating+How+Different+Design+Constraints%2C+Project+Organization+and+Contexts+Limited+the+Utility+of+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0079\"><span><span class=\"hlFld-ContribAuthor\">R\u00f6nkk\u00f6, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Hellman</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Kilander</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Dittrich</span>. <span class=\"NLM_year\">2004</span>. \u201c<span class=\"NLM_chapter-title\">Personas Is Not Applicable: Local Remedies Interpreted in a Wider Context</span>.\u201d In <i>Proceedings of the Eighth Conference on Participatory Design: Artful Integration: Interweaving Media, Materials and Practices \u2013 Volume 1</i>, <span class=\"NLM_fpage\">112</span>\u2013<span class=\"NLM_lpage\">120</span>. doi:<span class=\"NLM_pub-id\">10.1145/1011870.1011884</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0079&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F1011870.1011884\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=112-120&author=K.+R%C3%B6nkk%C3%B6&author=M.+Hellman&author=B.+Kilander&author=Y.+Dittrich&title=Personas+Is+Not+Applicable%3A+Local+Remedies+Interpreted+in+a+Wider+Context\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0080\"><span><span class=\"hlFld-ContribAuthor\">Salimans, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Goodfellow</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Zaremba</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V.</span> Cheung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Radford</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Chen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Chen</span>. <span class=\"NLM_year\">2016</span>. \u201c<span class=\"NLM_chapter-title\">Improved Techniques for Training GANs</span>.\u201d In <i>Advances in Neural Information Processing Systems 29</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. D.</span> Lee</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Sugiyama</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">U. V.</span> Luxburg</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Guyon</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Garnett</span>, <span class=\"NLM_fpage\">2234</span>\u2013<span class=\"NLM_lpage\">2242</span>. <span class=\"NLM_publisher-name\">Curran Associates, Inc</span>. <a class=\"ext-link\" href=\"http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf\" target=\"_blank\">http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2016&pages=2234-2242&author=T.+Salimans&author=I.+Goodfellow&author=W.+Zaremba&author=V.+Cheung&author=A.+Radford&author=X.+Chen&author=X.+Chen&title=Improved+Techniques+for+Training+GANs\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0081\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Almerekhi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Dey</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018a</span>. \u201c<span class=\"NLM_chapter-title\">Inter-Rater Agreement for Social Computing Studies</span>.\u201d In <i>Proceedings of The Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS \u2013 2018), October 15</i>. The Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS \u2013 2018), Valencia, Spain.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0081&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FSNAMS.2018.8554744\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018a&author=J.+Salminen&author=H.+Almerekhi&author=P.+Dey&author=B.+J.+Jansen&title=Inter-Rater+Agreement+for+Social+Computing+Studies\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0082\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>. <span class=\"NLM_year\">2018b</span>. \u201c<span class=\"NLM_article-title\">Are Personas Done? Evaluating Their Usefulness in the Age of Digital Analytics</span>.\u201d <i>Persona Studies</i> 4 (2): <span class=\"NLM_fpage\">47</span>\u2013<span class=\"NLM_lpage\">65</span>. doi:<span class=\"NLM_pub-id\">10.21153/psj2018vol4no2art737</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0082&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.21153%2Fpsj2018vol4no2art737\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=4&publication_year=2018b&pages=47-65&issue=2&author=J.+Salminen&author=B.+J.+Jansen&author=J.+An&author=H.+Kwak&author=S.+Jung&title=Are+Personas+Done%3F+Evaluating+Their+Usefulness+in+the+Age+of+Digital+Analytics&doi=10.21153%2Fpsj2018vol4no2art737\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0083\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>. <span class=\"NLM_year\">2019a</span>. \u201c<span class=\"NLM_chapter-title\">Automatic Persona Generation for Online Content Creators: Conceptual Rationale and a Research Agenda</span>.\u201d In <i>Personas\u2014User Focused Design</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Nielsen</span>, <span class=\"NLM_fpage\">135</span>\u2013<span class=\"NLM_lpage\">160</span>. <span class=\"NLM_publisher-loc\">London</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-1-4471-7427-1_8</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0083&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-1-4471-7427-1_8\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019a&pages=135-160&author=J.+Salminen&author=B.+J.+Jansen&author=J.+An&author=H.+Kwak&author=S.-G.+Jung&title=Automatic+Persona+Generation+for+Online+Content+Creators%3A+Conceptual+Rationale+and+a+Research+Agenda\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0084\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Nielsen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019b</span>. \u201c<span class=\"NLM_article-title\">Confusion and Information Triggered by Photos in Persona Profiles</span>.\u201d <i>International Journal of Human-Computer Studies</i> 129: <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">14</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ijhcs.2019.03.005</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0084&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ijhcs.2019.03.005\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0084&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000472687700001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=129&publication_year=2019b&pages=1-14&author=J.+Salminen&author=S.+Jung&author=J.+An&author=H.+Kwak&author=L.+Nielsen&author=B.+J.+Jansen&title=Confusion+and+Information+Triggered+by+Photos+in+Persona+Profiles&doi=10.1016%2Fj.ijhcs.2019.03.005\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0085\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019c</span>. \u201c<span class=\"NLM_chapter-title\">Detecting Demographic Bias in Automatically Generated Personas</span>.\u201d In <i>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">LBW0122:1</span>\u2013<span class=\"NLM_lpage\">LBW0122:6</span>. doi:<span class=\"NLM_pub-id\">10.1145/3290607.3313034</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0085&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3290607.3313034\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019c&pages=LBW0122%3A1-LBW0122%3A6&author=J.+Salminen&author=S.-G.+Jung&author=B.+J.+Jansen&title=Detecting+Demographic+Bias+in+Automatically+Generated+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0086\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. G.</span> Jung</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019d</span>. \u201c<span class=\"NLM_chapter-title\">The Future of Data-Driven Personas: A Marriage of Online Analytics Numbers and Human Attributes</span>.\u201d In <i>ICEIS 2019 \u2013 Proceedings of the 21st International Conference on Enterprise Information Systems</i>, <span class=\"NLM_fpage\">596</span>\u2013<span class=\"NLM_lpage\">603</span>. <a class=\"ext-link\" href=\"https://pennstate.pure.elsevier.com/en/publications/the-future-of-data-driven-personas-a-marriage-of-online-analytics\" target=\"_blank\">https://pennstate.pure.elsevier.com/en/publications/the-future-of-data-driven-personas-a-marriage-of-online-analytics</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0086&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.5220%2F0007744706080615\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019d&pages=596-603&author=J.+Salminen&author=S.+G.+Jung&author=B.+J.+Jansen&title=The+Future+of+Data-Driven+Personas%3A+A+Marriage+of+Online+Analytics+Numbers+and+Human+Attributes\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0087\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019e</span>. \u201c<span class=\"NLM_chapter-title\">The Effect of Smiling Pictures on Perceptions of Personas</span>.\u201d In <i>UMAP\u201919 Adjunct: Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization</i>. doi:<span class=\"NLM_pub-id\">10.1145/3314183.3324973</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0087&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3314183.3324973\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019e&author=J.+Salminen&author=S.-G.+Jung&author=J.+M.+Santos&author=B.+J.+Jansen&title=The+Effect+of+Smiling+Pictures+on+Perceptions+of+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0088\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019f</span>. \u201c<span class=\"NLM_article-title\">Does a Smile Matter if the Person Is Not Real?: The Effect of a Smile and Stock Photos on Persona Perceptions</span>.\u201d <i>International Journal of Human\u2013Computer Interaction</i> 0 (0): <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">23</span>. doi:<span class=\"NLM_pub-id\">10.1080/10447318.2019.1664068</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0088&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F10447318.2019.1664068&amp;tollfreelink=2_18_a0332bf1d9c1df8b5aabef314d6cb14b7620b3d836adee079ccd7dbe4924dcd3\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=0&publication_year=2019f&pages=1-23&issue=0&author=J.+Salminen&author=S.-G.+Jung&author=J.+M.+Santos&author=B.+J.+Jansen&title=Does+a+Smile+Matter+if+the+Person+Is+Not+Real%3F%3A+The+Effect+of+a+Smile+and+Stock+Photos+on+Persona+Perceptions&doi=10.1080%2F10447318.2019.1664068\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0089\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018c</span>. \u201c<span class=\"NLM_chapter-title\">Persona Perception Scale: Developing and Validating an Instrument for Human-Like Representations of Data</span>.\u201d In <i>CHI\u201918 Extended Abstracts: CHI Conference on Human Factors in Computing Systems Extended Abstracts Proceedings</i>. CHI 2018 Extended Abstracts, Montr\u00e9al, Canada. doi:<span class=\"NLM_pub-id\">10.1145/3170427.3188461</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0089&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3170427.3188461\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018c&author=J.+Salminen&author=H.+Kwak&author=J.+M.+Santos&author=S.-G.+Jung&author=J.+An&author=B.+J.+Jansen&title=Persona+Perception+Scale%3A+Developing+and+Validating+an+Instrument+for+Human-Like+Representations+of+Data\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0090\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Nielsen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018d</span>. \u201c<span class=\"NLM_chapter-title\">\u2018Is More Better?\u2019: Impact of Multiple Photos on Perception of Persona Profiles</span>.\u201d In <i>Proceedings of ACM CHI Conference on Human Factors in Computing Systems (CHI2018), April 21</i>. ACM CHI Conference on Human Factors in Computing Systems (CHI2018), ACM, Montr\u00e9al, Canada. Paper 317.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0090&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3173574.3173891\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018d&author=J.+Salminen&author=L.+Nielsen&author=S.-G.+Jung&author=J.+An&author=H.+Kwak&author=B.+J.+Jansen&title=%E2%80%98Is+More+Better%3F%E2%80%99%3A+Impact+of+Multiple+Photos+on+Perception+of+Persona+Profiles\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0091\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Eslami</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019g</span>. \u201c<span class=\"NLM_article-title\">Persona Transparency: Analyzing the Impact of Explanations on Perceptions of Data-Driven Personas</span>.\u201d <i>International Journal of Human\u2013Computer Interaction</i> 0 (0): <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">13</span>. doi:<span class=\"NLM_pub-id\">10.1080/10447318.2019.1688946</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0091&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F10447318.2019.1688946&amp;tollfreelink=2_18_26ba6235947b0ff6baa1ec4656b5d6ca46f3fc6043f40ca45417f4bc50784fb7\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=0&publication_year=2019g&pages=1-13&issue=0&author=J.+Salminen&author=J.+M.+Santos&author=S.-G.+Jung&author=M.+Eslami&author=B.+J.+Jansen&title=Persona+Transparency%3A+Analyzing+the+Impact+of+Explanations+on+Perceptions+of+Data-Driven+Personas&doi=10.1080%2F10447318.2019.1688946\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0092\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> \u015eeng\u00fcn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Vieweg</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Harrell</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Generating Cultural Personas from Social Data: A Perspective of Middle Eastern Users</span>.\u201d In <i>Proceedings of The Fourth International Symposium on Social Networks Analysis, Management and Security (SNAMS-2017)</i>. The Fourth International Symposium on Social Networks Analysis, Management and Security (SNAMS-2017), IEEE. Prague, Czech Republic, 1-8.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0092&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FFiCloudW.2017.97\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&author=J.+Salminen&author=S.+%C5%9Eeng%C3%BCn&author=H.+Kwak&author=B.+J.+Jansen&author=J.+An&author=S.+Jung&author=S.+Vieweg&author=F.+Harrell&title=Generating+Cultural+Personas+from+Social+Data%3A+A+Perspective+of+Middle+Eastern+Users\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0093\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> \u015eeng\u00fcn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Vieweg</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Harrell</span>. <span class=\"NLM_year\">2018e</span>. \u201c<span class=\"NLM_article-title\">From 2,772 Segments to Five Personas: Summarizing a Diverse Online Audience by Generating Culturally Adapted Personas</span>.\u201d <i>First Monday</i> 23 (6). doi:<span class=\"NLM_pub-id\">10.5210/fm.v23i6.8415</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0093&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.5210%2Ffm.v23i6.8415\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=23&publication_year=2018e&issue=6&author=J.+Salminen&author=S.+%C5%9Eeng%C3%BCn&author=H.+Kwak&author=B.+J.+Jansen&author=J.+An&author=S.+Jung&author=S.+Vieweg&author=F.+Harrell&title=From+2%2C772+Segments+to+Five+Personas%3A+Summarizing+a+Diverse+Online+Audience+by+Generating+Culturally+Adapted+Personas&doi=10.5210%2Ffm.v23i6.8415\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0094\"><span><span class=\"hlFld-ContribAuthor\">\u015eeng\u00fcn, <span class=\"NLM_given-names\">S.</span></span> <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_article-title\">A Semiotic Reading of Digital Avatars and Their Role of Uncertainty Reduction in Digital Communication</span>.\u201d <i>Journal of Media Critiques</i> 1 (Special): <span class=\"NLM_fpage\">149</span>\u2013<span class=\"NLM_lpage\">162</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0094&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.17349%2Fjmc114311\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=2014&pages=149-162&issue=Special&author=S.+%C5%9Eeng%C3%BCn&title=A+Semiotic+Reading+of+Digital+Avatars+and+Their+Role+of+Uncertainty+Reduction+in+Digital+Communication\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0095\"><span><span class=\"hlFld-ContribAuthor\">Seng\u00fcn, <span class=\"NLM_given-names\">S.</span></span> <span class=\"NLM_year\">2015</span>. \u201c<span class=\"NLM_article-title\">Why Do I Fall for the Elf, When I am No Orc Myself? The Impl\u0131cat\u0131ons of V\u0131rtual Avatars \u0131n D\u0131g\u0131tal Commun\u0131cat\u0131on</span>.\u201d <i>Comunica\u00e7\u00e3o e Sociedade</i> 27: <span class=\"NLM_fpage\">181</span>\u2013<span class=\"NLM_lpage\">193</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0095&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.17231%2Fcomsoc.27%282015%29.2096\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=27&publication_year=2015&pages=181-193&author=S.+Seng%C3%BCn&title=Why+Do+I+Fall+for+the+Elf%2C+When+I+am+No+Orc+Myself%3F+The+Impl%C4%B1cat%C4%B1ons+of+V%C4%B1rtual+Avatars+%C4%B1n+D%C4%B1g%C4%B1tal+Commun%C4%B1cat%C4%B1on\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0096\"><span><span class=\"hlFld-ContribAuthor\">Shmelkov, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Schmid</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Alahari</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">How Good Is My GAN?</span>\u201d In <i>Computer Vision \u2013 ECCV 2018</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V.</span> Ferrari</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Hebert</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Sminchisescu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Weiss</span>, <span class=\"NLM_fpage\">218</span>\u2013<span class=\"NLM_lpage\">234</span>. <span class=\"NLM_publisher-name\">Springer International Publishing</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-01216-8_14</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0096&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-01216-8_14\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=218-234&author=K.+Shmelkov&author=C.+Schmid&author=K.+Alahari&title=How+Good+Is+My+GAN%3F\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0097\"><span><span class=\"hlFld-ContribAuthor\">Shmueli-Scheuer, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Sandbank</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Konopnicki</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">O. P.</span> Nakash</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Exploring the Universe of Egregious Conversations in Chatbots</span>.\u201d In <i>Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion</i>, <span class=\"NLM_fpage\">ACM, New York, NY, Article 16</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0097&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3180308.3180324\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=ACM%2C+New+York%2C+NY%2C+Article+16&author=M.+Shmueli-Scheuer&author=T.+Sandbank&author=D.+Konopnicki&author=O.+P.+Nakash&title=Exploring+the+Universe+of+Egregious+Conversations+in+Chatbots\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0098\"><span><span class=\"hlFld-ContribAuthor\">Song, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Li</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Wang</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Qi</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">Talking Face Generation by Conditional Recurrent Adversarial Network</span>.\u201d In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, <span class=\"NLM_fpage\">Macao, China. 919</span>\u2013<span class=\"NLM_lpage\">925</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0098&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.24963%2Fijcai.2019%2F129\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=Macao%2C+China.+919-925&author=Y.+Song&author=D.+Li&author=A.+Wang&author=H.+Qi&title=Talking+Face+Generation+by+Conditional+Recurrent+Adversarial+Network\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0099\"><span><span class=\"hlFld-ContribAuthor\">Tan, <span class=\"NLM_given-names\">W. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C. S.</span> Chan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H. E.</span> Aguirre</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Tanaka</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">ArtGAN: Artwork Synthesis with Conditional Categorical GANs</span>.\u201d In <i>2017 IEEE International Conference on Image Processing (ICIP)</i>, <span class=\"NLM_fpage\">IEEE. 3760</span>\u2013<span class=\"NLM_lpage\">3764</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0099&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICIP.2017.8296985\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=IEEE.+3760-3764&author=W.+R.+Tan&author=C.+S.+Chan&author=H.+E.+Aguirre&author=K.+Tanaka&title=ArtGAN%3A+Artwork+Synthesis+with+Conditional+Categorical+GANs\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0100\"><span><span class=\"hlFld-ContribAuthor\">Weiss, <span class=\"NLM_given-names\">J. K.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E. L.</span> Cohen</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Clicking for Change: The Role of Empathy and Negative Affect on Engagement with a Charitable Social Media Campaign</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1185</span>\u2013<span class=\"NLM_lpage\">1193</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1578827</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0100&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1578827&amp;tollfreelink=2_18_bfdbadcca82097fb41a86f42767975539644af3031ac0e88267a6b1c8854be0e\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0100&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1185-1193&issue=12&author=J.+K.+Weiss&author=E.+L.+Cohen&title=Clicking+for+Change%3A+The+Role+of+Empathy+and+Negative+Affect+on+Engagement+with+a+Charitable+Social+Media+Campaign&doi=10.1080%2F0144929X.2019.1578827\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0101\"><span><span class=\"hlFld-ContribAuthor\">Wongpakaran, <span class=\"NLM_given-names\">N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Wongpakaran</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Wedding</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Gwet</span>. <span class=\"NLM_year\">2013</span>. \u201c<span class=\"NLM_article-title\">A Comparison of Cohen\u2019s Kappa and Gwet\u2019s AC1 when Calculating Inter-Rater Reliability Coefficients: A Study Conducted with Personality Disorder Samples</span>.\u201d <i>BMC Medical Research Methodology</i> 13 (1): <span class=\"NLM_fpage\">61</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0101&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1186%2F1471-2288-13-61\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0101&amp;dbid=8&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=23627889\" target=\"_blank\">[PubMed]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=13&publication_year=2013&pages=61&issue=1&author=N.+Wongpakaran&author=T.+Wongpakaran&author=D.+Wedding&author=K.+L.+Gwet&title=A+Comparison+of+Cohen%E2%80%99s+Kappa+and+Gwet%E2%80%99s+AC1+when+Calculating+Inter-Rater+Reliability+Coefficients%3A+A+Study+Conducted+with+Personality+Disorder+Samples\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0102\"><span><span class=\"hlFld-ContribAuthor\">Yang, <span class=\"NLM_given-names\">X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Li</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Lyu</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">Exposing Deep Fakes Using Inconsistent Head Poses</span>.\u201d In <i>ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, <span class=\"NLM_fpage\">IEEE, 8261</span>\u2013<span class=\"NLM_lpage\">8265</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0102&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICASSP.2019.8683164\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=IEEE%2C+8261-8265&author=X.+Yang&author=Y.+Li&author=S.+Lyu&title=Exposing+Deep+Fakes+Using+Inconsistent+Head+Poses\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0103\"><span><span class=\"hlFld-ContribAuthor\">Yin, <span class=\"NLM_given-names\">W.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Fu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Sigal</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Xue</span>. <span class=\"NLM_year\">2017</span>. \u201cSemi-Latent GAN: Learning to Generate and Modify Facial Images from Attributes.\u201d <i>ArXiv:1704.02166 [Cs]</i>. <a class=\"ext-link\" href=\"http://arxiv.org/abs/1704.02166\" target=\"_blank\">http://arxiv.org/abs/1704.02166</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Yin%2C+W.%2C+Y.+Fu%2C+L.+Sigal%2C+and+X.+Xue.+2017.+%E2%80%9CSemi-Latent+GAN%3A+Learning+to+Generate+and+Modify+Facial+Images+from+Attributes.%E2%80%9D+ArXiv%3A1704.02166+%5BCs%5D.+http%3A%2F%2Farxiv.org%2Fabs%2F1704.02166.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0104\"><span><span class=\"hlFld-ContribAuthor\">Yuan, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Su</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Liu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Zeng</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Locally and Multiply Distorted Image Quality Assessment via Multi-Stage CNNs</span>.\u201d <i>Information Processing &amp; Management</i> 57 (4): <span class=\"NLM_fpage\">102175</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ipm.2019.102175</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0104&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ipm.2019.102175\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0104&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000531082800005\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=57&publication_year=2020&pages=102175&issue=4&author=Y.+Yuan&author=H.+Su&author=J.+Liu&author=G.+Zeng&title=Locally+and+Multiply+Distorted+Image+Quality+Assessment+via+Multi-Stage+CNNs&doi=10.1016%2Fj.ipm.2019.102175\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0105\"><span><span class=\"hlFld-ContribAuthor\">Zhang, <span class=\"NLM_given-names\">X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.-F.</span> Brown</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Shankar</span>. <span class=\"NLM_year\">2016</span>. \u201c<span class=\"NLM_chapter-title\">Data-driven Personas: Constructing Archetypal Users with Clickstreams and User Telemetry</span>.\u201d In <i>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">ACM, New York, NY, 5350</span>\u2013<span class=\"NLM_lpage\">5359</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0105&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2858036.2858523\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2016&pages=ACM%2C+New+York%2C+NY%2C+5350-5359&author=X.+Zhang&author=H.-F.+Brown&author=A.+Shankar&title=Data-driven+Personas%3A+Constructing+Archetypal+Users+with+Clickstreams+and+User+Telemetry\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0106\"><span><span class=\"hlFld-ContribAuthor\">Zhang, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Isola</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. A.</span> Efros</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Shechtman</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">O.</span> Wang</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</span>.\u201d In <i>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, <span class=\"NLM_fpage\">586</span>\u2013<span class=\"NLM_lpage\">595</span>. doi:<span class=\"NLM_pub-id\">10.1109/CVPR.2018.00068</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0106&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2018.00068\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=586-595&author=R.+Zhang&author=P.+Isola&author=A.+A.+Efros&author=E.+Shechtman&author=O.+Wang&title=The+Unreasonable+Effectiveness+of+Deep+Features+as+a+Perceptual+Metric\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0107\"><span><span class=\"hlFld-ContribAuthor\">Zhao, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Xue</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Cai</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Gao</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Parsing Human Image by Fusing Semantic and Spatial Features: A Deep Learning Approach</span>.\u201d <i>Information Processing &amp; Management</i> 57 (6): <span class=\"NLM_fpage\">102306</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ipm.2020.102306</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0107&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ipm.2020.102306\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0107&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000582206800034\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=57&publication_year=2020&pages=102306&issue=6&author=R.+Zhao&author=Y.+Xue&author=J.+Cai&author=Z.+Gao&title=Parsing+Human+Image+by+Fusing+Semantic+and+Spatial+Features%3A+A+Deep+Learning+Approach&doi=10.1016%2Fj.ipm.2020.102306\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0108\"><span><span class=\"hlFld-ContribAuthor\">Zhou, <span class=\"NLM_given-names\">M. X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Chen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Xiao</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Yang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Chi</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Williams</span>. <span class=\"NLM_year\">2019a</span>. \u201c<span class=\"NLM_chapter-title\">Getting Virtually Personal: Chatbots who Actively Listen to You and Infer Your Personality</span>.\u201d In <i>Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion</i>, <span class=\"NLM_fpage\">ACM, New York, NY, 123</span>\u2013<span class=\"NLM_lpage\">124</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0108&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3308557.3308667\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019a&pages=ACM%2C+New+York%2C+NY%2C+123-124&author=M.+X.+Zhou&author=W.+Chen&author=Z.+Xiao&author=H.+Yang&author=T.+Chi&author=R.+Williams&title=Getting+Virtually+Personal%3A+Chatbots+who+Actively+Listen+to+You+and+Infer+Your+Personality\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0109\"><span><span class=\"hlFld-ContribAuthor\">Zhou, <span class=\"NLM_given-names\">H.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Liu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Liu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Luo</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Wang</span>. <span class=\"NLM_year\">2019b</span>. \u201c<span class=\"NLM_article-title\">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</span>.\u201d <i>Proceedings of the AAAI Conference on Artificial Intelligence</i> 33 (01): <span class=\"NLM_fpage\">9299</span>\u2013<span class=\"NLM_lpage\">9306</span>. doi:<span class=\"NLM_pub-id\">10.1609/aaai.v33i01.33019299</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0109&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1609%2Faaai.v33i01.33019299\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=33&publication_year=2019b&pages=9299-9306&issue=01&author=H.+Zhou&author=Y.+Liu&author=Z.+Liu&author=P.+Luo&author=X.+Wang&title=Talking+Face+Generation+by+Adversarially+Disentangled+Audio-Visual+Representation&doi=10.1609%2Faaai.v33i01.33019299\" target=\"_blank\">[Google Scholar]</a></span></span></span></li></ul></div><div class=\"response\"><div class=\"sub-article-title\"></div></div>\n</article>\n</div>\n<div class=\"tab tab-pane\" id=\"relatedContent\">\n</div>\n<div class=\"tab tab-pane \" id=\"metrics-content\">\n<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics\" data-pb-dropzone=\"publicationContentDropZoneMetrics\">\n<div class=\"widget literatumArticleMetricsWidget none  widget-none\" id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"literatumArticleMetricsWidget\" data-ajax-widget-id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"access__limit\" data-pb-dropzone=\"accessLimitPage\">\n</div>\n</div>\n</div>\n</div>\n<input id=\"viewLargeImageCaption\" type=\"hidden\" value=\"View Large Image\" /></div>\n</div>\n</div>\n<div class=\"widget gql-alternative-widget none  widget-none  widget-compact-horizontal\" id=\"fb6dfe6a-7c21-4a53-ab68-74b96f8a474d\">\n<div class=\"wrapped \">\n<h2 class=\"widget-header header-none  header-compact-horizontal\">Alternative formats</h2>\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"tabs\">\n<ul class=\"tab-nav no-stick\" role=\"tablist\">\n<li><a href=\"https://www.tandfonline.com/doi/pdf/10.1080/0144929X.2020.1838610\" class=\"show-pdf\" role=\"button\" aria-selected=\"false\" target=\"_blank\"> PDF</a></li>\n<li><a href=\"https://www.tandfonline.com/doi/epub/10.1080/0144929X.2020.1838610\" class=\"show-epub\" role=\"button\" aria-selected=\"false\" target=\"_blank\"> EPUB</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-4 \">\n<div class=\"contents\" data-pb-dropzone=\"contents2\">\n<div class=\"widget general-bookmark-share none  widget-none  widget-compact-all\" id=\"c8494935-e102-4ff5-9395-4ffa44a77f1c\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\">\n<ul>\n<li>\n<div class=\"addthis_toolbox addthis_20x20_style\">\n<div class=\"custom_images\">\n<a class=\"addthis_button_twitter\">\n<span class=\"at-icon-twitter\"></span>\n</a>\n<a class=\"addthis_button_facebook\">\n<span class=\"at-icon-facebook\"></span>\n</a>\n<a class=\"addthis_button_email\">\n<span class=\"at-icon-email\"></span>\n</a>\n<a class=\"addthis_button_none\">\n<span class=\"at-icon-none\"></span>\n</a>\n<a class=\"addthis_button_compact\" tabindex=\"-1\"><span class=\"at-icon-wrapper\"></span>\n<span aria-describedby=\"shareOptions-description\">\n<span class=\"off-screen\" id=\"shareOptions-description\">More Share Options</span>\n</span>\n</a>\n</div>\n</div>\n</li>\n</ul>\n<script type=\"text/javascript\">\n    \n    var script = document.createElement('script');\n    script.type='text/javascript';\n    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';\n    script.async = true;\n    $('head').append(script)\n</script>\n</div>\n</div>\n</div>\n<div class=\"widget general-html none  widget-none\" id=\"16111d74-c554-42b2-a277-f2727ad2b285\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \">&nbsp;</div>\n</div>\n</div>\n<div class=\"widget layout-tabs none further-fonts collapsed-view further-tab-margin collapsed-sticky widget-none  widget-compact-vertical\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"layout-tabs-dropzone\" data-pb-dropzone=\"layoutTabsDropzone\">\n<div class=\"widget general-html none furtherReadingTitle widget-none  widget-compact-all\" id=\"982b80ad-6fe6-4b98-9675-9b6eef03d365\">\n<div class=\"wrapped \">\n<h2 class=\"widget-header header-none  header-compact-all\">Related research <span class=\"tooltip-collapse\"></span></h2>\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"info hide\">\n<p><b>People also read</b> lists articles that other readers of this article have read.</p>\n<p><b>Recommended articles</b> lists articles that we recommend and is powered by our AI driven recommendation engine.</p>\n<p><b>Cited by</b> lists all citing articles based on Crossref citations.<br />Articles with the Crossref icon will open in a new tab.</p>\n</div></div>\n</div>\n</div>\n</div>\n<div class=\"tabs tabs-widget \" aria-live=\"polite\" aria-atomic=\"true\" aria-relevant=\"additions\">\n<ul class=\"tab-nav\" role=\"tablist\">\n<li class=\"active\" role=\"tab\">\n<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" title=\"show People also read\" class=\"\">People also read</a>\n</li>\n<li class=\"\" role=\"tab\">\n<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\" title=\"show Recommended articles\" class=\"\">Recommended articles</a>\n</li>\n<li class=\"\" role=\"tab\">\n<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" title=\"show Cited by\" class=\"frwidget-tabs--cby\">Cited by</a>\n</li>\n</ul>\n<div class=\"tab-content\">\n<div class=\"tab-pane active\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\">\n<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" data-pb-dropzone-name=\"People also read\">\n<div class=\"widget ajaxCFCRWidget none  widget-none  widget-compact-all\" id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCFCRWidget\" data-ajax-widget-id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\">\n<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-b6de7b7c-de82-45a5-9538-313dd15c6659\" data-pb-dropzone-name=\"Recommended articles\">\n<div class=\"widget ajaxAtmCRWidget none  widget-none  widget-compact-all\" id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxAtmCRWidget\" data-ajax-widget-id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\" data-ajax-spin=\"true\" data-ajax-observe=\"true\"></div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\">\n<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" data-pb-dropzone-name=\"Cited by\">\n<div class=\"widget ajaxCitedByWidget none  widget-none  widget-compact-all\" id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCitedByWidget\" data-ajax-widget-id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n<div class=\"widget pageFooter none  widget-none  widget-compact-all\" id=\"d97c173f-d838-4de1-bbd7-ed69f0d36a91\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><footer class=\"page-footer\">\n<div data-pb-dropzone=\"main\">\n<div class=\"widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all\" id=\"1f15adc0-4a59-4d27-93fe-8cbb14a5108a\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">\n<div class=\"row row-md gutterless \">\n<div class=\"col-md-1-1 fit-padding\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget pbOptimizerWidget none  widget-none  widget-compact-all\" id=\"af788167-0054-4892-bd47-5de7cbd64256\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div data-optimizer data-widget-id=\"af788167-0054-4892-bd47-5de7cbd64256\" id=\"widget-af788167-0054-4892-bd47-5de7cbd64256\" data-observer>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"64a44adf-45ed-4da3-be26-ef25beb9dbee\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"6918e9df-910a-4206-9bd0-1a02bc17f740\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container-fluid\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-1-2 footer_left_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"aa9510dd-52ed-4b74-8211-fb510cd9468e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Information for</h3>\n<ul>\n<li><a href=\"https://authorservices.taylorandfrancis.com/\">Authors</a></li>\n<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/\">Corporate partners</a></li>\n<li><a href=\"https://editorresources.taylorandfrancisgroup.com/\">Editors</a></li>\n<li><a href=\"/page/librarians\">Librarians</a></li>\n<li><a href=\"/societies\">Societies</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-sm-1-2 footer_right_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Open access</h3>\n<ul>\n<li><a href=\"/openaccess\">Overview</a></li>\n<li><a href=\"/openaccess/openjournals\">Open journals</a></li>\n<li><a href=\"/openaccess/openselect\">Open Select</a></li>\n<li><a href=\"/openaccess/dove\">Dove Medical Press</a></li>\n<li><a href=\"/openaccess/f1000\">F1000Research</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"fc564559-f496-499c-87c7-d851f371f061\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container-fluid\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-1-2 footer_left_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"cdd1a577-15dc-4271-8941-33a105ec6510\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Opportunities</h3>\n<ul>\n<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/marketing/\">Reprints and e-prints</a></li>\n<li><a href=\"https://taylorandfrancis.com/partnership/commercial/advertising-solutions/\" class=\"footer-ad-click\">Advertising solutions</a></li>\n<li><a href=\"https://taylorandfrancis.com/partnership/commercial/accelerated-publication/\">Accelerated publication</a></li>\n<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/purchasing-options/\">Corporate access solutions</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-sm-1-2 footer_right_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"f3fb3d36-db42-4373-9d0e-432958bf2fbc\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">\n<h3>Help and information</h3>\n<ul>\n<li><a href=\"https://help.tandfonline.com\">Help and contact</a></li>\n<li><a href=\"https://newsroom.taylorandfrancisgroup.com/\">Newsroom</a></li>\n<li><a href=\"/action/showPublications?pubType=journal\">All journals</a></li>\n<li><a href=\"https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot\">Books</a></li>\n</ul>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"b2eecf80-9109-455e-a805-028552718986\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">\n<div class=\"row row-md  \">\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"b997c64c-ce48-41ce-b3d6-9cb2d1c99131\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none \"><div class=\"container-fluid\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-1-2 footer_left_col\">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"914433f6-0ea6-4a47-9781-07564061be86\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-social-label\">\n<h3>Keep up to date</h3>\n</div>\n<div class=\"font-size-correction-sml\">Register to receive personalised research and resources by email</div>\n<div class=\"bs\">\n<div class=\"pull-left links font-size-correction\">\n<a class=\"font-size-correction-link\" href=\"https://taylorandfrancis.formstack.com/forms/tfoguest_signup\"><i class=\"fa fa-envelope-square\" title=\"Register to receive personalised research and resources by email\"></i>Sign me up</a>\n</div></div></div>\n</div>\n</div>\n<div class=\"widget literatumSocialLinks none  widget-none  widget-compact-all\" id=\"3b6a5e53-cd62-452f-adc1-92e187a0849d\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"bs\">\n<div class=\"pull-left links\">\n<a href=\"http://facebook.com/TaylorandFrancisGroup\">\n<i class=\"icon-facebook\" title=\"Taylor and Francis Group Facebook page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"fb-description\">\n<span class=\"off-screen\" id=\"fb-description\">Taylor and Francis Group Facebook page</span>\n</span>\n</a>\n</div>\n<div class=\"pull-left links\">\n<a href=\"https://twitter.com/tandfonline\">\n<i class=\"fa fa-twitter-square\" title=\"Taylor and Francis Group Twitter page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"twitter-description\">\n<span class=\"off-screen\" id=\"twitter-description\">Taylor and Francis Group Twitter page</span>\n</span>\n</a>\n</div>\n<div class=\"pull-left links\">\n<a href=\"http://linkedin.com/company/taylor-&-francis-group\">\n<i class=\"fa fa-linkedin-square\" title=\"Taylor and Francis Group LinkedIn page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"linkedin-description\">\n<span class=\"off-screen\" id=\"linkedin-description\">Taylor and Francis Group Linkedin page</span>\n</span>\n</a>\n</div>\n<div class=\"clearfix\"></div>\n<div class=\"pull-left links\">\n<a href=\"https://www.youtube.com/user/TaylorandFrancis\">\n<i class=\"fa fa-youtube-square\" title=\"Taylor and Francis Group YouTube page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"youtube-description\">\n<span class=\"off-screen\" id=\"youtube-description\">Taylor and Francis Group Youtube page</span>\n</span>\n</a>\n</div>\n<div class=\"pull-left links\">\n<a href=\"http://www.weibo.com/tandfchina\">\n<i class=\"fa fa-weibo\" title=\"Taylor and Francis Group Weibo page\" aria-hidden=\"true\" role=\"button\"></i>\n<span aria-describedby=\"weibo-description\">\n<span class=\"off-screen\" id=\"weibo-description\">Taylor and Francis Group Weibo page</span>\n</span>\n</a>\n</div>\n<div class=\"clearfix\"></div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-sm-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-md-1-2 \">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget responsive-layout none  widget-none  widget-compact-horizontal\" id=\"8d803f96-081d-4768-ab7d-280a77af723b\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">\n<div class=\"row row-sm  \">\n<div class=\"col-sm-3-4 \">\n<div class=\"contents\" data-pb-dropzone=\"contents0\">\n<div class=\"widget general-html none footer-info-container widget-none  widget-compact-vertical\" id=\"b247ecb9-84c9-4762-b270-20f8be1f0ae4\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"informa-group-info\">\n<span>Copyright \u00a9 2021 Informa UK Limited</span>\n<span><a href=\"https://informa.com/privacy-policy/\">Privacy policy</a></span>\n<span><a href=\"/cookies\">Cookies</a></span>\n<span><a href=\"/terms-and-conditions\">Terms & conditions</a></span>\n<span><a href=\"/accessibility\">Accessibility</a></span>\n<p>Registered in England & Wales No. 3099067<br />\n5 Howick Place | London | SW1P 1WG</p>\n</div></div>\n</div>\n</div>\n</div>\n</div>\n<div class=\"col-sm-1-4 footer_tandf_logo\">\n<div class=\"contents\" data-pb-dropzone=\"contents1\">\n<div class=\"widget general-image none  widget-none  widget-compact-vertical\" id=\"b6bde365-079b-454f-94f6-1841291656a1\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-vertical\"><a href=\"http://taylorandfrancis.com/\" title=\"Taylor and Francis Group\">\n<img src=\"/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png\" alt=\"Taylor and Francis Group\" />\n</a></div>\n</div>\n</div>\n</div>\n</div>\n</div>\n</div></div>\n</div>\n</div>\n<div class=\"widget cookiePolicy none  widget-none  widget-compact-all\" id=\"cea739ac-da2c-4d77-9cf1-cb3e0da7e31e\">\n<div class=\"wrapped \">\n<div class=\"widget-body body body-none  body-compact-all\"><div class=\"banner\">\n<a href=\"#\" class=\"btn\">Accept</a>\n<p class=\"message\">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href=\"/cookies\">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>\n</div></div>\n</div>\n</div>\n</div>\n</footer></div>\n</div>\n</div>\n</div>\n</div>\n<script type=\"text/javascript\" src=\"/wro/kriw~product.js\"></script>\n<script>\nloadCSS(\"/wro/kriw~lastInBody-css.css\");\nloadCSS(\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\");\nwindow.scriptSettings=[{js: \"/wro/kriw~jwplayer.js\",selector:'.mediaThumbnailContainer'},\n{js:'/wro/kriw~ajax-widgets.js',css:\"/wro/kriw~ajax-widgets.css\",selector:'.ajaxWidget'},\n{js: '/wro/kriw~loi-api.js',selector:'.toc-fns,.literatumListOfIssuesResponsiveWidget,.literatumListOfIssuesWidget'}\n,{js:\"/wro/kriw~seamless-access-fn.js\",selector: \".seamlessAccess_wrapper,.institutional-login\"}];\nwindow.addEventListener('load',TandfUtils.scriptLoader);\n</script>\n<noscript>\n    <link rel=\"stylesheet\" href=\"/wro/kriw~lastInBody-css.css\">\n    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\">\n</noscript>\n<script defer src=\"https://static.cloudflareinsights.com/beacon.min.js/v64f9daad31f64f81be21cbef6184a5e31634941392597\" integrity=\"sha512-gV/bogrUTVP2N3IzTDKzgP0Js1gg4fbwtYB6ftgLbKQu/V8yH2+lrKCfKHelh4SO3DPzKj4/glTO+tNJGDnb0A==\" data-cf-beacon='{\"rayId\":\"6b66c92829ef6b18\",\"token\":\"b6951d00f50a499ab38e94f58955e14d\",\"version\":\"2021.11.0\",\"si\":100}' crossorigin=\"anonymous\"></script>\n</body>\n</html>\n", "response": ["GET /doi/abs/10.1080/0144929X.2020.1838610 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "HTTP/1.1 302 Found", "Date: Tue, 30 Nov 2021 20:22:04 GMT", "Content-Type: text/html; charset=utf-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "Cache-Control: private", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Set-Cookie: JSESSIONID=0fc4a0f2-fedb-4586-9dbe-b2ced603e53c; path=/; domain=.tandfonline.com; age=-1; HttpOnly; SameSite=None; Secure", "Set-Cookie: JSESSIONID=0fc4a0f2-fedb-4586-9dbe-b2ced603e53c; path=/; domain=.tandfonline.com; age=-1; HttpOnly; SameSite=None; Secure", "Set-Cookie: SERVER=WZ6myaEXBLGG7Eibg8MVDg==; domain=.tandfonline.com; path=/; secure; HttpOnly", "Set-Cookie: MAID=nLqgc8S15pn8Ch5BKIaWpQ==; domain=.tandfonline.com; path=/; secure; expires=Mon, 26-Sep-2022 20:22:04 GMT; HttpOnly", "Set-Cookie: MACHINE_LAST_SEEN=2021-11-30T12%3A22%3A04.041-08%3A00; domain=.tandfonline.com; path=/; secure; expires=Mon, 26-Sep-2022 20:22:04 GMT; HttpOnly", "Set-Cookie: I2KBRCK=1; domain=.tandfonline.com; path=/; secure; expires=Wed, 30-Nov-2022 20:22:04 GMT; HttpOnly", "P3P: CP=\"NOI DSP ADM OUR IND OTC\"", "Location: https://www.tandfonline.com/doi/abs/10.1080/0144929X.2020.1838610?cookieSet=1", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66c92229a36b18-GRU", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "The URL has moved <a href=\"https://www.tandfonline.com/doi/abs/10.1080/0144929X.2020.1838610?cookieSet=1\">here</a>", "GET /doi/abs/10.1080/0144929X.2020.1838610?cookieSet=1 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: JSESSIONID=0fc4a0f2-fedb-4586-9dbe-b2ced603e53c; SERVER=WZ6myaEXBLGG7Eibg8MVDg==; MAID=nLqgc8S15pn8Ch5BKIaWpQ==; MACHINE_LAST_SEEN=2021-11-30T12%3A22%3A04.041-08%3A00; I2KBRCK=1", "HTTP/1.1 302 Found", "Date: Tue, 30 Nov 2021 20:22:04 GMT", "Content-Type: text/html; charset=utf-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "Cache-Control: private", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Location: https://www.tandfonline.com/doi/abs/10.1080/0144929X.2020.1838610", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66c9240c426b18-GRU", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "The URL has moved <a href=\"https://www.tandfonline.com/doi/abs/10.1080/0144929X.2020.1838610\">here</a>", "GET /doi/abs/10.1080/0144929X.2020.1838610 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: I2KBRCK=1; JSESSIONID=0fc4a0f2-fedb-4586-9dbe-b2ced603e53c; MACHINE_LAST_SEEN=2021-11-30T12%3A22%3A04.041-08%3A00; MAID=nLqgc8S15pn8Ch5BKIaWpQ==; SERVER=WZ6myaEXBLGG7Eibg8MVDg==", "HTTP/1.1 302 Found", "Date: Tue, 30 Nov 2021 20:22:04 GMT", "Content-Type: text/html; charset=utf-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Cache-Control: no-cache", "Pragma: no-cache", "X-Webstats-RespID: 105fc3a7dfbd270df79555d77f44974b", "Location: https://www.tandfonline.com/doi/full/10.1080/0144929X.2020.1838610", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66c925aeaf6b18-GRU", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "The URL has moved <a href=\"https://www.tandfonline.com/doi/full/10.1080/0144929X.2020.1838610\">here</a>", "GET /doi/full/10.1080/0144929X.2020.1838610 HTTP/1.1", "Host: www.tandfonline.com", "User-Agent: PostmanRuntime/7.28.4", "Accept-Encoding: gzip, deflate", "Accept: */*", "Connection: keep-alive", "Cookie: I2KBRCK=1; JSESSIONID=0fc4a0f2-fedb-4586-9dbe-b2ced603e53c; MACHINE_LAST_SEEN=2021-11-30T12%3A22%3A04.041-08%3A00; MAID=nLqgc8S15pn8Ch5BKIaWpQ==; SERVER=WZ6myaEXBLGG7Eibg8MVDg==", "HTTP/1.1 200 OK", "Date: Tue, 30 Nov 2021 20:22:05 GMT", "Content-Type: text/html; charset=UTF-8", "Transfer-Encoding: chunked", "Connection: keep-alive", "X-XSS-Protection: 1; mode=block", "X-Content-Type-Options: nosniff", "Strict-Transport-Security: max-age=15552000", "X-Frame-Options: SAMEORIGIN", "Cache-Control: no-cache, no-store", "Pragma: no-cache", "X-Webstats-RespID: d0dd611d4cf8ad1746f90d35abd4fd50", "Content-Language: en", "CF-Cache-Status: DYNAMIC", "Expect-CT: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"", "Server: cloudflare", "CF-RAY: 6b66c92829ef6b18-GRU", "Content-Encoding: gzip", "alt-svc: h3=\":443\"; ma=86400, h3-29=\":443\"; ma=86400, h3-28=\":443\"; ma=86400, h3-27=\":443\"; ma=86400", "<!DOCTYPE html>", "<html lang=\"en\" class=\"pb-page\" data-request-id=\"47f9a879-9772-47a6-8b7e-7a353d33de60\"><head data-pb-dropzone=\"head\"><meta name=\"pbContext\" content=\";journal:journal:tbit20;requestedJournal:journal:tbit20;page:string:Article/Chapter View;ctype:string:Journal Content;article:article:10.1080/0144929X.2020.1838610;wgroup:string:Publication Websites;website:website:TFOPB;pageGroup:string:Publication Pages;subPage:string:Full Text\" />", "<link rel=\"schema.DC\" href=\"http://purl.org/DC/elements/1.0/\" /><meta name=\"citation_journal_title\" content=\"Behaviour &amp; Information Technology\" /><meta name=\"dc.Title\" content=\"Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas\" /><meta name=\"dc.Creator\" content=\"Joni  Salminen\" /><meta name=\"dc.Creator\" content=\"Soon-gyo  Jung\" /><meta name=\"dc.Creator\" content=\"Ahmed Mohamed Sayed  Kamel\" /><meta name=\"dc.Creator\" content=\"Jo\u00e3o M.  Santos\" /><meta name=\"dc.Creator\" content=\"Bernard J.  Jansen\" /><meta name=\"dc.Subject\" content=\"Evaluation; human\u2013computer interaction; user behaviour; human factors; artificially generated facial pictures\" /><meta name=\"dc.Description\" content=\"We conduct two studies to evaluate the suitability of artificially generated facial pictures for use in a customer-facing system using data-driven personas. STUDY 1 investigates the quality of a sa...\" /><meta name=\"Description\" content=\"We conduct two studies to evaluate the suitability of artificially generated facial pictures for use in a customer-facing system using data-driven personas. STUDY 1 investigates the quality of a sa...\" /><meta name=\"dc.Publisher\" content=\"Taylor &amp; Francis\" /><meta name=\"dc.Date\" scheme=\"WTN8601\" content=\"6 Nov 2020\" /><meta name=\"dc.Type\" content=\"research-article\" /><meta name=\"dc.Format\" content=\"text/HTML\" /><meta name=\"dc.Identifier\" scheme=\"publisher-id\" content=\"1838610\" /><meta name=\"dc.Identifier\" scheme=\"doi\" content=\"10.1080/0144929X.2020.1838610\" /><meta name=\"dc.Identifier\" scheme=\"submission-id\" content=\"TBIT-2020-0373.R1\" /><meta name=\"dc.Source\" content=\"https://doi.org/10.1080/0144929X.2020.1838610\" /><meta name=\"dc.Language\" content=\"en\" /><meta name=\"dc.Coverage\" content=\"world\" /><meta name=\"dc.Rights\" content=\"\u00a9 2020 The Author(s). Published by Informa UK Limited, trading as Taylor &amp; Francis Group\" /><meta name=\"keywords\" content=\"Evaluation,human\u2013computer interaction,user behaviour,human factors,artificially generated facial pictures\" /><meta name=\"citation_fulltext_world_readable\" content=\"\" />", "<link rel=\"meta\" type=\"application/atom+xml\" href=\"https://doi.org/10.1080%2F0144929X.2020.1838610\" />", "<link rel=\"meta\" type=\"application/rdf+json\" href=\"https://doi.org/10.1080%2F0144929X.2020.1838610\" />", "<link rel=\"meta\" type=\"application/unixref+xml\" href=\"https://doi.org/10.1080%2F0144929X.2020.1838610\" />", "<title>Full article: Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</title>", "<meta charset=\"UTF-8\">", "<meta name=\"robots\" content=\"noarchive\" />", "<meta property=\"og:title\" content=\"Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas\" />", "<meta property=\"og:type\" content=\"article\" />", "<meta property=\"og:url\" content=\"https://www.tandfonline.com/doi/abs/10.1080/0144929X.2020.1838610\" />", "<meta property=\"og:image\" content=\"https://www.tandfonline.com/doi/cover-img/10.1080/tbit20\" />", "<meta property=\"og:site_name\" content=\"Taylor & Francis\" />", "<meta property=\"og:description\" content=\"(2020). Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas. Behaviour &amp; Information Technology. Ahead of Print.\" />", "<meta name=\"twitter:card\" content=\"summary_large_image\">", "<meta name=\"twitter:site\" content=\"@tandfonline\">", "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />", "<link href=\"//connect.facebook.net\" rel=\"preconnect\" />", "<link href=\"//go.taylorandfrancis.com\" rel=\"preconnect\" />", "<link href=\"//pi.pardot.com\" rel=\"preconnect\" />", "<link href=\"//static.hotjar.com\" rel=\"preconnect\" />", "<link href=\"//cdn.pbgrd.com\" rel=\"preconnect\" />", "<link href=\"//f1-eu.readspeaker.com\" rel=\"preconnect\" />", "<link href=\"//www.googleadservices.com\" rel=\"preconnect\" />", "<link href=\"https://m.addthis.com\" rel=\"preconnect\" />", "<link href=\"https://wl.figshare.com\" rel=\"preconnect\" />", "<link href=\"https://pagead2.googlesyndication.com\" rel=\"preconnect\" />", "<link href=\"https://www.googletagmanager.com\" rel=\"preconnect\" />", "<link href=\"https://www.google-analytics.com\" rel=\"preconnect\" />", "<link href=\"https://fonts.googleapis.com\" rel=\"preconnect\" />", "<link href=\"https://fonts.gstatic.com\" rel=\"preconnect\" />", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/css/font-awesome/fonts/fontawesome-webfont.woff2?v=4.7.0\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/icomoon/icomoon.woff?g276mb\" type=\"font/woff\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-300italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-600italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-700italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-italic.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link rel=\"preload\" as=\"font\" href=\"/templates/jsp/_style2/_tandf/pb2/fonts/google/opensans/open-sans-v23-latin-regular.woff2\" type=\"font/woff2\" crossorigin=\"anonymous\">", "<link type=\"text/css\" rel=\"stylesheet\" href=\"/wro/kriw~product.css\">", "<link rel=\"stylesheet\" type=\"text/css\" href=\"/pb/css/t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" id=\"pb-css\" data-pb-css-id=\"t1637925934047-v1636963890000/head_4_698_1485_2139_2347_7872_en.css\" />", "<script type=\"text/javascript\" src=\"//cdn.pbgrd.com/core-tandf.js\" async></script>", "<script data-ad-client=\"ca-pub-5143040550582507\" src=\"https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\" async></script>", "<script>", "    (function(h,o,t,j,a,r){", "        h.hj=h.hj||function(){(h.hj.q=h.hj.q||[]).push(arguments)};", "        h._hjSettings={hjid:864760,hjsv:6};", "        a=o.getElementsByTagName('head')[0];", "        r=o.createElement('script');r.async=1;", "        r.src=t+h._hjSettings.hjid+j+h._hjSettings.hjsv;", "        a.appendChild(r);", "    })(window,document,'https://static.hotjar.com/c/hotjar-','.js?sv=');", "</script>", "<script>var _prum=[['id','54ff88bcabe53dc41d1004a5'],['mark','firstbyte',(new Date()).getTime()]];(function(){var s=document.getElementsByTagName('script')[0],p=document.createElement('script');p.async='async';p.src='//rum-static.pingdom.net/prum.min.js';s.parentNode.insertBefore(p,s);})();</script>", "<script type=\"text/javascript\">", "        window.rsConf={general:{popupCloseTime:8000,usePost:true},params:'//cdn1.readspeaker.com/script/26/webReader/webReader.js?pids=wr'};", "    </script>", "<script type=\"application/javascript\" src=\"//f1-eu.readspeaker.com/script/10118/webReader/webReader.js?pids=wr\" id=\"read-speaker\" async></script>", "<script>var tandfData={\"search\":{\"cbRec\":8},\"seamlessAccess\":{\"apiUrl\":\"https://service.seamlessaccess.org/ps/\",\"context\":\"seamlessaccess.org\"},\"identity\":{\"isSpv\":false,\"isAuthenticated\":false},\"pubCount\":{\"citedCount\":1},\"actionLog\":{\"eventGroupKey\":\"d77116a5-639a-4772-974b-29cb497068d5\"}};</script>", "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">", "<link rel=\"canonical\" href=\"https://www.tandfonline.com/doi/full/10.1080/0144929X.2020.1838610\" />", "</head>", "<body class=\"pb-ui\">", "<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-W2RHRDH');</script>", "<noscript><iframe src=\"https://www.googletagmanager.com/ns.html?id=GTM-W2RHRDH\" height=\"0\" width=\"0\" style=\"display:none;visibility:hidden\"></iframe></noscript>", "<script type=\"text/javascript\" src=\"/wro/kriw~jquery-3.5.0.js\"></script>", "<div class=\"skipContent off-screen\"><a href=\"#top-content-scroll\" class=\"skipToContent\" title=\"Skip to Main Content\" tabIndex=\"0\">Skip to Main Content</a></div>", "<script type=\"text/javascript\">(function(e){var t=e.getElementsByClassName(\"skipToContent\");t.length>0&&(t[0].onclick=function(){var t=e.getElementById(\"top-content-scroll\");null==t&&(t=e.getElementsByClassName(\"top-content-scroll\").item(0)),t.setAttribute(\"tabindex\",\"0\"),t.focus()})})(document);</script>", "<div id=\"pb-page-content\" data-ng-non-bindable>", "<div data-pb-dropzone=\"main\" data-pb-dropzone-name=\"Main\">", "<div class=\"widget pageHeader none  widget-none  widget-compact-all\" id=\"a4d4fdd3-c594-4d68-9f06-b69b8b37ed56\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><header class=\"page-header\" aria-label=\"Main Banner\">", "<div data-pb-dropzone=\"main\">", "<div class=\"widget responsive-layout none header-top widget-none  widget-compact-all\" id=\"036fa949-dc25-4ffe-9df0-d7daefee281b\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-xs  \">", "<div class=\"col-xs-1-6 header-index\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-image alignLeft header-logo hidden-xs widget-none  widget-compact-horizontal\" id=\"e817489e-2520-418b-a731-b62e247e74df\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\" title=\"Taylor and Francis Online\">", "<img src=\"/pb-assets/Global/tfo_logo-1444989687640.png\" alt=\"Taylor and Francis Online\" />", "</a></div>", "</div>", "</div>", "<div class=\"widget general-image none header-logo hidden-sm hidden-md hidden-lg widget-none  widget-compact-horizontal\" id=\"b3fe8380-8b88-4558-b004-6485d3aea155\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><a href=\"/\">", "<img src=\"/pb-assets/Global/tfo_logo_sm-1459688573210.png\" />", "</a></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-xs-5-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget layout-inline-content alignRight  widget-none  widget-compact-all\" id=\"a8a37801-55c7-4566-bdef-e4e738967e38\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">", "<div class=\"widget layout-inline-content none customLoginBar widget-none\" id=\"fbe90803-b9c8-4bef-9365-cb53cc4bfa0e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">", "<div class=\"widget literatumInstitutionBanner none bannerWidth widget-none\" id=\"3ff4d9f6-0fd0-44d0-89cd-6b16c5bb33ba\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"institution-image hidden-xs logout-institution-image\">", "</div></div>", "</div>", "</div>", "<div class=\"widget literatumNavigationLoginBar none  widget-none  widget-compact-all\" id=\"1d69ec8f-0b13-42ca-bc6d-f5a385caf8c4\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"loginBar not-logged-in\">", "<span class=\"icon-user\"></span>", "<a href=\"/action/showLogin?uri=%2Fdoi%2Ffull%2F10.1080%2F0144929X.2020.1838610\" class=\"sign-in-link\">", "Log in", "</a>", "<span class=\"loginSeprator\">&nbsp;|&nbsp;</span>", "<a href=\"/action/registration?redirectUri=%2F\" class=\"register-link\">", "Register", "</a>", "</div></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget eCommerceCartIndicatorWidget none literatumCartLink widget-none\" id=\"9de10bb5-08af-48bc-b9f6-3f6433229f3e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><a href=\"/action/showCart?FlowID=1\" class=\"cartLabel\">", "<span class=\"hidden-xs hidden-sm visible-tl-inline-block\">Cart</span>", "<span class=\"cartItems\" data-id=\"cart-size\" role=\"status\">", "</span>", "</a></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none breadcrumbs-container widget-none  widget-compact-all\" id=\"64c16283-4b04-4d90-ac0f-4db85fcd0cf5\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumBreadcrumbs none breadcrumbs-widget widget-none  widget-compact-all\" id=\"b1c121c1-cbbd-4241-8774-7120f3a783e8\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\">", "<nav aria-label=\"Breadcrumb\">", "<ol class=\"breadcrumbs\">", "<li class=\"\">", "<a href=\"/\" class=\"bc-click\">", "Home", "</a>", "</li>", "<li class=\"\">", "<a href=\"/action/showPublications?pubType=journal\" class=\"bc-click\">", "All Journals", "</a>", "</li>", "<li class=\"\">", "<a href=\"/toc/tbit20/current\" class=\"bc-click\">", "Behaviour & Information Technology", "</a>", "</li>", "<li class=\"\">", " <a href=\"/loi/tbit20\" class=\"bc-click\">", "List of Issues", "</a>", "</li>", "<li class=\"\">", "<a href=\"/toc/tbit20/0/0\" class=\"bc-click\">", "Latest Articles", "</a>", "</li>", "<li class=\"\">", "<a href=\"#\" class=\"bc-click\" aria-current=\"page\">", "Using artificially generated pictures in ....", "</a>", "</li>", "</ol>", "</nav>", "<script type=\"application/ld+json\">", "    {", "        \"@context\": \"https://schema.org\",", "        \"@type\": \"BreadcrumbList\",", "        \"itemListElement\":", "        [{", "            \"@type\": \"ListItem\",", "            \"position\": \"1\",", "            \"name\": \"Home\"", "            ,\"item\": \"https://www.tandfonline.com/\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"2\",", "            \"name\": \"All Journals\"", "            ,\"item\": \"https://www.tandfonline.com/action/showPublications?pubType=journal\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"3\",", "            \"name\": \"Behaviour & Information Technology\"", "            ,\"item\": \"https://www.tandfonline.com/toc/tbit20/current\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"4\",", "            \"name\": \"List of Issues\"", "            ,\"item\": \"https://www.tandfonline.com/loi/tbit20\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"5\",", "            \"name\": \"Latest Articles\"", "            ,\"item\": \"https://www.tandfonline.com/toc/tbit20/0/0\"", "        },", "        {", "            \"@type\": \"ListItem\",", "            \"position\": \"6\",", "            \"name\": \"Using artificially generated pictures in .... \"", "            ", "        }", "        ]", "    }", "</script></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</header></div>", "</div>", "</div>", "<div data-widget-def=\"pageBody\" data-widget-id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\" role=\"main\">", "<div class=\"widget pageBody none  widget-none  widget-compact-all\" id=\"35d9ca18-265e-4501-9038-4105e95a4b7d\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\">", "<div class=\"page-body pagefulltext\">", "<div data-pb-dropzone=\"main\">", "<div class=\"widget responsive-layout none publicationSerialHeader article-chapter-view widget-none  widget-compact-all\" id=\"1728e801-36cd-4288-9f53-392bad29506a\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md gutterless \">", "<div class=\"col-md-5-12 search_container \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget quickSearchWidget none search-customize-width widget-none  widget-compact-all\" id=\"d46e3260-1f5c-4802-821a-28a03a699c82\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"quickSearchFormContainer \">", "<form action=\"/action/doSearch\" name=\"quickSearch\" class=\"quickSearchForm \" title=\"Quick Search\" role=\"search\" method=\"get\" onsubmit=\"appendSearchFilters(this)\" aria-label=\"Quick Search\"><span class=\"simpleSearchBoxContainer\">", "<input name=\"AllField\" class=\"searchText main-search-field autocomplete\" value=\"\" type=\"search\" id=\"searchText\" title=\"Type search term here\" aria-label=\"Search\" placeholder=\"Enter keywords, authors, DOI, ORCID etc\" autocomplete=\"off\" data-history-items-conf=\"3\" data-publication-titles-conf=\"3\" data-publication-items-conf=\"3\" data-topics-conf=\"3\" data-contributors-conf=\"3\" data-fuzzy-suggester=\"false\" data-auto-complete-target=\"title-auto-complete\" />", "</span>", "<span class=\"searchDropDownDivRight\">", "<label for=\"searchInSelector\" class=\"visuallyhidden\">Search in:</label>", "<select id=\"searchInSelector\" name=\"SeriesKey\" class=\"js__searchInSelector\">", "<option value=\"tbit20\" id=\"thisJournal\" data-search-in=\"thisJournal\">", "This Journal", "</option>", "<option value=\"\" data-search-in=\"default\">", "Anywhere", "</option>", "</select>", "</span>", "<span class=\"quick-search-btn\">", "<input class=\"mainSearchButton searchButtons pointer\" title=\"Search\" role=\"button\" type=\"submit\" value=\"\" aria-label=\"Search\" />", " </span></form>", "</div>", "<div class=\"advancedSearchLinkDropZone\" data-pb-dropzone=\"advancedSearchLinkDropZone\">", "<div class=\"widget general-html alignRight  hidden-xs_sm widget-none  widget-compact-all\" id=\"323e2a31-1c81-4995-bd17-8e149458c214\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><a href=\"/search/advanced\" class=\"advSearchArticle\">Advanced search</a></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-7-12 serNav_container\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget literatumSeriesNavigation none  widget-none\" id=\"7730bfe1-9fca-4cf4-a6d6-2a0148105437\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"issueSerialNavigation journal\">", "<div class=\"cover\">", "<img src=\"/action/showCoverImage?journalCode=tbit20\" alt=\"Publication Cover\" width=\"90\" height=\"120\" />", "</div>", "<div class=\"info \">", "<div class=\"title-container\">", "<h1 class=\"journal-heading\">", "<a href=\"/toc/tbit20/current\">", "Behaviour &amp; Information Technology", "</a>", "</h1>", "<span class=\"issue-heading\">", "<a href=\"/toc/tbit20/0/0\">Latest Articles</a>", "</span>", "</div>", "<div class=\"seriesNavDropZone\" data-pb-dropzone=\"seriesNavDropZone\">", "<div class=\"widget general-html none serial-btns smooth-mv widget-none  widget-compact-horizontal\" id=\"753455df-1eeb-47ca-bdc9-e19022075973\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"serial-action\">", "<a href=\"http://mc.manuscriptcentral.com/tbit\" class=\"green submitAnArticle\"><span>Submit an article</span></a>", "<a href=\"/toc/tbit20/current\" class=\"jHomepage\"><span>Journal homepage</span></a>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none  widget-none  widget-compact-vertical\" id=\"e42aea8f-434a-4d39-aaef-f56af3ff00dc\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumDisplayingAccessLogo none  widget-none  widget-compact-all\" id=\"6aacf107-e82d-494d-a14c-0c00bba52560\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"accessLogo\">", "<div>", "<img class=\"accessIconLocation\" src=\"/pb-assets/3rdPartyLogos/accessOA-1452596421933.png\" alt=\"Open access\" />", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none publicationContentHeader widget-none  widget-compact-all\" id=\"63f402e4-3498-4709-8d7d-ee8e69f93467\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumArticleMetricsWidget none  widget-none  widget-compact-vertical\" id=\"5afd8b6d-7e09-43ff-8ad6-afa3764e543c\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"articleMetricsContainer\">", "<div class=\"content compactView\">", "<div class=\"section\">", "<div class=\"value\">", "949", "</div>", "<div class=\"title\">", "Views", "</div>", "</div>", "<div class=\"section\">", "<div class=\"value\">", "1", "</div>", "<div class=\"title\">", "CrossRef citations to date", "</div>", "</div>", "<div class=\"section score\">", "<div class=\"altmetric-score true\">", "<div class=\"value\" data-doi=\"10.1080/0144929X.2020.1838610\">", "<span class=\"metrics-score\">0</span>", "</div>", "<div class=\"title\">", "Altmetric", "</div>", "</div>", "</div>", "<script>tandfData.altmetric={key:'be0ef6915d1b2200a248b7195d01ef22'}</script>", "<script src=\"/wro/kriw~altmetric.js\" async></script>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-2-3 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget literatumPublicationHeader none literatumPublicationTitle widget-none  widget-compact-all\" id=\"fa57727f-b942-4eb8-9ed2-ecfe11ac03f5\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div id=\"read-speaker-container\" style=\"display: block; height: 40px\">", "<div id=\"readspeaker_button1\" class=\"rs_skip rsbtn rs_preserve\">", "<a href=\"//app-eu.readspeaker.com/cgi-bin/rsent?customerid=10118&amp;lang=en_us&readclass=rs_readArea&url=https%3A%2F%2Fwww.tandfonline.com%2Fdoi%2Ffull%2F10.1080%2F0144929X.2020.1838610\" rel=\"nofollow\" class=\"rsbtn_play\" accesskey=\"L\" title=\"Listen to this page using ReadSpeaker webReader\" style=\"border-radius: 0 11.4px 11.4px 2px;\">", "<span class=\"rsbtn_left rsimg rspart\"><span class=\"rsbtn_text\"><span>Listen</span></span></span>", "<span class=\"rsbtn_right rsimg rsplay rspart\"></span>", "</a>", "</div>", "</div>", "<div class=\"toc-heading\">Research Article</div>", "<h1><span class=\"NLM_article-title hlFld-title\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span></h1><div class=\"literatumAuthors\"><div class=\"publicationContentAuthors\"><div class=\"hlFld-ContribAuthor\"><span class=\"NLM_contrib-group\"><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Salminen%2C+Joni\">Joni Salminen</a><span class=\"overlay\">a Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar;b Turku School of Economics at the University of Turku, Turku, Finland</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Jung%2C+Soon-gyo\">Soon-gyo Jung</a><span class=\"overlay\">a Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\">Ahmed Mohamed Sayed Kamel</a><span class=\"overlay\">c Department of Clinical Pharmacy, Cairo University, Giza, Egypt</span></div>, </span><span class=\"contribDegrees \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Santos%2C+Jo%C3%A3o+M\">Jo\u00e3o M. Santos</a><span class=\"overlay\">d Instituto Universit\u00e1rio de Lisboa (ISCTE-IUL), Lisbon, Portugal</span></div> &amp; </span><span class=\"contribDegrees corresponding \"><div class=\"entryAuthor\"><a class=\"author\" href=\"/author/Jansen%2C+Bernard+J\">Bernard J. Jansen<i class=\"fa fa-envelope\" aria-hidden=\"true\" style=\"margin-left: 0.5em;\"></i></a><span class=\"overlay\">a Qatar Computing Research Institute, Hamad Bin Khalifa University, Doha, Qatar<span class=\"corr-sec\"><span class=\"heading\">Correspondence</span><span class=\"corr-email\"><i class=\"fa fa-envelope\" style=\"color: #10147E; padding-right: 7px\" aria-hidden=\"true\"></i><a href=\"mailto:jjansen@acm.org\">jjansen@acm.org</a></span><br /></span></span></div></span></span></div></div></div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none  widget-none  widget-compact-all\" id=\"5f562208-b1d5-4e5a-81c7-356431240f04\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">", "<div class=\"row row-md gutterless \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget layout-inline-content none  widget-none  widget-compact-all\" id=\"87ac5840-18fa-4a14-8eca-065b90ede3d7\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"inline-dropzone\" data-pb-dropzone=\"content\">", "<div class=\"widget literatumContentItemHistory none  widget-none  widget-compact-all\" id=\"32bf868e-52ce-411a-9dc3-717743aad997\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div>Received 16 Apr 2020</div><div>Accepted 13 Oct 2020</div><div>Published online: 06 Nov 2020</div></div>", "</div>", "</div>", "<div class=\"widget literatumArticleToolsWidget none  widget-none  widget-compact-all\" id=\"ed673666-7b5d-470e-bd33-c5c679d996cb\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"articleTools\">", "<ul class=\"linkList\">", "<li class=\"downloadCitations\">", "<a href=\"/action/showCitFormats?doi=10.1080%2F0144929X.2020.1838610&area=0000000000000001\"><i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>Download citation</a>", "</li>", "<li class=\"dx-doi\">", "<a href=\"https://doi.org/10.1080/0144929X.2020.1838610\"><i class=\"fa fa-external-link-square\" style=\"margin: 0 0.25rem 0 0\" aria-hidden=\"true\"></i>https://doi.org/10.1080/0144929X.2020.1838610</a>", "</li>", "<script src=\"/wro/kriw~crossmark.js\" async></script>", "<li class=\"cross_mark\">", "<a class=\"cross_mark--link\" data-doi=\"10.1080/0144929X.2020.1838610\" data-target=\"crossmark\" href=\"#\">", "<img src=\"/templates/jsp/images/CROSSMARK_Color_horizontal.svg\" alt=\"CrossMark Logo\" width=\"100\" height=\"22px\" />", "<span aria-describedby=\"crossMark-description\"><span class=\"off-screen\" id=\"crossMark-description\">CrossMark</span></span>", "</a>", "</li>", "</ul>", "</div></div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents2\">", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none publicationContentBody widget-none\" id=\"f4a74f7a-9ba2-4605-86b1-8094cb1f01de\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-6 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget sectionsNavigation none  widget-none\" id=\"f15bd2de-bb18-4067-8ab9-03ea3be30bf7\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"sections-nav\" role=\"navigation\" aria-label=\"Article Navigation\"><span class=\"title\">In this article<a href=\"#\" class=\"close\" tabindex=\"-1\"><span aria-label=\"close button for mobile\"><span class=\"off-screen\" id=\"close-description\">Close</span></span></a></span><ul class=\"sections-list\"><li><a href=\"#abstract\">ABSTRACT</a></li><li><span class=\"sub-art-heading\"><a href=\"#_i3\">1. Introduction</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i6\">2. Related literature</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i11\">3. Methodology</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i16\">4. STUDY 1: crowdsourced evaluation</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i23\">5. STUDY 2: effects on persona perceptions</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i34\">6. Discussion</a></span><ul class=\"sub-art-titles\"></ul></li><li><span class=\"sub-art-heading\"><a href=\"#_i39\">7. Conclusion</a></span><ul class=\"sub-art-titles\"></ul></li><li><a href=\"#coi-statement\">Disclosure statement</a></li><li><a href=\"#inline_frontnotes\">Footnotes</a></li><li><a href=\"#references-Section\">References</a></li></ul></div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-7-12 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget responsive-layout none rs_readArea widget-none  widget-compact-all\" id=\"9751b4f9-64b9-44c0-955b-f75246902839\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container-fluid\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-1 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget literatumPublicationContentWidget none rs_preserve widget-none  widget-compact-all\" id=\"d29f04e9-776c-4996-a0d8-931023161e00\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><script type=\"text/javascript\" async src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\">", "    MathJax.Hub.Config({", "        \"HTML-CSS\": {scale: 70, linebreaks: {automatic: true, width: \"container\"}},", "        SVG: {linebreaks: {automatic: true, width: \"25%\"}},", "        menuSettings: {zoom: \"Click\"},", "        /* This is necessary to lazy loading. */", "        skipStartupTypeset: true", "    });", "</script>", "<script type=\"application/javascript\" async src=\"/wro/kriw~mathjax.js\"></script>", "<div class=\"articleMeta ja\">", "<div class=\"tocHeading\">", "<h2>Research Article</h2>", "</div>", "<div class=\"hlFld-Title\">", "<div class=\"publicationContentTitle\">", "<h1 class=\"chaptertitle\">", "Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas", "</h1>", "</div>", "</div>", "<div class=\"copyrightStatement\">", "</div>", "<div class=\"articleMetaDrop publicationContentDropZone\" data-pb-dropzone=\"articleMetaDropZone\">", "</div>", "<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone1\" data-pb-dropzone=\"articleMetaDropZone1\">", "</div>", "<div class=\"copyrightline\">", "</div>", "<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZone2\" data-pb-dropzone=\"articleMetaDropZone2\">", "</div>", "</div>", "<div class=\"publication-tabs ja publication-tabs-dropdown\">", "<div class=\"tabs tabs-widget\">", "<ul class=\"tab-nav\" role=\"tablist\">", "<li class=\"active\" role=\"tab\">", "<a href=\"/doi/full/10.1080/0144929X.2020.1838610?scroll=top&amp;needAccess=true\" class=\"show-full\">", "<i class=\"fa fa-file-text\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">", "Full Article", "</span>", "</a>", "</li>", "<li role=\"tab\">", "<a href=\"/doi/figure/10.1080/0144929X.2020.1838610?scroll=top&amp;needAccess=true\" class=\"show-figure\">", "<i class=\"fa fa-image\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">Figures & data</span>", "</a>", "</li>", "<li role=\"tab\">", " <a href=\"/doi/ref/10.1080/0144929X.2020.1838610?scroll=top\" class=\"show-references\">", "<i class=\"fa fa-book\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">References</span>", "</a>", "</li>", "<li class=\"citedbyTab \" role=\"tab\">", "<a href=\"/doi/citedby/10.1080/0144929X.2020.1838610?scroll=top&amp;needAccess=true\">", "<i class=\"fa fa-quote-left\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">", "Citations", "</span>", "</a>", "</li>", "<li role=\"tab\" class=\"metrics-tab\">", "<a href=\"#metrics-content\" class=\"show-metrics\">", "<i class=\"fa fa-bar-chart\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">Metrics</span>", "</a>", "</li>", "<li role=\"tab\" class=\"licencing-tab \">", "<a href=\"/action/showCopyRight?scroll=top&amp;doi=10.1080%2F0144929X.2020.1838610\" class=\"show-copyright\">", "<i class=\"fa fa-copyright\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">Licensing</span>", "</a>", "</li>", "<li role=\"tab\" class=\"permissions-tab \">", "<a href=\"/doi/abs/10.1080/0144929X.2020.1838610?tab=permissions&amp;scroll=top\" class=\"show-permissions\">", "<i class=\"fa fa-print\" aria-hidden=\"true\"></i>", "<span class=\"nav-data\">", "Reprints & Permissions</span></a>", "</li>", "<li class=\"pdf-tab \" role=\"tab\">", "<a href=\"/doi/pdf/10.1080/0144929X.2020.1838610?needAccess=true\" class=\"show-pdf\" role=\"button\" target=\"_blank\">", "<span class=\"nav-data\">", "PDF", "</span>", "</a>", "</li>", "<li class=\"epub-tab \" role=\"tab\">", "<a href=\"/doi/epub/10.1080/0144929X.2020.1838610?needAccess=true\" class=\"show-epub noBefore oneButton oneButtonDesktop \" role=\"button\" target=\"_blank\">EPUB</a>", "</li>", "</ul>", "<div class=\"tab-content \">", "<a id=\"top-content-scroll\"></a>", "<div class=\"tab tab-pane active\">", "<article class=\"article\">", "<p class=\"fulltext\"></p><div class=\"hlFld-Abstract\"><p class=\"fulltext\"></p><div class=\"sectionInfo abstractSectionHeading\"><h2 id=\"abstract\" class=\"section-heading-2\">ABSTRACT<div id=\"mathJaxToggle\" class=\"hideElement\"><label for=\"mathJaxToggle\"><span>Formulae display:</span><input type=\"checkbox\" id=\"mathJaxToggleCheck\" name=\"mathJaxToggleCheck\" /></label><span class=\"mathJaxLogo\"><img src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/templates/jsp/_style2/_tandf/pb2/images/math-jax.gif\"}' alt=\"MathJax Logo\" /><span class=\"qMrk\">?</span><span class=\"auPopUp hideElement\"><span class=\"pointyEdge\"></span>Mathematical formulae have been encoded as MathML and are displayed in this HTML version using MathJax in order to improve their display. Uncheck the box to turn MathJax off. This feature requires Javascript. Click on a formula to zoom.</span></span></div><span class=\"math-settings hidden-lg\"></span></h2></div><div class=\"abstractSection abstractInFull\"><p class=\"summary-title\"><b>ABSTRACT</b></p><p>We conduct two studies to evaluate the suitability of artificially generated facial pictures for use in a customer-facing system using data-driven personas. STUDY 1 investigates the quality of a sample of 1,000 artificially generated facial pictures. Obtaining 6,812 crowd judgments, we find that 90% of the images are rated medium quality or better. STUDY 2 examines the application of artificially generated facial pictures in data-driven personas using an experimental setting where the high-quality pictures are implemented in persona profiles. Based on 496 participants using 4 persona treatments (2\u2009\u00d7\u20092 research design), findings of Bayesian analysis show that using the artificial pictures in persona profiles did not decrease the scores for Authenticity, Clarity, Empathy, and Willingness to Use of the data-driven personas.</p></div></div><div class=\"abstractKeywords\"><div class=\"hlFld-KeywordText\"><div><p class=\"kwd-title\" aria-label=\"Keywords\">KEYWORDS: </p><a href=\"/keyword/Evaluation\" class=\"kwd-btn keyword-click\" role=\"button\">Evaluation</a><a href=\"/keyword/Human%E2%80%93computer+Interaction\" class=\"kwd-btn keyword-click\" role=\"button\">human\u2013computer interaction</a><a href=\"/keyword/User+Behaviour\" class=\"kwd-btn keyword-click\" role=\"button\">user behaviour</a><a href=\"/keyword/Human+Factors\" class=\"kwd-btn keyword-click\" role=\"button\">human factors</a><a href=\"/keyword/Artificially+Generated+Facial+Pictures\" class=\"kwd-btn keyword-click\" role=\"button\">artificially generated facial pictures</a></div></div></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneAbs\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"d28d5637-3950-463d-a4f7-bc92bc490fff\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div></div><div class=\"hlFld-Fulltext\"><div id=\"S001\" class=\"NLM_sec NLM_sec-type_intro NLM_sec_level_1\"><h2 id=\"_i3\" class=\"section-heading-2\">1. Introduction</h2><p>There is tremendous research interest concerning artificial image generation (AIG). The state-of-the-art studies in this field use <i>Generative Adversarial Networks</i> (GANs) (Goodfellow et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0037\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>) and Conditional GANs (Lu, Tai, and Tang <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0064\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) to generate images that are promised to be photorealistic and easily deployable. GANs have been applied, for example, to automatically create art (Tan et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0099\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), cartoons (Liu et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0061\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), medical images (Nie et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0068\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), and facial pictures (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), the latter including transformations such as increasing/decreasing a person's age or altering their gender (Antipov, Baccouche, and Dugelay <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0006\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Choi et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0020\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Isola et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0048\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>).</p><p>Due its low cost, AIG provides novel opportunities for a wide range of applications, including health-care (Nie et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0068\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), advertising (Neumann, Pyromallis, and Alexander <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0067\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), and user analytics for human computer interaction (HCI) and design purposes (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). However, despite the far-reaching interest in AIG among academia and across industries, there is scant research on <i>evaluating the suitability of the generated images for practical use in deployed systems</i>. This means that the quality and impact of the artificial images on user perceptions are often neglected, lacking user studies of their deployment in real systems. This area of evaluation is an overlooked but critical area of research, as it is the \u2018final step\u2019 of deployment that actually determines if the quality of the AIG is good enough, as prior work has shown the impact that pictures can have on real systems (King, Lazard, and White <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0056\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>). Therefore, the impact of AIG on user experience (UX) and design applications is a largely unaddressed field of study, although with work in related areas of empathy (Weiss and Cohen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0100\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). For example, Weiss and Cohen (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0100\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) that aspects of empathy with subjects in videos is complex in terms of encouraging or discouraging engagement with the content.</p><p>Most typically, artificial pictures are evaluated using <i>technical metrics</i> (Yuan et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0104\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>) that are abstract and do not reflect user perceptions or UX. An example is the <i>Fr\u00e8chet inception distance</i> (FID) (Heusel et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0041\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) that measures the similarity of two image distributions (i.e. the generated set and the training set). While metrics such as FID are without question necessary for measuring the <i>technical quality</i> of the generated images (Zhao et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0107\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>), we argue there is also a substantial need for evaluating the <i>user experience</i> of the pictures for real-world systems and applications.</p><p>In this regard, the user study tradition from HCI is helpful \u2013 in addition to technical metrics, <i>user-centric metrics</i> gauging UX and user perceptions (Ashraf, Jaafar, and Sulaiman <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0008\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Brauner et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0015\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) can be deployed. The potential impact of AIG is transformational, including domains of public relations, marketing, advertising, ecommerce sites, retail brochures, chatbots, virtual agents, design, and others. The use of artificially generated facial images is generally free of copyright restrictions and can allow for a wide range of demographic diversity (age, gender, ethnicity). Nonetheless, these benefits only hold if the pictures are \u2018good enough\u2019 for real applications. Given the multiple application areas of AIG, the results of an evaluation study measuring the impact of artificial facial pictures on UX is of immediate interest for researchers and practitioners alike.</p><p>To address the call for user studies concerning AIG, we carry out two evaluation studies: (a) one addressing the overall <i>perceived quality</i> of artificial pictures among crowd workers, and (b) another addressing user perceptions when implementing the pictures for data-driven personas (DDPs). Our research question is: <i>Are artificially generated facial pictures \u2018good enough\u2019 for a system requiring substantial images of people?</i></p><p>DDPs are personas imaginary people representing real user segments, as defined traditionally in HCI (Cooper <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0023\" data-refLink=\"_i41 _i42\" href=\"#\">2004</a></span>) created from social media and Web analytics data (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0004\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>, <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>). Although the DDP process may vary system to system, most will have the six major steps shown in <a href=\"#F0001\">Figure 1</a>. <div class=\"figure figureViewer\" id=\"F0001\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 1. </span> Data-driven persona development approach. Six-step process common for most DDP methods.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0001image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0001_ob.jpg\"}' width=\"500\" height=\"37\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0001\"><p><span class=\"captionLabel\">Figure 1. </span> Data-driven persona development approach. Six-step process common for most DDP methods.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0001\"></div></p><p>The advantage of DDPs, relative to traditional personas (Brangier and Bornet <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0014\" data-refLink=\"_i41 _i42\" href=\"#\">2011</a></span>) that are manually created and typically include 3\u20137 personas per set (Hong et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0044\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), is that one can create hundreds of DDPs from the data to reflect different behavioural and demographic nuances in the underlying user population (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0082\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>). For example, a news organisation distributing its contents in social media platforms to audiences originating from dozens of countries can have dozens of audience segments relevant for different decision-making scenarios in other geographic areas (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0093\" data-refLink=\"_i41 _i42\" href=\"#\">2018e</a></span>). DDPs summarise these segments into easily approachable human profiles (see <a href=\"#F0002\">Figure 2</a> for example) that can be used within the organisation to understand the persona's needs (Nielsen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0069\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) and communicate (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0090\" data-refLink=\"_i41 _i42\" href=\"#\">2018d</a></span>) about these needs as a part of user-centric decision making (Idoughi, Seffah, and Kolski <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0046\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). <div class=\"figure figureViewer\" id=\"F0002\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 2. </span> Example of DDP. The persona has a picture (stock photo in this example), name, age, text description, topics of interest, quotes, most viewed contents, and audience size. The picture is purchased and downloaded manually from an online photobank; the practical goal of this research is to replace manual photo curation through automatic image generation.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0002image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0002_oc.jpg\"}' width=\"500\" height=\"428\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0002\"><p><span class=\"captionLabel\">Figure 2. </span> Example of DDP. The persona has a picture (stock photo in this example), name, age, text description, topics of interest, quotes, most viewed contents, and audience size. The picture is purchased and downloaded manually from an online photobank; the practical goal of this research is to replace manual photo curation through automatic image generation.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0002\"></div></p><p>One of the important issues for automatically creating DDPs from data is the availability of persona pictures \u2013 since DDP systems can create dozens of personas in near real time, there is a need for an inventory of pictures to use when rendering the personas for end users to view and interact with. Conceptually, this leads to a need for an AIG module that creates suitable persona pictures on demand (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). However, prior to engaging in system development, there is a need for ensuring that artificially generated pictures are not detrimental to user perceptions of the personas, or otherwise, one risks futile efforts with immature technology. In a sense, therefore, the question of picture quality for DDPs is also a question of feasibility study (of implementation).</p><p>Note that pictures constitute an essential element of the persona profile (Baxter, Courage, and Caine <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0012\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>; Nielsen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0070\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>). Pictures are instrumental for the persona to appear believable, and they have been found impactful for central persona perceptions, such as empathy (Probster, Haque, and Marsden <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0076\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>). Therefore, DDPs require these pictures in order to realise the many benefits associated with the use of personas in the HCI literature (Long <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0063\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>; Nielsen and Storgaard Hansen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0072\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>).</p><p>To evaluate the <i>quality</i>, we first generate a sample of 1,000 artificial facial pictures using a state-of-the-art generator. To evaluate this sample, we then obtain 6,812 judgments from crowdworkers. To evaluate <i>user perceptions</i>, we conduct a 2\u2009\u00d7\u20092 experiment with DDPs with a real/artificial picture. For measurement of user perceptions, we deploy the Persona Perception Scale (PPS) instrument (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>) to gauge the impact of artificial pictures on the DDPs\u2019 <i>authenticity</i> and <i>clarity</i>, as well as the sense of <i>empathy</i>, and <i>willingness to use</i> among the online pool of respondents.</p><p>Thus, our research goal is to evaluate artificially generated pictures across multiple dimensions for deployment in DDPs. Note that our goal is <i>not</i> to make a technical AIG contribution. Rather, we apply a pre-existing method for persona profiles and then evaluate the results for user perceptions. So, our contribution is in the area of practical design and implementation of AIG.</p><p>Note also that even though we focus on DDPs in this research, many other domains and use cases have similar needs in terms of requiring large collections of diverse facial images, including HCI and human-robot interaction such as avatars (Ablanedo et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0001\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; \u015eeng\u00fcn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0094\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>; Seng\u00fcn <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0095\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>), robots (dos Santos et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0028\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>; Duffy <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0029\" data-refLink=\"_i41 _i42\" href=\"#\">2003</a></span>; Edwards et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0030\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>; Holz, Dragone, and O\u2019Hare <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0043\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>), and chatbots (Araujo <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0007\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Go and Shyam Sundar <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0036\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Shmueli-Scheuer et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0097\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0108\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). Thus, our evaluation study has a cross-sectional value for other design purposes where artificial facial pictures would be useful.</p></div><div id=\"S002\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i6\" class=\"section-heading-2\">2. Related literature</h2><div id=\"S002-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i7\">2.1. Lack of evaluation studies for artificial pictures</h3><p>To quantify the need for evaluation studies of AIG in real systems, we carried out a scoping review (Bazzano et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0013\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) by extracting information from 20 research articles that generate artificial facial pictures. The articles were retrieved via Google Scholar using relevant search phrases (\u2018automatic image generation\u2009+\u2009faces\u2019, \u2018facial image creation\u2019, \u2018artificial picture generation\u2009+\u2009face\u2019, etc.) and focusing on peer-reviewed conference/journal articles published between 2015 and 2019. The list of articles, along with the extracted evaluation methods, is provided in <i>Supplementary Material</i>.</p><p>Results show that evaluation methods in these articles almost always contain one or more technical metrics (90%, <i>N</i>\u2009=\u200918) and always a short, subjective evaluation by the authors (100%, <i>N</i>\u2009=\u200920), in the line of \u2018manual inspection revealed some errors but generally good quality\u2019 (not an actual quote). Among the 20 articles, less than half (45%, <i>N</i>\u2009=\u20099) measured actual human perceptions (typically using crowdsourced ratings). More importantly, none of the articles provided an evaluation study that would implement the generated pictures into a real system or application. The results of this scoping review thus show a general lack of user studies for practical evaluation of AIG in real systems or use cases (0% of the research we could locate did so).</p><p>As stated, the evaluation of AIG focuses on technical metrics (Gao et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0034\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>) of image generation (e.g. inception score (Dey et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0024\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Di and Patel <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0025\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Salimans et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0080\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>; Yin et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0103\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>), FID (Dey et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0024\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Lin et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0060\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), Euclidean distance (Gecer et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0035\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), cosine similarity (Dey et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0024\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), reconstruction errors (Chen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0019\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Lee et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0058\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), or accuracy of face recognition (Di, Sindagi, and Patel <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0026\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Liu et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0062\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>)). Many of the technical metrics are said to have various strengths and weaknesses (Barratt and Sharma <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0010\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Shmelkov, Schmid, and Alahari <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0096\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Zhang et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0106\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>). The main weakness is that they do not capture user perceptions or UX ramifications of the pictures in real applications. This is because the technical metrics are not directly related to end-user experience when the user is observing the pictures within the <i>context</i> of their intended use (e.g. as part of DDPs).</p><p>Human evaluation studies, on the other hand, tend to focus on comparing the outputs of different algorithms, again ignoring the importance of context on the evaluation results. Typically, participants are asked to rank pictures produced using different algorithms from best to worst (Li et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0059\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Liu et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0062\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0109\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>) or rate the pictures by user perception metrics, such as realism, overall quality, and identity (Yin et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0103\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0109\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>). For example, Li et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0059\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) recruited 84 volunteers to rank three generated images out of 10 non-makeup and 20 makeup test images based on quality, realism, and makeup style similarity. Lee et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0058\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) employed a similar approach by asking users which image is more realistic out of samples created using different generation methods. Similarly, Choi et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0020\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) asked crowd workers in Amazon Mechanical Turk (AMT) to rank the generated images based on realism, quality of attribute transfer (hair colour, gender, or age), and preservation of the person's original identity. The participants were shown four images at a time, generated using different methods. Zhang et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0106\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) conducted a two-alternative forced choice (2AFC) test by asking AMT participants which of the provided pictures is more similar to a reference picture.</p><p>On rarer instances, user perception metrics, such as realism, overall quality, and identity have been deployed. For example, Zhou et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0108\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>) evaluated the quality of their generated results by asking if the participants consider the generated faces as realistic (\u2018yes\u2019 or \u2018no\u2019). In their study, 88.4% of the pictures were considered realistic. Iizuka, Simo-Serra, and Ishikawa (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0047\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) recruited ten volunteers to evaluate the \u2018naturalness\u2019 of the generated pictures; the volunteers were asked to guess if a picture was real or generated. Overall, 77% of the generated pictures were deemed to be real. Yin et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0103\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) asked students to compare 100 generated pictures with original pictures along with three criteria: (1) <i>saliency</i> (the degree of the attributes that has been changed in the picture), (2) <i>quality</i> (the overall quality of the picture), and (3) <i>identity</i> (if the generated and the original picture are the same person). Their AIG method achieved an average quality rating of 4.20 out of 5. While these studies are closer to the realm of UX, we could not locate previous research that would (a) <i>investigate the effect of artificial pictures on UX of a real system</i>, or (b) <i>evaluate the impact of using artificially generated pictures on user perceptions</i>. However, evaluating AIG approaches for user perceptions and UX in real systems, is crucial for determining the success of AIG in real usage contexts for design, HCI, and various other areas of application (\u00d6zmen and Yucel <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0073\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>).</p></div><div id=\"S002-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i8\">2.2. Data-driven persona development</h3><p>A persona is a fictive person that describes a user or customer segment (Cooper <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0022\" data-refLink=\"_i41 _i42\" href=\"#\">1999</a></span>). Originating from HCI, personas are used in various domains, such as user experience/design (Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>), marketing (Jenkinson <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0052\" data-refLink=\"_i41 _i42\" href=\"#\">1994</a></span>), and online analytics (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0082\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>) to increase the empathy by designers, software developers, marketers (etc.) toward the users or customers of a product (Dong, Kelkar, and Braun <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0027\" data-refLink=\"_i41 _i42\" href=\"#\">2007</a></span>). Personas make it possible for decision makers to see use cases \u2018through the eyes of the user\u2019 (Goodwin <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0038\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>) and facilitate communication between team members through shared mental models (Pruitt and Adlin <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0077\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>). Researchers are increasingly developing methodologies for DDPs (McGinn and Kotamraju <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0066\" data-refLink=\"_i41 _i42\" href=\"#\">2008</a></span>; Zhang, Brown, and Shankar <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0105\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>) and automatic persona generation (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0004\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>; An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>), mainly due to the increase in the availability of online user data and to increase the robustness of personas given the alternative forms of user understanding (Jansen, Salminen, and Jung <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0049\" data-refLink=\"_i41 _i42\" href=\"#\">2020</a></span>). DDPs typically leverage quantitative social media and online analytics data to create personas that represent users or customers of a specific channel<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0001\" href=\"#\" data-refLink=\"fn\"><sup>1</sup></a></span> (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0092\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>). Regarding the development of DDPs, for the generated pictures to be useful for personas, they need to be \u2018taken for real\u2019, meaning that they do not hinder the user perceptions of the personas (e.g. not reduce the persona's authenticity).</p></div><div id=\"S002-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i9\">2.3. Persona user perceptions</h3><p>Evaluation of user perceptions has been noted as a major concern of personas. Scholars have observed that personas need justification, mainly for their accuracy and usefulness in real organisations and usage scenarios (Chapman and Milham <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0018\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>; Friess <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0033\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>; Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). Prior research typically examines persona user perceptions via case studies (Faily and Flechais <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0032\" data-refLink=\"_i41 _i42\" href=\"#\">2011</a></span>; Jansen, Van Mechelen, and Slegers <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0050\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Nielsen and Storgaard Hansen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0072\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>), ethnography (Friess <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0033\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>), usability standards (Long <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0063\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>), or using statistical evaluation (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>; Brickey, Walczak, and Burgess <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0016\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>; Zhang, Brown, and Shankar <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0105\" data-refLink=\"_i41 _i42\" href=\"#\">2016</a></span>). For example, Friess (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0033\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>) investigated the adoption of personas among designers. Long (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0063\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>) measured the effectiveness of using personas as a design tool, using Nielsen's usability heuristics. Nielsen et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0071\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>) analyze the match between journalists\u2019 preconceptions and personas created from the audience data, whereas Chapman et al. (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0017\" data-refLink=\"_i41 _i42\" href=\"#\">2008</a></span>) evaluate personas as quantitative information. While these evaluation approaches are interesting, survey methods provide a lucrative alternative for understanding how end users perceive personas. Survey research typically measures perceptions as latent constructs, apt for measurement of attitudes and perceptions that cannot be directly observed (Barrett <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0011\" data-refLink=\"_i41 _i42\" href=\"#\">2007</a></span>). This approach seems intuitively compatible with personas, as researchers have reported several attitudinal perceptions concerning personas (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0085\" data-refLink=\"_i41 _i42\" href=\"#\">2019c</a></span>). These are captured in the PPS survey instrument (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0088\" data-refLink=\"_i41 _i42\" href=\"#\">2019f</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0091\" data-refLink=\"_i41 _i42\" href=\"#\">2019g</a></span>) that includes eight constructs and twenty-eight items to measure user perceptions of personas. We deploy this instrument in this research, as it covers essential user perceptions in the persona context.</p></div><div id=\"S002-S2004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i10\">2.4. Hypotheses</h3><p>Following prior persona research, we formulate the following hypotheses to test persona user perceptions. <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">H01: Using artificial pictures does not decrease the authenticity of the persona. HCI research has shown that authenticity (or <i>credibility</i>, <i>believability</i>) is a crucial issue for persona acceptance in real organisations \u2014 if the personas come across as \u2018fake\u2019, decision makers are unlikely to adopt them for use (Chapman and Milham <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0018\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>; Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). This is especially relevant for our context because personas already are fictitious people describing real user groups (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>), so we need to ensure that enhancing these fictitious people with artificially generated pictures does not further risk the perception of realism.</p></li><li><p class=\"inline\">H02: Using artificial pictures does not decrease the clarity of the persona profile. For personas to be useful, they should not be abstract or misleading (Matthews, Judge, and Whittaker <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0065\" data-refLink=\"_i41 _i42\" href=\"#\">2012</a></span>). HCI researchers have found that personas with inconsistent information make end users of personas confused (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0090\" data-refLink=\"_i41 _i42\" href=\"#\">2018d</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0084\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>). Again, we need to ensure that artificial pictures do not make persona profiles more \u2018messy\u2019 or unclear for the end users.</p></li><li><p class=\"inline\">H03: Using artificial pictures does not decrease empathy towards the persona. Empathy is considered, among HCI scholars, as a key advantage of personas compared to other forms of presenting user data (Cooper <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0022\" data-refLink=\"_i41 _i42\" href=\"#\">1999</a></span>; Nielsen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0069\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). The generated personas need to \u2018resonate\u2019 with end users to make a real impact. Therefore, to be successful, artificial pictures should not reduce the sense of empathy towards the persona.</p></li><li><p class=\"inline\">H04: Using artificial pictures does not decrease the willingness to use the persona. Willingness to use (WTU) is a crucial construct for the adoption of personas for practical decision making (R\u00f6nkk\u00f6 <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0078\" data-refLink=\"_i41 _i42\" href=\"#\">2005</a></span>; R\u00f6nkk\u00f6 et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0079\" data-refLink=\"_i41 _i42\" href=\"#\">2004</a></span>). HCI research has shown that if persona users do not show a willingness to learn more about the persona for their task at hand, persona creation risks remaining a futile exercise (R\u00f6nkk\u00f6 et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0079\" data-refLink=\"_i41 _i42\" href=\"#\">2004</a></span>).</p></li></ul></p><p>Overall, ranking high on these perceptions is considered positive (desirable) within the HCI literature. This leads to defining the \u2018good enough\u2019 quality of artificial pictures in the DDP context such that <i>a \u2018good enough\u2019 picture quality does not decrease (a) the authenticity (i.e. the persona is still considered as \u2018real\u2019 as with real photographs), (b) clarity of the persona profile, (c) the sense of empathy felt toward the persona, or (d) the willingness to learn more about the persona.</i> In other words, it is the design goal of replacing real photographs with artificial pictures in the context of personas, with the concept being transferrable to other domains.</p></div></div><div id=\"S003\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i11\" class=\"section-heading-2\">3. Methodology</h2><div id=\"S003-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i12\">3.1. Overview of evaluation steps</h3><p>Our evaluation of picture quality consists of two separate studies: (1) crowdsourced evaluation study of AIG quality, and (2) user study measuring the perceptions of an online panel concerning personas with artificially generated pictures. The latter study tests if DDPs are perceived differently when using artificial pictures, while addressing the hypotheses presented in the previous section.</p></div><div id=\"S003-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i13\">3.2. Research context</h3><p>Our research context is a DDP system: <i>Automatic Persona Generation</i> (APG<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0002\" href=\"#\" data-refLink=\"fn\"><sup>2</sup></a></span>). As a DDP system, APG requires thousands of realistic facial pictures to produce a wide range of believable persona profiles for client organisations (Pruitt and Adlin <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0077\" data-refLink=\"_i41 _i42\" href=\"#\">2006</a></span>) covering a wide range of ages and ethnicities. An overview of the typical DDP development process is presented in <a href=\"#F0003\">Figure 3</a>. <div class=\"figure figureViewer\" id=\"F0003\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 3. </span> APG data and processing flowchart from server configuration to data collection and persona generation.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0003image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0003_oc.jpg\"}' width=\"500\" height=\"172\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0003\"><p><span class=\"captionLabel\">Figure 3. </span> APG data and processing flowchart from server configuration to data collection and persona generation.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0003\"></div></p><p>A practical limitation of APG is the need for manually acquiring facial pictures for the persona profiles (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>). Because the pictures for APG are acquired from online stock photo banks (e.g. iStockPhoto, 123rf.com, etc.), manual effort is required to curate a large number of pictures. A large number of pictures is needed because APG can generate thousands of personas for client organisations \u2013 for each persona, a unique facial picture is required. Organisations over a lengthy period can have dozens of unique personas. Using stock photo banks also involves a financial cost (ranging from $1 to $20 USD per picture), making picture curation both time-consuming and costly. Given the goal of fully automated persona generation (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0083\" data-refLink=\"_i41 _i42\" href=\"#\">2019a</a></span>), there is a practical need for automatic image generation.</p><p>Thus, we evaluate the automatically generated facial pictures for use in APG (Jung et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0053\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>; Jung et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0054\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>). APG generates personas from online analytics and social media data (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0086\" data-refLink=\"_i41 _i42\" href=\"#\">2019d</a></span>). <a href=\"#F0002\">Figure 2</a> shows an example of a persona generated using the system. The practical purpose of automatically generated images is to replace the manual curation of persona profile pictures, saving time and money. Note that the cost and effort are not unique problems of APG, but generalise to all similar images systems, as the pictures need to be provided for each new persona generated.</p></div><div id=\"S003-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i15\">3.3. Deploying StyleGAN for persona pictures</h3><p>For AIG, we utilise a pre-trained version of <i>StyleGAN</i> (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), a state-of-the-art generator that represents a leap towards photorealistic facial pictures and can be freely accessed on GitHub.<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0003\" href=\"#\" data-refLink=\"fn\"><sup>3</sup></a></span> StyleGAN was chosen for this research because (a) it is a leap toward generating photorealistic facial images, especially relative to the previous state-of-art, (b) the trained model is publicly available, and (c) its deployment is robust for possible use in real systems. StyleGAN generated the images, so this is a back end process.</p><p>We use a pretrained model from the creators of StyleGAN (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). This model was trained on CelebA-HQ and FFHQ datasets using eight Tesla V100\u2005GPUs. It is implemented in TensorFlow,<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0004\" href=\"#\" data-refLink=\"fn\"><sup>4</sup></a></span> an open-source machine learning library and is available in a GitHub repository.<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0005\" href=\"#\" data-refLink=\"fn\"><sup>5</sup></a></span> We access this pre-trained model via the GitHub repository that contains the model and the required source code to run it.</p><p>Our goal is to use this pre-trained model to generate a sample of 1,000 realistic facial pictures. The method of applying the published code to generate the pictures is straightforward. We provide the exact steps below to facilitate replication studies: <div class=\"quote\"><p>\u2022 <b>Step 1:</b> Import the required Python packages (<i>os</i>, <i>pickle</i>, <i>numpy</i>, from <i>PIL</i>: <i>Image</i>, <i>dnnlib</i>).</p></div> <div class=\"quote\"><p>\u2022 <b>Step 2:</b> Define the parameters and paths</p></div> <div class=\"quote\"><p>\u2022 <b>Step 3:</b> Initialize the environment and load the pretrained StyleGAN model.</p></div> <div class=\"quote\"><p>\u2022 <b>Step 4:</b> Set random states and generate new random input. Randomization is needed because the model always generates the same face for a particular input vector. To generate unique images, a unique set of input arrays should be provided. This is done by setting a random state equal to the current number of iterations, which allows us to have unique images and reproducible results at the same time.</p></div> <div class=\"quote\"><p>\u2022 <b>Step 5:</b> Generate images using the random input array created in the previous step.</p></div> <div class=\"quote\"><p>\u2022 <b>Steps 6:</b> Save the generated images as files to the output folder. We use the resolution of 1024\u2009\u00d7\u20091024 pixels. Other available resolutions are 512\u2009\u00d7\u2009512\u2005px and 256\u2009\u00d7\u2009256\u2005px.</p></div></p><p>The above steps with the mentioned parameters enable us to generate artificial pictures with similar quality to those in the StyleGAN research paper (Karras, Laine, and Aila <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0055\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). For replicability, we are sharing the Python code we used for AIG in <i>Supplementary Material</i>.</p></div></div><div id=\"S004\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i16\" class=\"section-heading-2\">4. STUDY 1: crowdsourced evaluation</h2><div id=\"S004-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i17\">4.1. Method</h3><p>We evaluate the human-perceived quality of 1,000 generated facial pictures. To facilitate comparison with prior work using human evaluation for artificial pictures (Choi et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0020\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Song et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0098\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>; Zhang et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0106\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>), we opt for crowdsourcing, using Figure Eight to collect the ratings. This platform has been widely used for gathering manually annotated training data (Alam, Ofli, and Imran <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0002\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>) and ratings (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0081\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>) in various subdomains of computer science. The pictures were shown in the full 1024\u2009\u00d7\u20091024 pixels format to provide the crowd raters enough detail for a valid evaluation. The following task description was provided to the crowd raters, including the quality criteria and examples:</p><p><i>You are shown a facial picture of a person. Look at the picture and choose how well it represents a real person. The options:</i> <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">5: Perfect\u2014the picture is indistinguishable from a real person.</p></li><li><p class=\"inline\">4: High quality\u2014the picture has minor defects, but overall it's pretty close to a real person.</p></li><li><p class=\"inline\">3: Medium quality\u2014the picture has some flaws that suggest it's not a real person.</p></li><li><p class=\"inline\">2: Low quality\u2014the picture has severe malformations or defects that instantly show it's a fake picture.</p></li><li><p class=\"inline\">1: Unusable\u2014the picture does not represent a person at all.</p></li></ul></p><p>We also clarified to the participants that the use case is to find realistic pictures specifically for persona profiles, explaining that these are descriptive people of some user segment. Additionally, we indicated in the title that the task is to evaluate <i>artificial</i> pictures of people, to manage the expectations of the crowd raters accordingly (Pitk\u00e4nen and Salminen <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0075\" data-refLink=\"_i41 _i42\" href=\"#\">2013</a></span>). Other than the persona aspect, these are similar to guidelines used in prior work to facilitate image comparisons.</p><p>Following the quality control guidelines for crowdsourcing by Huang, Weber, and Vieweg (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0045\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>) and Alonso (<span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0003\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>), we implemented suitable parameters in the Figure Eight platform. We also enabled <i>Dynamic judgments</i>, meaning the platform automatically collects more ratings when there is a higher disagreement among the raters. Based on the results of a pilot study with 100 pictures, not used in the final research, we set the maximum number of ratings to 5 and confidence goal to 0.65. The default number of raters was three, so the platform only went to 5 raters if a 0.65 confidence was not achieved.<span class=\"ref-lnk fn-ref-lnk lazy-ref\"><a data-rid=\"EN0006\" href=\"#\" data-refLink=\"fn\"><sup>6</sup></a></span></p></div><div id=\"S004-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i18\">4.2. Results</h3><p>We spent $266.98 USD to obtain 6,812 crowdsourced image ratings. This was the number of evaluations from trusted contributors, not including the test questions. Note that if the accuracy of a crowd rater's ratings relative to the test questions falls below the minimum accuracy threshold (in our case, 80%), the rater is disqualified, and the evaluations become untrusted. There were 423 untrusted judgments (6% of the total submitted ratings), i.e. ratings coming from contributors that continuously fail to correctly rate the test pictures. Thus, 94% of the total ratings were deemed trustworthy. The majority label for each rated picture is assigned by comparing the confidence-adjusted ratings of each available class, calculated as follows: <span class=\"NLM_disp-formula-image disp-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_um0001.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_um0001.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula disp-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>Confidenc</mtext></mstyle><msub><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>e</mtext></mstyle><mrow><mrow><mi mathvariant=\"normal\">class</mi></mrow></mrow></msub><mo>=</mo><mstyle displaystyle=\"true\" scriptlevel=\"0\"><mrow><mfrac><mrow><msubsup><mrow><mo movablelimits=\"false\">\u2211</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>\u2061</mo><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>trus</mtext></mstyle><msub><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>t</mtext></mstyle><mrow><mrow><mi mathvariant=\"normal\">class</mi></mrow></mrow></msub></mrow><mrow><msubsup><mrow><mo movablelimits=\"false\">\u2211</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>\u2061</mo><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>trus</mtext></mstyle><msub><mstyle displaystyle=\"false\" scriptlevel=\"0\"><mtext>t</mtext></mstyle><mrow><mrow><mi mathvariant=\"normal\">all</mi></mrow></mrow></msub></mrow></mfrac></mrow><mo>,</mo><mspace width=\"thickmathspace\"></mspace></mstyle></math></span>where the confidence score of the class is given by the sum of the trust scores from all <i>n</i> raters of that picture. The trust score is based on a crowdworker's historical accuracy (relative to test questions) on all the jobs he/she has participated in. For example, if the confidence score of \u2018perfect\u2019 is 0.66 and \u2018medium quality\u2019 is 0.72, then the chosen majority label is \u2018medium quality\u2019 (0.72\u2009&gt;\u20090.66).</p><p>The results (see <a class=\"ref showTableEventRef\" data-ID=\"T0001\">Table 1</a>) show \u2018High quality\u2019 as the most frequent class. Sixty percent (60%) of the generated pictures are rated as either \u2018Perfect\u2019 or \u2018High quality\u2019. The average quality score was 3.7 out of 5 (SD\u2009=\u20090.91) when calculated from majority votes and 3.8 when calculated from all the ratings. 9.9% of the pictures were rated as \u2018Low quality\u2019, and none was rated as \u2018Unusable\u2019. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 1. The results of crowd evaluation based on a majority vote of the picture quality. Most frequent class bolded. Example facial image from each of the 5 classes shown for comparison.</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"false\" id=\"T0001-table-wrapper\"><a data-id=\"T0001\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p></div><div id=\"S004-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i21\">4.3. Reliability analysis</h3><p>To assess the reliability of the crowd ratings, we measured the interrater agreement of the quality ratings among crowdworkers. For this, we used two metrics: <i>Gwet's AC1</i> (AC1) and <i>percentage agreement</i> (PA). Using AC1 is appropriate when the outcome is ordinal, the number of ratings varies across items (Gwet <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0039\" data-refLink=\"_i41 _i42\" href=\"#\">2008</a></span>) and where the Kappa metric is low despite a high level of agreement (Banerjee et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0009\" data-refLink=\"_i41 _i42\" href=\"#\">1999</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0081\" data-refLink=\"_i41 _i42\" href=\"#\">2018a</a></span>). Because of these properties, we chose AC1 with ordinal weights as the interrater agreement metric. In addition, PA was calculated as a simple baseline measure. Standard errors were used to construct the 95% confidence interval (CI) for AC1. For PA, 95% CI was calculated using 100 bootstrapped samples.</p><p>Results (see <a class=\"ref showTableEventRef\" data-ID=\"T0002\">Table 2</a>) show a high PA agreement (86.2%). The interrater reliability was 0.627, in the range of <i>good</i> (i.e. 0.6\u22120.8) (Wongpakaran et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0101\" data-refLink=\"_i41 _i42\" href=\"#\">2013</a></span>). The results were statistically significant (<i>p</i>\u2009&lt;\u20090.001), with the probability of observing such results by chance is less than 0.1%. Therefore, the crowd ratings can be considered to have satisfactory internal validity. However, the quality of some pictures is more easily agreed upon than others. When stratified, the overall agreement and AC1 were similar across <i>low</i>, <i>moderate</i>, and <i>high</i> quality labels (PA \u223c 85%, and AC1 \u223c 0.75). However, the agreement was lower when the picture was rated perfect (PA\u2009=\u200976.7%, AC1\u2009=\u20090.498). This implies that \u2018perfect\u2019 is more difficult to determine than the other rating labels. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 2. Agreement metrics for the crowdsourced ratings showing satisfactory internal validity.</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"T0002-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=T0002&amp;doi=10.1080%2F0144929X.2020.1838610&amp;downloadType=CSV\">CSV</a><a data-id=\"T0002\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p></div></div><div id=\"S005\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i23\" class=\"section-heading-2\">5. STUDY 2: effects on persona perceptions</h2><div id=\"S005-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i24\">5.1. Experiment design</h3><p>We created two base personas using the APG (An et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0005\" data-refLink=\"_i41 _i42\" href=\"#\">2018b</a></span>) methodology described previously; one male and one female. We leave the evaluation of other genders for future research. The experiment variable is the use of an artificial image in the persona profile. The other elements of the persona profiles are identical between the two treatments. For this, we manipulated the base personas by introducing either (a) <i>a real photograph of a person</i> or (b) a <i>demographically matching artificial picture</i> (see <a href=\"#F0004\">Figure 4</a>). <div class=\"figure figureViewer\" id=\"F0004\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 4. </span> Artificial male picture [A], Real male picture [B], Artificial female picture [C], and Real female picture [D]. Among the male/female personas, all other content in the persona profile was the same except the picture that alternated between Artificial and Real. Pictures of the full persona profiles are provided in Supplementary Material.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0004image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0004_oc.jpg\"}' width=\"500\" height=\"163\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0004\"><p><span class=\"captionLabel\">Figure 4. </span> Artificial male picture [A], Real male picture [B], Artificial female picture [C], and Real female picture [D]. Among the male/female personas, all other content in the persona profile was the same except the picture that alternated between Artificial and Real. Pictures of the full persona profiles are provided in Supplementary Material.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0004\"></div></p><p>The demographic match was determined manually by two researchers who judged that the chosen pictures were similar for gender, age, and race. Using a modified Delphi method, a seed image of either a real or article picture was select using the meta-data attributes of gender, age, and race. The researchers independently selected matching images for each. The two researchers then jointed selected the mutually agreed upon image for the treatments. The artificial pictures were chosen from the ones rated \u2018perfect\u2019 by the crowd raters. The real photos were sourced from online repositories, with Creative Commons license.</p><p>In total, four persona treatments were created: <i>Male Persona with Real Picture</i> (MPR), <i>Male Persona with Artificial Picture</i> (MPA), <i>Female Persona with Real Picture</i> (FPR), and <i>Female Persona with Artificial Picture</i> (FPA). The created personas were mixed into four sequences: <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\"><b>Sequence 1:</b> MPR \u2192 FPA</p></li><li><p class=\"inline\"><b>Sequence 2:</b> MPA \u2192 FPR</p></li><li><p class=\"inline\"><b>Sequence 3:</b> FPR \u2192 MPA</p></li><li><p class=\"inline\"><b>Sequence 4:</b> FPA \u2192 MPR</p></li></ul></p><p>Each participant was randomly assigned to one of the sequences. To counterbalance the dataset, we ensured an even number of participants (<i>N</i>\u2009=\u2009520/4\u2009=\u2009130) for each sequence. Technically, the participants self-selected the sequence, as each participant could only take one survey. The participants were excluded from answering in more than one survey based on their (anonymous) Respondent ID. The gender distribution for each of the four sequences, as shown: S1 (<i>M</i>: 41.5% <i>F</i>: 58.5%), S2 (<i>M</i>: 39.0% <i>F</i>: 61.0%), S3 (<i>M</i>: 39.8% <i>F</i>:60.2%), S4 (<i>M</i>: 34.5% <i>F</i>: 64.5%).</p></div><div id=\"S005-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i26\">5.2. Recruitment of participants</h3><p>We created a survey for each sequence. In each survey, we (a) explain to participants what the research is about and what personas are (\u2018<i>a persona is defined as a fictive person describing a specific customer group</i>\u2019). Then, we (b) show an example persona with explanations of the content, and (c) explain the task scenario (\u2018<i>Imagine that you are creating a YouTube video for the target group that the persona you will be shown next describes</i>\u2019.). After this, (d) the participants are shown one of the four treatments, asked to review the information carefully, and complete the PPS questionnaire.</p><p>In total, 520 participants were recruited using Prolific, an online survey platform often applied in social science research (Palan and Schitter <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0074\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>). Prolific was chosen for this evaluation step (as opposed to previously used Figure Eight), as it provides background information of the participants (e.g. gender, age) that can be deployed for further analyses. The average age of the participants was 35 years old (SD\u2009=\u20097.2), with 59.1% being female, overall. The nationality of the participants was the United Kingdom, they had at least an undergraduate degree, and none were students. We verified the quality of the answers using an attention check question (\u2018<i>It's important that you pay attention to this study. Please select \u201cSlightly agree\u201d\u2019</i>.). Out of 520 answers, 19 (3.7%) failed the attention check; these answers were removed. In addition, five answers were timed out by the Prolific platform. Therefore, we ended up with 496 qualified participants (95.4% of the total).</p></div><div id=\"S005-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i27\">5.3. Measurement</h3><p>The perceptions are measured using the PPS (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>), a survey instrument measuring what individuals think about specific personas (see <a class=\"ref showTableEventRef\" data-ID=\"T0003\">Table 3</a>). The PPS has previously deployed in several persona experiments (see [Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0088\" data-refLink=\"_i41 _i42\" href=\"#\">2019f</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0091\" data-refLink=\"_i41 _i42\" href=\"#\">2019g</a></span>]). Note that the <i>authenticity</i> construct is similar to constructs in earlier artificial image evaluation \u2013 specifically to <i>realism</i> (Zhou et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0109\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>) and <i>naturalness</i> (Iizuka, Simo-Serra, and Ishikawa <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0047\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>). However, the other constructs expand the perceptions typically used for image evaluation. In this sense, the hypotheses (a) add novelty to the measurement of user perceptions regarding the employment of artificial images in a real system output, and (b) are relevant for the design and use of personas. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 3. Survey statements. The participants answered using a 7-point Likert scale, ranging from Strongly Disagree to Strongly Agree. The statements were validated in (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>). WTU \u2013 willingness to use.</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"true\" id=\"T0003-table-wrapper\"><a class=\"downloadButton btn btn-sm\" role=\"button\" href=\"/action/downloadTable?id=T0003&amp;doi=10.1080%2F0144929X.2020.1838610&amp;downloadType=CSV\">CSV</a><a data-id=\"T0003\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p></div><div id=\"S005-S2004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i29\">5.4. Analysis procedure</h3><p>The participants were grouped based on the persona presented (either the male or the female one), and whether the persona picture was artificial or real. The data was re-arranged to disentangle the gender of the persona, leading to one male-persona dataset (with a \u2018real\u2019 and an \u2018artificial\u2019 group), and a female-persona dataset with similar groups. This allowed the usage of a standard MANOVA (Hair et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0040\" data-refLink=\"_i41 _i42\" href=\"#\">2009</a></span>) to determine whether the measurements differed across artificial and real pictures; both genders were analysed independently.</p><p>To enhance the robustness of the findings, Bayesian independent samples tests were used to estimate Bayesian Factors (BF), comparing the likelihoods between the null and alternative hypotheses (Lee <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0057\" data-refLink=\"_i41 _i42\" href=\"#\">2014</a></span>). A Na\u00efve Bayes approach was employed with regards to priors.</p></div><div id=\"S005-S2005\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i30\">5.5. Results</h3><p><b>Male persona:</b> Beginning with the multivariate tests, no significant effects were registered for Type of Picture (Pillai's Trace\u2009=\u20090.017, <i>F</i>(5, 476)\u2009=\u20091.651, <span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0001.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0001.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup><mo>=</mo><mn>0.017</mn></math></span>, <i>p</i>\u2009=\u20090.145), indicating that none of the measurements differed across male real and artificial pictures for the persona profile. Nevertheless, we proceeded with an analysis of univariate tests, which confirmed that none of the measurements differed across types of pictures. The univariate differences for between-subjects are summarised in <a class=\"ref showTableEventRef\" data-ID=\"T0004\">Table 4</a>. <div class=\"tableViewerArticleInfo hidden\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"tableView\"><div class=\"tableCaption\"><div class=\"short-legend\"><h3><b>Table 4. Univariate tests for between-subjects effects (df(error)\u2009=\u20091(480)).</b></h3></div></div><div class=\"tableDownloadOption\" data-hasCSVLnk=\"false\" id=\"T0004-table-wrapper\"><a data-id=\"T0004\" class=\"downloadButton btn btn-sm displaySizeTable\" href=\"#\" role=\"button\">Display Table</a></div></div></p><p>The lack of differences in scale ratings (see <a href=\"#F0005\">Figure 5</a>) also indicates that the use of real or artificial pictures results in no differences for <i>authenticity</i>, <i>clarity</i>, <i>empathy</i>, or <i>willingness to use</i> for the male persona. <div class=\"figure figureViewer\" id=\"F0005\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 5. </span> Means of the scale variables for the <i>male</i> persona. Error bars indicate standard error.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0005image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0005_oc.jpg\"}' width=\"500\" height=\"252\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0005\"><p><span class=\"captionLabel\">Figure 5. </span> Means of the scale variables for the <i>male</i> persona. Error bars indicate standard error.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0005\"></div></p><p>The Bayesian analysis on the male persona indicates strong lack of evidence for differences regarding <i>clarity</i> (BF\u2009=\u200913.856; <i>F</i>(1, 480)\u2009=\u20090.002; <i>p</i>\u2009=\u20090.965) and <i>willingness to use</i> (BF\u2009=\u200910.030; <i>F</i>(1, 480)\u2009=\u20090.658; <i>p</i>\u2009=\u20090.418), and moderate lack of evidence for differences regarding <i>authenticity</i> (BF\u2009=\u20095.126; <i>F</i>(1, 480)\u2009=\u20092.023; <i>p</i>\u2009=\u20090.156) and <i>empathy</i> (BF\u2009=\u20095.040; <i>F</i>(1, 480)\u2009=\u20092.057; <i>p</i>\u2009=\u20090.152) (Jeffreys <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0051\" data-refLink=\"_i41 _i42\" href=\"#\">1998</a></span>).</p><p><b>Female persona:</b> Beginning with the multivariate tests, unlike with the male persona, significant effects were registered for Type of Picture (Pillai's Trace\u2009=\u20090.051, <i>F</i>(5, 476)\u2009=\u20095.081, <span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0004.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0004.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup><mo>=</mo><mn>0.051</mn></math></span>, <i>p</i>\u2009&lt;\u20090.001), indicating that at least one of the measurements differed between real and artificial pictures for the female persona. Thus, we proceeded with univariate testing to determine which of the measurements exhibited differences across picture type (see <a class=\"ref showTableEventRef\" data-ID=\"T0004\">Table 4</a>). <i>Authenticity</i> had significant differences across types of picture (BF\u2009=\u20090.032; <i>F</i>(1, 480)\u2009=\u200912.479, <i>p</i>\u2009&lt;\u20090.001). Artificial female pictures were perceived as more <i>authentic</i> (<i>M</i>\u2009=\u20095.075, SD\u2009=\u20091.016) than real pictures (<i>M</i>\u2009=\u20094.711, SD\u2009=\u20091.235). None of the other measurements differed across types of pictures. <a href=\"#F0006\">Figure 6</a> illustrates the comparison between the two groups for the female persona. <div class=\"figure figureViewer\" id=\"F0006\"><div class=\"hidden figureViewerArticleInfo\"><span class=\"figViewerTitle\">Using artificially generated pictures in customer-facing systems: an evaluation study with data-driven personas</span><div class=\"articleAuthors articleInfoSection\"><div class=\"authorsHeading\">All authors</div><div class=\"authors\"><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Salminen%2C+Joni\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Salminen%2C+Joni\"><span class=\"NLM_given-names\">Joni</span> Salminen</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jung%2C+Soon-gyo\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jung%2C+Soon-gyo\"><span class=\"NLM_given-names\">Soon-gyo</span> Jung</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Kamel%2C+Ahmed+Mohamed+Sayed\"><span class=\"NLM_given-names\">Ahmed Mohamed Sayed</span> Kamel</a></span>, </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Santos%2C+Jo%C3%A3o+M\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Santos%2C+Jo%C3%A3o+M\"><span class=\"NLM_given-names\">Jo\u00e3o M.</span> Santos</a></span> &amp; </a><a class=\"entryAuthor\" href=\"/action/doSearch?Contrib=Jansen%2C+Bernard+J\"><span class=\"hlFld-ContribAuthor\"><a href=\"/author/Jansen%2C+Bernard+J\"><span class=\"NLM_given-names\">Bernard J.</span> Jansen</a></span></a></div></div><div class=\"articleLowerInfo articleInfoSection\"><div class=\"articleLowerInfoSection articleInfoDOI\"><a href=\"https://doi.org/10.1080/0144929X.2020.1838610\">https://doi.org/10.1080/0144929X.2020.1838610</a></div><div class=\"articleInfoPublicationDate articleLowerInfoSection border\"><h6>Published online:</h6>06 November 2020</div></div></div><div class=\"figureThumbnailContainer\"><div class=\"figureInfo\"><td align=\"left\" valign=\"top\" width=\"100%\"><div class=\"short-legend\"><p><span class=\"captionLabel\">Figure 6. </span> Means of the scale variables for the <i>female</i> persona. Error bars indicate standard error.</p></div></td></div><a href=\"#\" class=\"thumbnail\" aria-label=\"thumbnail image\"><img id=\"F0006image\" src=\"//:0\" data-src='{\"type\":\"image\",\"src\":\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_f0006_oc.jpg\"}' width=\"500\" height=\"252\" /></a><div class=\"figureDownloadOptions\"><a href=\"#\" class=\"downloadBtn btn btn-sm\" role=\"button\">Display full size</a></div></div></div><div class=\"hidden rs_skip\" id=\"fig-description-F0006\"><p><span class=\"captionLabel\">Figure 6. </span> Means of the scale variables for the <i>female</i> persona. Error bars indicate standard error.</p></div><div class=\"hidden rs_skip\" id=\"figureFootNote-F0006\"></div></p><p>This was corroborated by the Bayesian Factors that indicate that strong lack of evidence regarding differences for <i>clarity</i> (BF\u2009=\u200913.865; <i>F</i>(1, 480)\u2009=\u20090.001, <i>p</i>\u2009=\u20090.980) and <i>willingness to use</i> (BF\u2009=\u200913.828; <i>F</i>(1, 480)\u2009=\u20090.006, <i>p</i>\u2009=\u20090.938), and moderate lack of evidence for <i>empathy</i> (BF\u2009=\u20098.290; <i>F</i>(1, 480)\u2009=\u20091.045, <i>p</i>\u2009=\u20090.307) (Jeffreys <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0051\" data-refLink=\"_i41 _i42\" href=\"#\">1998</a></span>).</p><p>Finally, as one the statements in PPS specifically dealt with the picture of the persona (Item 3: \u2018<i>The picture of the persona looks authentic</i>\u2019.), we inspected the mean scores of this statement separately. In line with our other findings, the artificial female persona picture is in fact considered to be more authentic than the real photograph (M<sub>FPA</sub>\u2009=\u20095.74 vs. M<sub>FPR</sub>\u2009=\u20094.89). This difference is statistically significant (<i>t</i>(480)\u2009=\u20096.896, <i>p</i>\u2009&lt;\u20090.001). For the male persona, differences are minimal (M<sub>MPA</sub>\u2009=\u20095.11 vs. M<sub>MPR</sub>\u2009=\u20095.14) and not statistically significant (<i>t</i>(480)\u2009=\u2009\u22120.187, <i>p</i>\u2009=\u20090.851).</p><p>In summary, for H01, there were no significant differences in the perceptions for the male persona; however, for the female persona, artificial pictures actually <i>increased</i> the perceived authenticity. For the other perceptions, there was no significant change when replacing the real photo with the artificially generated picture. Therefore, <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">There was no evidence that using artificial pictures decrease the perceived authenticity of the persona (H01: supported).</p></li><li><p class=\"inline\">There was no evidence that using artificial pictures decrease the clarity of the persona profile (H02: supported).</p></li><li><p class=\"inline\">There was no evidence that using artificial pictures decrease empathy towards the persona (H03: supported).</p></li><li><p class=\"inline\">There was no evidence that using artificial pictures decrease the willingness to use the persona (H04).</p></li></ul></p></div></div><div id=\"S006\" class=\"NLM_sec NLM_sec-type_discussion NLM_sec_level_1\"><h2 id=\"_i34\" class=\"section-heading-2\">6. Discussion</h2><div id=\"S006-S2001\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i35\">6.1. Can artificial pictures be used for DDPs?</h3><p>Our analysis focuses on a timely problem in a relevant, yet underexplored area. However, it is one of increasing importance in a media rich online environment (Church, Iyer, and Zhao <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0021\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>). The impact of artificial facial pictures on user perceptions has not been studied thoroughly in previous HCI design literature. The lack of applied user studies is understandable given that until recently, the generated facial pictures were not close to realistic, so the research focus was on improving algorithms. However, as the quality of the facial pictures improves, the focus ought to shift towards evaluation studies in real-world use cases, systems, and applications. As there is a lack of literature in this regard, the research presented here contains a step forward in analysing the use of artificially facial generated pictures in real systems.</p><p>In terms of results, the crowd evaluation suggests that more than half of the artificial pictures are considered as either <i>perfect</i> or <i>high quality</i>. The ratio of \u2018perfect and high-quality\u2019 pictures to the rest is around 1.5, implying that most of the pictures are satisfactory according to the guidelines we provided. The persona perception analysis shows that the use of artificial pictures vs. real pictures in persona profiles does not reduce the authenticity of the persona or people's willingness to use the persona, two crucial concerns of persona applicability. Therefore, we find the state-of-the-art of AIG satisfactory for a persona and most likely for other systems requiring the substantial use of facial images. So, it is possible to replace the need for manually retrieving pictures from online photo banks with a process of automatically generated pictures.</p></div><div id=\"S006-S2002\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i36\">6.2. Gender differences in perception</h3><p>Regarding the female persona with an artificial picture being perceived as more authentic, we surmise that there might be a \u2018stock photo\u2019 effect involved, rather than a gender effect. This proposition is backed up by previous findings of stock photos being perceived differently by individuals than non-stock photos (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0088\" data-refLink=\"_i41 _i42\" href=\"#\">2019f</a></span>). Visually, to the respondents, the real photo chosen for the female persona appears different from the one chosen for the male persona (see <a href=\"#F0004\">Figure 4</a>). It is difficult to explain or quantify why this is. We interpret this finding such that the choice of pictures for a persona profile, and perhaps other system contexts, is a delicate matter; even small nuances can affect user perceptions.</p><p>This interpretation is generally in line with previous HCI research regarding the foundational impact of photos in persona profiles (Hill et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0042\" data-refLink=\"_i41 _i42\" href=\"#\">2017</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0090\" data-refLink=\"_i41 _i42\" href=\"#\">2018d</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0084\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>). Possibly, stock photos can appear, at times, less realistic than photos of \u2018real people\u2019 because they are \u2018too shiny, too perfect\u2019 (or \u2018too smiling\u2019 [Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0087\" data-refLink=\"_i41 _i42\" href=\"#\">2019e</a></span>]). Thus, <i>if the generator's outputs are closer to real people than stock photos in their appearance</i>, it is possible that these pictures are deemed more realistic than stock photos. However, this does not explain why the effect was found for the female persona and not for the male one. The only way to establish if there is a gender effect that influences perceptions of stock photos is to conduct repeated experiments with stock photos of different people. In addition to repeated experiments, for future research, the gender difference suggests another variable to consider: \u2018the degree of photo-editing\u2019 or \u2018shiny factor\u2019 (i.e. how polished the stock photo is and how this affects persona perceptions). The proper adjustment of this variable is best ensured via a manipulation check.</p></div><div id=\"S006-S2003\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i37\">6.3. Wider implications for HCI and system implementation</h3><p>In a broader context, the manuscript contributes to evaluating a machine learning tool for UX/UI design work. For the use of artificial images, guidelines are provided. <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">Solutions for Mitigating Subjectivity: The results indicate that evaluating the quality of artificial facial pictures contains a moderate to a high level of subjectivity, making reliable evaluation for production systems costlier. We hypothesise that there will always be some degree of subjectivity, as individuals vary in their ability to pay attention to details. This can be partially remedied by <i>choosing the pictures with the highest agreement between the raters</i>, or <i>using a binary rating scale</i> (i.e. \u2018good enough\u2019 vs. \u2018not good enough\u2019) as the agreement is generally easier to obtain with fewer classes (Alonso <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0003\" data-refLink=\"_i41 _i42\" href=\"#\">2015</a></span>). The observed \u2018disagreement\u2019 may be partly fallacious because people might agree whether a picture is either usable (4 or 5) or non-usable (1 or 2), but the exact agreement between 4 or 5, for example, is lower. As stated, for practical purposes, it does not matter if a picture is \u2018Perfect\u2019 or \u2018High quality\u2019, as both classes are decent, at least for this use case.</p></li><li><p class=\"inline\">Handling of Borderline Cases. Regarding pictures to use in a production system, we recommend a <i>borderline principle</i>: if in doubt of the picture quality, reject it. The marginal cost of generating new pictures is diminishingly low but showing a low-quality picture decreases user experience, sometimes drastically. For this reason, the economics of automatic image generation are in favour of rejecting borderline images more than letting through distorted images. However, rejecting borderline images does increase the total cost of evaluation because to obtain <i>n</i> useful pictures, one now has to obtain <i>n</i>\u2009\u00d7\u2009(1\u2009+\u2009false positive rate) ratings, which is (<i>n</i>\u2009\u00d7\u2009(1\u2009+\u2009false positive rate) \u2013 <i>n</i>) / <i>n</i> ratings more than <i>n</i> ratings. Additionally, as we have shown, the higher the disagreement among the crowd raters, the more ratings required.</p></li><li><p class=\"inline\">Final Choice for Human. In evaluating the suitability of artificial pictures for use in real applications, domain expertise is needed because, irrespective of quality guidelines, the crowd may have different quality standards than domain experts. For example, the crowd can be used to filter out low-quality photos, but the \u2018better\u2019 quality photos should be evaluated specifically by domain experts, as different domains likely have different quality standards. For personas, the pictures need to be of high quality, but when implementing them for the system, they are cropped into a smaller resolution that helps obfuscate minor errors.</p></li></ul></p></div><div id=\"S006-S2004\" class=\"NLM_sec NLM_sec_level_2\"><h3 class=\"section-heading-3\" id=\"_i38\">6.4. Future research avenues</h3><p>The following avenues for future research are proposed. <ul class=\"NLM_list NLM_list-list_type-bullet\"><li><p class=\"inline\">Suitability in Other Domains. For example, <i>how do quality standards and requirements by users and organizations differ across domains and use cases? How well are artificial (\u2018fake\u2019) pictures detected by end users, such as consumers and voters?</i> This research ties in with the nascent field of \u2018deep fakes\u2019 (Yang, Li, and Lyu <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0102\" data-refLink=\"_i41 _i42\" href=\"#\">2019</a></span>), i.e. images and videos purposefully manipulated for a political or commercial agenda. To this end, future studies could investigate the wider impact of using AI-generated images for profile pictures on sharing, economy platforms, or social media and news sites, and how that impact user perceptions, such as trust. Another interesting domain for suitability studies includes marketing, as facial pictures are widely deployed to advertise products such as fashion and luxury items.</p></li><li><p class=\"inline\">Algorithmic Bias. It would be important to investigate if the generated pictures involve an algorithmic bias \u2013 given that the training data may be biased, it would be worthwhile to analyze how diverse the generated pictures for different ethnicities, ages, and genders. Regarding persona perceptions, the race could be a confounding factor in our research and should be analysed separately in future research. A related question is: does the picture quality vary by demographic factors such as gender and race? Studies on algorithmic bias have been carried out within the HCI community (Eslami et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0031\" data-refLink=\"_i41 _i42\" href=\"#\">2018</a></span>; Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0084\" data-refLink=\"_i41 _i42\" href=\"#\">2019b</a></span>) and should be extended to this context.</p></li><li><p class=\"inline\">Demographically Conditional Images. For future development, we envision a system that automatically generates persona-specific pictures based on specific features/attributes of the personas \u2013 this would enable \u2018on-demand\u2019 picture creation for new personas generated by APG, whereas currently, the pictures need to manually tagged for age, gender, and country.</p></li></ul></p></div></div><div id=\"S007\" class=\"NLM_sec NLM_sec_level_1\"><h2 id=\"_i39\" class=\"section-heading-2\">7. Conclusion</h2><p>Our research goal was to evaluate the applicability of artificial pictures for personas along two dimensions: their quality and their impact on user perceptions. We found that more than half of the pictures were rated as perfect or high quality, with none as unusable. Moreover, the use of artificial pictures did not decrease the perceptions of personas that are found important in the HCI literature. These results can be considered as a vote of confidence for the current state of technology concerning the automatic generation of facial pictures and their use in data-driven persona profiles.</p></div></div><script type=\"text/javascript\">", "                        window.figureViewer={doi:'10.1080/0144929X.2020.1838610',path:'/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527',figures:[{i:'F0001',g:[{m:'tbit_a_1838610_f0001_ob.jpg',l:'tbit_a_1838610_f0001_ob.jpeg',size:'45 KB'}]}", "                            ,{i:'F0002',g:[{m:'tbit_a_1838610_f0002_oc.jpg',l:'tbit_a_1838610_f0002_oc.jpeg',size:'374 KB'}]}", "                            ,{i:'F0003',g:[{m:'tbit_a_1838610_f0003_oc.jpg',l:'tbit_a_1838610_f0003_oc.jpeg',size:'113 KB'}]}", "                            ,{i:'F0004',g:[{m:'tbit_a_1838610_f0004_oc.jpg',l:'tbit_a_1838610_f0004_oc.jpeg',size:'140 KB'}]}", "                            ,{i:'F0005',g:[{m:'tbit_a_1838610_f0005_oc.jpg',l:'tbit_a_1838610_f0005_oc.jpeg',size:'42 KB'}]}", "                            ,{i:'F0006',g:[{m:'tbit_a_1838610_f0006_oc.jpg',l:'tbit_a_1838610_f0006_oc.jpeg',size:'43 KB'}]}", "                            ]}</script><script type=\"text/javascript\">window.tableViewer={doi:'10.1080/0144929X.2020.1838610',path:'/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527',tables:[{i:'T0001'},{i:'T0002'},{i:'T0003'},{i:'T0004'}]}</script><script type=\"text/javascript\">window.tableIDIndexMap = {\"id\":-1};window.tableIDIndexMap['T0001'] = 1; window.tableIDIndexMap['T0002'] = 2; window.tableIDIndexMap['T0003'] = 3; window.tableIDIndexMap['T0004'] = 4; </script><div id=\"table-content-T0001\" class=\"hidden\"><table class=\"table frame_bottom\"><div class=\"caption\"><b>Table 1. The results of crowd evaluation based on a majority vote of the picture quality. Most frequent class bolded. Example facial image from each of the 5 classes shown for comparison.</b></div><colgroup><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left last\">\u00a0</th></tr></thead><tbody><tr valign=\"top\" class=\"last\"><td align=\"center\" class=\" align_center last\"><span class=\"NLM_inline-graphic\"><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/medium/tbit_a_1838610_ilg0001.gif\" alt=\"\" /></span></td></tr></tbody></table></div><div id=\"table-content-T0002\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><b>Table 2. Agreement metrics for the crowdsourced ratings showing satisfactory internal validity.</b></div><colgroup><col /><col /><col /><col /><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">Measure</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">Value</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">SE</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">95% CI</th><th align=\"center\" valign=\"bottom\" class=\" align_center last\"><i>P</i></th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">PA</td><td align=\"char\" char=\".\" class=\" align_char\">86.2%</td><td align=\"char\" char=\".\" class=\" align_char\">0.6%</td><td align=\"left\" class=\" align_left\">85.27%, 87.2%</td><td align=\"left\" class=\" align_left last\">\u00a0</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">AC1</td><td align=\"char\" char=\".\" class=\" align_char\">0.627</td><td align=\"char\" char=\".\" class=\" align_char\">0.017</td><td align=\"left\" class=\" align_left\">0.59, 0.66</td><td align=\"left\" class=\" align_left last\">&lt;0.001</td></tr></tbody></table></div><div id=\"table-content-T0003\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><b>Table 3. Survey statements. The participants answered using a 7-point Likert scale, ranging from Strongly Disagree to Strongly Agree. The statements were validated in (Salminen et al. <span class=\"ref-lnk lazy-ref\"><a data-rid=\"CIT0089\" data-refLink=\"_i41 _i42\" href=\"#\">2018c</a></span>). WTU \u2013 willingness to use.</b></div><colgroup><col /><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">Perception</th><th align=\"center\" valign=\"bottom\" class=\" align_center last\">Statements</th></tr></thead><tbody><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Authenticity</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">The persona seems like a real person.</p></li><li><p class=\"inline\">I have met people like this persona.</p></li><li><p class=\"inline\">The picture of the persona looks authentic.</p></li><li><p class=\"inline\">The persona seems to have a personality.</p></li></ul></td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Clarity</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">The information about the persona is well presented.</p></li><li><p class=\"inline\">The text in the persona profile is clear enough to read.</p></li><li><p class=\"inline\">The information in the persona profile is easy to understand.</p></li></ul></td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Empathy</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">I feel like I understand this persona.</p></li><li><p class=\"inline\">I feel strong ties to this persona.</p></li><li><p class=\"inline\">I can imagine a day in the life of this persona.</p></li></ul></td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">Willingness To Use</td><td align=\"left\" class=\" align_left last\"> <ul class=\"NLM_list NLM_list-list_type-simple\"><li><p class=\"inline\">I would like to know more about this persona.</p></li><li><p class=\"inline\">This persona would improve my ability to make decisions about the customers it describes.</p></li><li><p class=\"inline\">I would make use of this persona in my task [of creating a YouTube video].</p></li></ul></td></tr></tbody></table></div><div id=\"table-content-T0004\" class=\"hidden\"><table class=\"table frame_topbot\"><div class=\"caption\"><b>Table 4. Univariate tests for between-subjects effects (df(error)\u2009=\u20091(480)).</b></div><colgroup><col /><col /><col /><col /><col /><col /><col /><col /></colgroup><thead valign=\"bottom\"><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">\u00a0</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">\u00a0</th><th colspan=\"3\" align=\"center\" valign=\"bottom\" class=\" align_center\"><i>Male</i> persona</th><th colspan=\"3\" align=\"center\" valign=\"bottom\" class=\" align_center last\"><i>Female</i> persona</th></tr><tr valign=\"top\"><th align=\"left\" valign=\"bottom\" class=\" align_left\">Independent variable</th><th align=\"center\" valign=\"bottom\" class=\" align_center\">Dependent variable</th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><i>F</i></th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0002.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0002.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup></math></span></th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><i>p</i>-value</th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><i>F</i></th><th align=\"center\" valign=\"bottom\" class=\" align_center\"><span class=\"NLM_disp-formula-image inline-formula\"><noscript><img src=\"/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0003.gif\" alt=\"\" /></noscript><img src=\"//:0\" alt=\"\" class=\"mml-formula\" data-formula-source=\"{&quot;type&quot; : &quot;image&quot;, &quot;src&quot; : &quot;/na101/home/literatum/publisher/tandf/journals/content/tbit20/0/tbit20.ahead-of-print/0144929x.2020.1838610/20210527/images/tbit_a_1838610_ilm0003.gif&quot;}\" /><span class=\"mml-formula\"></span></span><span class=\"NLM_disp-formula inline-formula\"><img src=\"//:0\" alt=\"\" data-formula-source=\"{&quot;type&quot; : &quot;mathjax&quot;}\" /><math><msubsup><mi>\u03b7</mi><mi>p</mi><mn>2</mn></msubsup></math></span></th><th align=\"center\" valign=\"bottom\" class=\" align_center last\"><i>p</i>- value</th></tr></thead><tbody><tr valign=\"top\"><td rowspan=\"4\" align=\"left\" class=\" align_left\">Type of Picture (real or artificial)</td><td align=\"left\" class=\" align_left\">Authenticity</td><td align=\"char\" char=\".\" class=\" align_char\">2.023</td><td align=\"char\" char=\".\" class=\" align_char\">0.004</td><td align=\"char\" char=\".\" class=\" align_char\">0.156</td><td align=\"char\" char=\".\" class=\" align_char\">12.479</td><td align=\"char\" char=\".\" class=\" align_char\">0.025</td><td align=\"char\" char=\".\" class=\" align_char last\">&lt;0.001</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Clarity</td><td align=\"char\" char=\".\" class=\" align_char\">0.002</td><td align=\"char\" char=\".\" class=\" align_char\">&lt;0.001</td><td align=\"char\" char=\".\" class=\" align_char\">0.965</td><td align=\"char\" char=\".\" class=\" align_char\">0.001</td><td align=\"char\" char=\".\" class=\" align_char\">&lt;0.001</td><td align=\"char\" char=\".\" class=\" align_char last\">0.980</td></tr><tr valign=\"top\"><td align=\"left\" class=\" align_left\">Empathy</td><td align=\"char\" char=\".\" class=\" align_char\">2.057</td><td align=\"char\" char=\".\" class=\" align_char\">0.004</td><td align=\"char\" char=\".\" class=\" align_char\">0.152</td><td align=\"char\" char=\".\" class=\" align_char\">1.045</td><td align=\"char\" char=\".\" class=\" align_char\">0.002</td><td align=\"char\" char=\".\" class=\" align_char last\">0.307</td></tr><tr valign=\"top\" class=\"last\"><td align=\"left\" class=\" align_left\">WTU</td><td align=\"char\" char=\".\" class=\" align_char\">0.658</td><td align=\"char\" char=\".\" class=\" align_char\">0.001</td><td align=\"char\" char=\".\" class=\" align_char\">0.418</td><td align=\"char\" char=\".\" class=\" align_char\">0.006</td><td align=\"char\" char=\".\" class=\" align_char\">&lt;0.001</td><td align=\"char\" char=\".\" class=\" align_char last\">0.938</td></tr></tbody></table></div><div id=\"coi-statement\" class=\"NLM_sec\"><div id=\"S008\" class=\"NLM_sec NLM_sec-type_COI-statement NLM_sec_level_1\"><h2 id=\"_i41\" class=\"section-heading-2\">Disclosure statement</h2><p>No potential conflict of interest was reported by the author(s).</p></div></div><a id=\"inline_frontnotes\"></a><h2>Notes</h2><div class=\"summation-section\"><a id=\"EN0001\"></a><p>1 A demo of the system can be accessed at <a class=\"ext-link\" href=\"https://persona.qcri.org\" target=\"_blank\">https://persona.qcri.org</a></p><a id=\"EN0002\"></a><p>2 <a class=\"ext-link\" href=\"https://persona.qcri.org\" target=\"_blank\">https://persona.qcri.org</a></p><a id=\"EN0003\"></a><p>3 <a class=\"ext-link\" href=\"https://github.com/NVlabs/stylegan\" target=\"_blank\">https://github.com/NVlabs/stylegan</a></p><a id=\"EN0004\"></a><p>4 <a class=\"ext-link\" href=\"https://www.tensorflow.org/\" target=\"_blank\">https://www.tensorflow.org/</a></p><a id=\"EN0005\"></a><p>5 <a class=\"ext-link\" href=\"https://github.com/NVlabs/stylegan\" target=\"_blank\">https://github.com/NVlabs/stylegan</a></p><a id=\"EN0006\"></a><p>6 Confidence is defined as agreement adjusted by trust score of each rater.</p></div><div class=\"pb-dropzone no-border-top\" data-pb-dropzone=\"contentNavigationDropZoneFull\"><div class=\"widget gql-content-navigation none  widget-none\" id=\"c8b0dea6-9842-46af-b708-142fe9107344\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"gql-content-navigation\" data-ajax-widget-id=\"c8b0dea6-9842-46af-b708-142fe9107344\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div></div><div id=\"references-Section\"><h2 id=\"figures\">References</h2><ul class=\"references numeric-ordered-list\"><li id=\"CIT0001\"><span><span class=\"hlFld-ContribAuthor\">Ablanedo, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Fairchild</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Griffith</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Rodeheffer</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Is This Person Real? Avatar Stylization and Its Influence on Human Perception in a Counseling Training Environment</span>.\u201d In: Chen J., Fragomeni G. (eds). In <i>International Conference on Virtual, Augmented and Mixed Reality</i>, <span class=\"NLM_fpage\">279</span>\u2013<span class=\"NLM_lpage\">289</span>. Lecture Notes in Computer Science, vol 10909. Springer, Cham. doi:<span class=\"NLM_pub-id\">10.1007/978-3-319-91581-4_20</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0001&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-319-91581-4_20\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=279-289&author=J.+Ablanedo&author=E.+Fairchild&author=T.+Griffith&author=C.+Rodeheffer&title=Is+This+Person+Real%3F+Avatar+Stylization+and+Its+Influence+on+Human+Perception+in+a+Counseling+Training+Environment\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0002\"><span><span class=\"hlFld-ContribAuthor\">Alam, <span class=\"NLM_given-names\">F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Ofli</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Imran</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Crisismmd: Multimodal Twitter Datasets from Natural Disasters</span>.\u201d In <i>Twelfth International AAAI Conference on Web and Social Media</i>. AAAI. Palo Alto, California, USA.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&author=F.+Alam&author=F.+Ofli&author=M.+Imran&title=Crisismmd%3A+Multimodal+Twitter+Datasets+from+Natural+Disasters\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0003\"><span><span class=\"hlFld-ContribAuthor\">Alonso, <span class=\"NLM_given-names\">O.</span></span> <span class=\"NLM_year\">2015</span>. \u201c<span class=\"NLM_chapter-title\">Practical Lessons for Gathering Quality Labels at Scale</span>.\u201d In <i>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</i>, <span class=\"NLM_fpage\">1089</span>\u2013<span class=\"NLM_lpage\">1092</span>. doi:<span class=\"NLM_pub-id\">10.1145/2766462.2776778</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0003&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2766462.2776778\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&pages=1089-1092&author=O.+Alonso&title=Practical+Lessons+for+Gathering+Quality+Labels+at+Scale\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0004\"><span><span class=\"hlFld-ContribAuthor\">An, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018a</span>. \u201c<span class=\"NLM_article-title\">Customer Segmentation Using Online Platforms: Isolating Behavioral and Demographic Segments for Persona Creation via Aggregated User Data</span>.\u201d <i>Social Network Analysis and Mining</i> 8 (1). doi:<span class=\"NLM_pub-id\">10.1007/s13278-018-0531-0</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0004&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2Fs13278-018-0531-0\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0004&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000442734100002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=8&publication_year=2018a&issue=1&author=J.+An&author=H.+Kwak&author=S.+Jung&author=J.+Salminen&author=B.+J.+Jansen&title=Customer+Segmentation+Using+Online+Platforms%3A+Isolating+Behavioral+and+Demographic+Segments+for+Persona+Creation+via+Aggregated+User+Data&doi=10.1007%2Fs13278-018-0531-0\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0005\"><span><span class=\"hlFld-ContribAuthor\">An, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018b</span>. \u201c<span class=\"NLM_article-title\">Imaginary People Representing Real Numbers: Generating Personas From Online Social Media Data</span>.\u201d <i>ACM Transactions on the Web (TWEB)</i> 12 (4): Article No. 27. doi:<span class=\"NLM_pub-id\">10.1145/3265986</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0005&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3265986\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0005&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000457144000007\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=12&publication_year=2018b&issue=4&author=J.+An&author=H.+Kwak&author=J.+Salminen&author=S.+Jung&author=B.+J.+Jansen&title=Imaginary+People+Representing+Real+Numbers%3A+Generating+Personas+From+Online+Social+Media+Data&doi=10.1145%2F3265986\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0006\"><span><span class=\"hlFld-ContribAuthor\">Antipov, <span class=\"NLM_given-names\">G.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Baccouche</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-L.</span> Dugelay</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Face Aging with Conditional Generative Adversarial Networks</span>.\u201d In <i>2017 IEEE International Conference on Image Processing (ICIP)</i>, IEEE, Beijing, <span class=\"NLM_fpage\">2089</span>\u2013<span class=\"NLM_lpage\">2093</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0006&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICIP.2017.8296650\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=2089-2093&author=G.+Antipov&author=M.+Baccouche&author=J.-L.+Dugelay&title=Face+Aging+with+Conditional+Generative+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0007\"><span><span class=\"hlFld-ContribAuthor\">Araujo, <span class=\"NLM_given-names\">T.</span></span> <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_article-title\">Living up to the Chatbot Hype: The Influence of Anthropomorphic Design Cues and Communicative Agency Framing on Conversational Agent and Company Perceptions</span>.\u201d <i>Computers in Human Behavior</i> 85: <span class=\"NLM_fpage\">183</span>\u2013<span class=\"NLM_lpage\">189</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.chb.2018.03.051</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0007&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.chb.2018.03.051\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0007&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000435622000018\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=85&publication_year=2018&pages=183-189&author=T.+Araujo&title=Living+up+to+the+Chatbot+Hype%3A+The+Influence+of+Anthropomorphic+Design+Cues+and+Communicative+Agency+Framing+on+Conversational+Agent+and+Company+Perceptions&doi=10.1016%2Fj.chb.2018.03.051\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0008\"><span><span class=\"hlFld-ContribAuthor\">Ashraf, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N. I.</span> Jaafar</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Sulaiman</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">System- vs. Consumer-Generated Recommendations: Affective and Social-Psychological Effects on Purchase Intention</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1259</span>\u2013<span class=\"NLM_lpage\">1272</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1583285</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0008&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1583285&amp;tollfreelink=2_18_1f2362c1c7976a48b7fe16c48cce10bbce95e9161acf376d5828e9ae7c174016\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0008&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200006\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1259-1272&issue=12&author=M.+Ashraf&author=N.+I.+Jaafar&author=A.+Sulaiman&title=System-+vs.+Consumer-Generated+Recommendations%3A+Affective+and+Social-Psychological+Effects+on+Purchase+Intention&doi=10.1080%2F0144929X.2019.1583285\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0009\"><span><span class=\"hlFld-ContribAuthor\">Banerjee, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Capozzoli</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> McSweeney</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Sinha</span>. <span class=\"NLM_year\">1999</span>. \u201c<span class=\"NLM_article-title\">Beyond Kappa: A Review of Interrater Agreement Measures</span>.\u201d <i>Canadian Journal of Statistics</i> 27 (1): <span class=\"NLM_fpage\">3</span>\u2013<span class=\"NLM_lpage\">23</span>. doi:<span class=\"NLM_pub-id\">10.2307/3315487</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0009&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.2307%2F3315487\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0009&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000080854000002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=27&publication_year=1999&pages=3-23&issue=1&author=M.+Banerjee&author=M.+Capozzoli&author=L.+McSweeney&author=D.+Sinha&title=Beyond+Kappa%3A+A+Review+of+Interrater+Agreement+Measures&doi=10.2307%2F3315487\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0010\"><span><span class=\"hlFld-ContribAuthor\">Barratt, <span class=\"NLM_given-names\">S.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Sharma</span>. <span class=\"NLM_year\">2018</span>. A Note on the Inception Score. <i>ArXiv:1801.01973 [Cs, Stat]</i>. <a class=\"ext-link\" href=\"http://arxiv.org/abs/1801.01973\" target=\"_blank\">http://arxiv.org/abs/1801.01973</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Barratt%2C+S.%2C+and+R.+Sharma.+2018.+A+Note+on+the+Inception+Score.+ArXiv%3A1801.01973+%5BCs%2C+Stat%5D.+http%3A%2F%2Farxiv.org%2Fabs%2F1801.01973.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0011\"><span><span class=\"hlFld-ContribAuthor\">Barrett, <span class=\"NLM_given-names\">P.</span></span> <span class=\"NLM_year\">2007</span>. \u201c<span class=\"NLM_article-title\">Structural Equation Modelling: Adjudging Model fit</span>.\u201d <i>Personality and Individual Differences</i> 42 (5): <span class=\"NLM_fpage\">815</span>\u2013<span class=\"NLM_lpage\">824</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0011&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.paid.2006.09.018\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0011&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000245028700002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=42&publication_year=2007&pages=815-824&issue=5&author=P.+Barrett&title=Structural+Equation+Modelling%3A+Adjudging+Model+fit\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0012\"><span><span class=\"hlFld-ContribAuthor\">Baxter, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Courage</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Caine</span>. <span class=\"NLM_year\">2015</span>. <i>Understanding Your Users: A Practical Guide to User Requirements Methods, Tools, and Techniques</i>. <span class=\"NLM_edition\">2nd ed.</span> Burlington, Massachusetts. <span class=\"NLM_publisher-name\">Morgan Kaufmann</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2015&author=K.+Baxter&author=C.+Courage&author=K.+Caine&title=Understanding+Your+Users%3A+A+Practical+Guide+to+User+Requirements+Methods%2C+Tools%2C+and+Techniques\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0013\"><span><span class=\"hlFld-ContribAuthor\">Bazzano, <span class=\"NLM_given-names\">A. N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Martin</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Hicks</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Faughnan</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Murphy</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_article-title\">Human-centred Design in Global Health: A Scoping Review of Applications and Contexts</span>.\u201d <i>PloS One</i> 12 (11).<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0013&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1371%2Fjournal.pone.0186744\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0013&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000414229700029\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=12&publication_year=2017&issue=11&author=A.+N.+Bazzano&author=J.+Martin&author=E.+Hicks&author=M.+Faughnan&author=L.+Murphy&title=Human-centred+Design+in+Global+Health%3A+A+Scoping+Review+of+Applications+and+Contexts\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0014\"><span><span class=\"hlFld-ContribAuthor\">Brangier, <span class=\"NLM_given-names\">E.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Bornet</span>. <span class=\"NLM_year\">2011</span>. \u201c<span class=\"NLM_chapter-title\">Persona: A Method to Produce Representations Focused on Consumers\u2019 Needs</span>.\u201d Eds: Waldemar Karwowski, Marcelo M. Soares, Neville A. Stanton. In <i>Human Factors and Ergonomics in Consumer Product Design</i>, <span class=\"NLM_fpage\">37</span>\u2013<span class=\"NLM_lpage\">61</span>. <span class=\"NLM_publisher-name\">Taylor and Francis</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0014&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1201%2Fb10950-5\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=37-61&author=E.+Brangier&author=C.+Bornet&title=Persona%3A+A+Method+to+Produce+Representations+Focused+on+Consumers%E2%80%99+Needs\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0015\"><span><span class=\"hlFld-ContribAuthor\">Brauner, <span class=\"NLM_given-names\">P.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Philipsen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. C.</span> Valdez</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Ziefle</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">What Happens when Decision Support Systems Fail? \u2014 The Importance of Usability on Performance in Erroneous Systems</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1225</span>\u2013<span class=\"NLM_lpage\">1242</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1581258</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0015&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1581258&amp;tollfreelink=2_18_672e5a20433733ce1bcda6b8d9b63cce32368d15868ac8dbf217bb04c4dc0f81\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0015&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1225-1242&issue=12&author=P.+Brauner&author=R.+Philipsen&author=A.+C.+Valdez&author=M.+Ziefle&title=What+Happens+when+Decision+Support+Systems+Fail%3F+%E2%80%94+The+Importance+of+Usability+on+Performance+in+Erroneous+Systems&doi=10.1080%2F0144929X.2019.1581258\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0016\"><span><span class=\"hlFld-ContribAuthor\">Brickey, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Walczak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Burgess</span>. <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_article-title\">Comparing Semi-Automated Clustering Methods for Persona Development</span>.\u201d <i>IEEE Transactions on Software Engineering</i> 38 (3): <span class=\"NLM_fpage\">537</span>\u2013<span class=\"NLM_lpage\">546</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0016&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FTSE.2011.60\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0016&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000304414400003\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2012&pages=537-546&issue=3&author=J.+Brickey&author=S.+Walczak&author=T.+Burgess&title=Comparing+Semi-Automated+Clustering+Methods+for+Persona+Development\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0017\"><span><span class=\"hlFld-ContribAuthor\">Chapman, <span class=\"NLM_given-names\">C. N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Love</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. P.</span> Milham</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> ElRif</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. L.</span> Alford</span>. <span class=\"NLM_year\">2008</span>. \u201c<span class=\"NLM_article-title\">Quantitative Evaluation of Personas as Information</span>.\u201d <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 52 (16): <span class=\"NLM_fpage\">1107</span>\u2013<span class=\"NLM_lpage\">1111</span>. doi:<span class=\"NLM_pub-id\">10.1177/154193120805201602</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0017&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1177%2F154193120805201602\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=52&publication_year=2008&pages=1107-1111&issue=16&author=C.+N.+Chapman&author=E.+Love&author=R.+P.+Milham&author=P.+ElRif&author=J.+L.+Alford&title=Quantitative+Evaluation+of+Personas+as+Information&doi=10.1177%2F154193120805201602\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0018\"><span><span class=\"hlFld-ContribAuthor\">Chapman, <span class=\"NLM_given-names\">C. N.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. P.</span> Milham</span>. <span class=\"NLM_year\">2006</span>. \u201c<span class=\"NLM_article-title\">The Personas\u2019 New Clothes: Methodological and Practical Arguments against a Popular Method</span>.\u201d <i>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</i> 50 (5): <span class=\"NLM_fpage\">634</span>\u2013<span class=\"NLM_lpage\">636</span>. doi:<span class=\"NLM_pub-id\">10.1177/154193120605000503</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0018&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1177%2F154193120605000503\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=50&publication_year=2006&pages=634-636&issue=5&author=C.+N.+Chapman&author=R.+P.+Milham&title=The+Personas%E2%80%99+New+Clothes%3A+Methodological+and+Practical+Arguments+against+a+Popular+Method&doi=10.1177%2F154193120605000503\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0019\"><span><span class=\"hlFld-ContribAuthor\">Chen, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Chen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Zhang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Mitchell</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Yu</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">Photo-Realistic Facial Details Synthesis from Single Image</span>.\u201d In <i>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</i>, <span class=\"NLM_fpage\">9428</span>\u2013<span class=\"NLM_lpage\">9438</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICCV.2019.00952</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0019&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICCV.2019.00952\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=9428-9438&author=A.+Chen&author=Z.+Chen&author=G.+Zhang&author=K.+Mitchell&author=J.+Yu&title=Photo-Realistic+Facial+Details+Synthesis+from+Single+Image\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0020\"><span><span class=\"hlFld-ContribAuthor\">Choi, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Choi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Kim</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-W.</span> Ha</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Kim</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Choo</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Stargan: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</span>.\u201d In <i>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</i>, IEEE, Salt Lake City, UT. <span class=\"NLM_fpage\">8789</span>\u2013<span class=\"NLM_lpage\">8797</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0020&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2018.00916\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=8789-8797&author=Y.+Choi&author=M.+Choi&author=M.+Kim&author=J.-W.+Ha&author=S.+Kim&author=J.+Choo&title=Stargan%3A+Unified+Generative+Adversarial+Networks+for+Multi-Domain+Image-to-Image+Translation\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0021\"><span><span class=\"hlFld-ContribAuthor\">Church, <span class=\"NLM_given-names\">E. M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Iyer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Zhao</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Pictures Tell a Story: Antecedents of Rich-Media Curation in Social Network Sites</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (4): <span class=\"NLM_fpage\">361</span>\u2013<span class=\"NLM_lpage\">374</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2018.1535620</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0021&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2018.1535620&amp;tollfreelink=2_18_b111162a921aac579d26b7781fec171ea9e245375dab467468b2077a78dd5de4\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0021&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000460627500004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=361-374&issue=4&author=E.+M.+Church&author=L.+Iyer&author=X.+Zhao&title=Pictures+Tell+a+Story%3A+Antecedents+of+Rich-Media+Curation+in+Social+Network+Sites&doi=10.1080%2F0144929X.2018.1535620\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0022\"><span><span class=\"hlFld-ContribAuthor\">Cooper, <span class=\"NLM_given-names\">A.</span></span> <span class=\"NLM_year\">1999</span>. <i>The Inmates Are Running the Asylum: Why High Tech Products Drive Us Crazy and How to Restore the Sanity</i>. <span class=\"NLM_edition\">1st ed</span>. <span class=\"NLM_publisher-name\">Carmel, Indiana. Sams \u2013 Pearson Education</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0022&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-322-99786-9_1\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=1999&author=A.+Cooper&title=The+Inmates+Are+Running+the+Asylum%3A+Why+High+Tech+Products+Drive+Us+Crazy+and+How+to+Restore+the+Sanity\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0023\"><span><span class=\"hlFld-ContribAuthor\">Cooper, <span class=\"NLM_given-names\">A.</span></span> <span class=\"NLM_year\">2004</span>. <i>The Inmates Are Running the Asylum: Why High Tech Products Drive Us Crazy and How to Restore the Sanity</i>. <span class=\"NLM_edition\">2nd ed.</span> <span class=\"NLM_publisher-name\">Carmel, Indiana. Pearson Higher Education</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&author=A.+Cooper&title=The+Inmates+Are+Running+the+Asylum%3A+Why+High+Tech+Products+Drive+Us+Crazy+and+How+to+Restore+the+Sanity\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0024\"><span><span class=\"hlFld-ContribAuthor\">Dey, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Juefei-Xu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. N.</span> Boddeti</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Savvides</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">RankGAN: A Maximum Margin Ranking GAN for Generating Faces</span>.\u201d In <i>Computer Vision \u2013 ACCV 2018. (Vol. 11363)</i>. <span class=\"NLM_publisher-loc\">Cham</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-20893-6_1</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0024&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-20893-6_1\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&author=R.+Dey&author=F.+Juefei-Xu&author=V.+N.+Boddeti&author=M.+Savvides&title=RankGAN%3A+A+Maximum+Margin+Ranking+GAN+for+Generating+Faces\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0025\"><span><span class=\"hlFld-ContribAuthor\">Di, <span class=\"NLM_given-names\">X.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. M.</span> Patel</span>. <span class=\"NLM_year\">2017</span>. \u201cFace Synthesis from Visual Attributes via Sketch Using Conditional VAEs and GANs.\u201d <i>ArXiv:1801.00077 [Cs]</i>. <a class=\"ext-link\" href=\"http://arxiv.org/abs/1801.00077\" target=\"_blank\">http://arxiv.org/abs/1801.00077</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Di%2C+X.%2C+and+V.+M.+Patel.+2017.+%E2%80%9CFace+Synthesis+from+Visual+Attributes+via+Sketch+Using+Conditional+VAEs+and+GANs.%E2%80%9D+ArXiv%3A1801.00077+%5BCs%5D.+http%3A%2F%2Farxiv.org%2Fabs%2F1801.00077.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0026\"><span><span class=\"hlFld-ContribAuthor\">Di, <span class=\"NLM_given-names\">X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. A.</span> Sindagi</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V. M.</span> Patel</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks</span>.\u201d In <i>2018 24th International Conference on Pattern Recognition (ICPR)</i>, <span class=\"NLM_fpage\">1079</span>\u2013<span class=\"NLM_lpage\">1084</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICPR.2018.8545081</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0026&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICPR.2018.8545081\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=1079-1084&author=X.+Di&author=V.+A.+Sindagi&author=V.+M.+Patel&title=GP-GAN%3A+Gender+Preserving+GAN+for+Synthesizing+Faces+from+Landmarks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0027\"><span><span class=\"hlFld-ContribAuthor\">Dong, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Kelkar</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Braun</span>. <span class=\"NLM_year\">2007</span>. \u201c<span class=\"NLM_chapter-title\">Getting the Most Out of Personas for Product Usability Enhancements</span>.\u201d In <i>Usability and Internationalization. HCI and Culture</i>, <span class=\"NLM_fpage\">291</span>\u2013<span class=\"NLM_lpage\">296</span>. <a class=\"ext-link\" href=\"http://www.springerlink.com/index/C0U2718G14HG1263.pdf\" target=\"_blank\">http://www.springerlink.com/index/C0U2718G14HG1263.pdf</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0027&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-540-73287-7_36\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2007&pages=291-296&author=J.+Dong&author=K.+Kelkar&author=K.+Braun&title=Getting+the+Most+Out+of+Personas+for+Product+Usability+Enhancements\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0028\"><span><span class=\"hlFld-ContribAuthor\">dos Santos, <span class=\"NLM_given-names\">T. F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. G.</span> de Castro</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. A.</span> Masiero</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P. T. A.</span> Junior</span>. <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_chapter-title\">Behavioral Persona for Human-Robot Interaction: A Study Based on Pet Robot</span> Kurosu M. (eds) In. <i>International Conference on Human-Computer Interaction</i>, <span class=\"NLM_fpage\">687</span>\u2013<span class=\"NLM_lpage\">696</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0028&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-319-07230-2_65\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=687-696&author=T.+F.+dos+Santos&author=D.+G.+de+Castro&author=A.+A.+Masiero&author=P.+T.+A.+Junior&title=Behavioral+Persona+for+Human-Robot+Interaction%3A+A+Study+Based+on+Pet+Robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0029\"><span><span class=\"hlFld-ContribAuthor\">Duffy, <span class=\"NLM_given-names\">B. R.</span></span> <span class=\"NLM_year\">2003</span>. \u201c<span class=\"NLM_article-title\">Anthropomorphism and the Social Robot</span>.\u201d <i>Robotics and Autonomous Systems</i> 42 (3\u20134): <span class=\"NLM_fpage\">177</span>\u2013<span class=\"NLM_lpage\">190</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0029&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2FS0921-8890%2802%2900374-3\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0029&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000181591200004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=42&publication_year=2003&pages=177-190&issue=3%E2%80%934&author=B.+R.+Duffy&title=Anthropomorphism+and+the+Social+Robot\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0030\"><span><span class=\"hlFld-ContribAuthor\">Edwards, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Edwards</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P. R.</span> Spence</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Harris</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Gambino</span>. <span class=\"NLM_year\">2016</span>. \u201c<span class=\"NLM_article-title\">Robots in the Classroom: Differences in Students\u2019 Perceptions of Credibility and Learning Between \u2018Teacher as Robot\u2019 and \u2018Robot as Teacher\u2019</span>.\u201d <i>Computers in Human Behavior</i> 65: <span class=\"NLM_fpage\">627</span>\u2013<span class=\"NLM_lpage\">634</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.chb.2016.06.005</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0030&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.chb.2016.06.005\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0030&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000386986000067\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=65&publication_year=2016&pages=627-634&author=A.+Edwards&author=C.+Edwards&author=P.+R.+Spence&author=C.+Harris&author=A.+Gambino&title=Robots+in+the+Classroom%3A+Differences+in+Students%E2%80%99+Perceptions+of+Credibility+and+Learning+Between+%E2%80%98Teacher+as+Robot%E2%80%99+and+%E2%80%98Robot+as+Teacher%E2%80%99&doi=10.1016%2Fj.chb.2016.06.005\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0031\"><span><span class=\"hlFld-ContribAuthor\">Eslami, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. R.</span> Krishna Kumaran</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Sandvig</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Karahalios</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Communicating Algorithmic Process in Online Behavioral Advertising</span>.\u201d In <i>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">New York, NY, USA, Paper 432</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0031&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3173574.3174006\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=New+York%2C+NY%2C+USA%2C+Paper+432&author=M.+Eslami&author=S.+R.+Krishna+Kumaran&author=C.+Sandvig&author=K.+Karahalios&title=Communicating+Algorithmic+Process+in+Online+Behavioral+Advertising\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0032\"><span><span class=\"hlFld-ContribAuthor\">Faily, <span class=\"NLM_given-names\">S.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Flechais</span>. <span class=\"NLM_year\">2011</span>. \u201c<span class=\"NLM_chapter-title\">Persona Cases: A Technique for Grounding Personas</span>.\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">2267</span>\u2013<span class=\"NLM_lpage\">2270</span>. doi:<span class=\"NLM_pub-id\">10.1145/1978942.1979274</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0032&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F1978942.1979274\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2011&pages=2267-2270&author=S.+Faily&author=I.+Flechais&title=Persona+Cases%3A+A+Technique+for+Grounding+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0033\"><span><span class=\"hlFld-ContribAuthor\">Friess, <span class=\"NLM_given-names\">E.</span></span> <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_chapter-title\">Personas and Decision Making in the Design Process: An Ethnographic Case Study</span>.\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">1209</span>\u2013<span class=\"NLM_lpage\">1218</span>. doi:<span class=\"NLM_pub-id\">10.1145/2207676.2208572</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0033&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2207676.2208572\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&pages=1209-1218&author=E.+Friess&title=Personas+and+Decision+Making+in+the+Design+Process%3A+An+Ethnographic+Case+Study\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0034\"><span><span class=\"hlFld-ContribAuthor\">Gao, <span class=\"NLM_given-names\">F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Zhu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Jiang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Niu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Han</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Yu</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Incremental Focal Loss GANs</span>.\u201d <i>Information Processing &amp; Management</i> 57 (3): <span class=\"NLM_fpage\">102192</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ipm.2019.102192</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0034&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ipm.2019.102192\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0034&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000528550100024\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=57&publication_year=2020&pages=102192&issue=3&author=F.+Gao&author=J.+Zhu&author=H.+Jiang&author=Z.+Niu&author=W.+Han&author=J.+Yu&title=Incremental+Focal+Loss+GANs&doi=10.1016%2Fj.ipm.2019.102192\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0035\"><span><span class=\"hlFld-ContribAuthor\">Gecer, <span class=\"NLM_given-names\">B.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Bhattarai</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Kittler</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.-K.</span> Kim</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Semi-supervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model</span>.\u201d In <i>Computer Vision \u2013 ECCV 2018. ECCV 2018 (Vol. 11215)</i>, <span class=\"NLM_fpage\">230</span>\u2013<span class=\"NLM_lpage\">248</span>. <span class=\"NLM_publisher-loc\">Cham</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-01252-6_14</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0035&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-01252-6_14\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=230-248&author=B.+Gecer&author=B.+Bhattarai&author=J.+Kittler&author=T.-K.+Kim&title=Semi-supervised+Adversarial+Learning+to+Generate+Photorealistic+Face+Images+of+New+Identities+from+3D+Morphable+Model\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0036\"><span><span class=\"hlFld-ContribAuthor\">Go, <span class=\"NLM_given-names\">E.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Shyam Sundar</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Humanizing Chatbots: The Effects of Visual, Identity and Conversational Cues on Humanness Perceptions</span>.\u201d <i>Computers in Human Behavior</i>. doi:<span class=\"NLM_pub-id\">10.1016/j.chb.2019.01.020</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0036&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.chb.2019.01.020\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0036&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000469154400030\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&author=E.+Go&author=S.+Shyam+Sundar&title=Humanizing+Chatbots%3A+The+Effects+of+Visual%2C+Identity+and+Conversational+Cues+on+Humanness+Perceptions&doi=10.1016%2Fj.chb.2019.01.020\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0037\"><span><span class=\"hlFld-ContribAuthor\">Goodfellow, <span class=\"NLM_given-names\">I.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Pouget-Abadie</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Mirza</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Xu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Warde-Farley</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Ozair</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Courville</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Bengio</span>. <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_chapter-title\">Generative Adversarial Nets</span>.\u201d In <i>Advances in Neural Information Processing Systems 27</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Ghahramani</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Welling</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Cortes</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N. D.</span> Lawrence</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. Q.</span> Weinberger</span>, <span class=\"NLM_fpage\">2672</span>\u2013<span class=\"NLM_lpage\">2680</span>. <span class=\"NLM_publisher-name\">Curran Associates, Inc</span>. <a class=\"ext-link\" href=\"http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf\" target=\"_blank\">http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=2672-2680&author=I.+Goodfellow&author=J.+Pouget-Abadie&author=M.+Mirza&author=B.+Xu&author=D.+Warde-Farley&author=S.+Ozair&author=A.+Courville&author=Y.+Bengio&title=Generative+Adversarial+Nets\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0038\"><span><span class=\"hlFld-ContribAuthor\">Goodwin, <span class=\"NLM_given-names\">K.</span></span> <span class=\"NLM_year\">2009</span>. <i>Designing for the Digital Age: How to Create Human-Centered Products and Services</i>. <span class=\"NLM_edition\">New York, New York. 1st ed.</span> <span class=\"NLM_publisher-name\">Wiley</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&author=K.+Goodwin&title=Designing+for+the+Digital+Age%3A+How+to+Create+Human-Centered+Products+and+Services\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0039\"><span><span class=\"hlFld-ContribAuthor\">Gwet, <span class=\"NLM_given-names\">K. L.</span></span> <span class=\"NLM_year\">2008</span>. \u201c<span class=\"NLM_article-title\">Computing Inter-Rater Reliability and Its Variance in the Presence of High Agreement</span>.\u201d <i>British Journal of Mathematical and Statistical Psychology</i> 61 (1): <span class=\"NLM_fpage\">29</span>\u2013<span class=\"NLM_lpage\">48</span>. doi:<span class=\"NLM_pub-id\">10.1348/000711006X126600</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0039&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1348%2F000711006X126600\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0039&amp;dbid=8&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=18482474\" target=\"_blank\">[PubMed]</a>, <a href=\"/servlet/linkout?suffix=CIT0039&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000256524900002\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=61&publication_year=2008&pages=29-48&issue=1&author=K.+L.+Gwet&title=Computing+Inter-Rater+Reliability+and+Its+Variance+in+the+Presence+of+High+Agreement&doi=10.1348%2F000711006X126600\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0040\"><span><span class=\"hlFld-ContribAuthor\">Hair, <span class=\"NLM_given-names\">J. F.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W. C.</span> Black</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Babin</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R. E.</span> Anderson</span>. <span class=\"NLM_year\">2009</span>. <i>Multivariate Data Analysis</i>. <span class=\"NLM_edition\">7th ed.</span> New York, New York. <span class=\"NLM_publisher-name\">Pearson</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&author=J.+F.+Hair&author=W.+C.+Black&author=B.+J.+Babin&author=R.+E.+Anderson&title=Multivariate+Data+Analysis\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0041\"><span><span class=\"hlFld-ContribAuthor\">Heusel, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Ramsauer</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Unterthiner</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Nessler</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Hochreiter</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_article-title\">Gans Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</span>.\u201d <i>Advances in Neural Information Processing Systems</i>, <span class=\"NLM_fpage\">6626</span>\u2013<span class=\"NLM_lpage\">6637</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=6626-6637&author=M.+Heusel&author=H.+Ramsauer&author=T.+Unterthiner&author=B.+Nessler&author=S.+Hochreiter&title=Gans+Trained+by+a+Two+Time-Scale+Update+Rule+Converge+to+a+Local+Nash+Equilibrium\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0042\"><span><span class=\"hlFld-ContribAuthor\">Hill, <span class=\"NLM_given-names\">C. G.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Haag</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Oleson</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Mendez</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Marsden</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Sarma</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Burnett</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Gender-Inclusiveness Personas vs. Stereotyping: Can We Have It Both Ways?</span>\u201d In <i>Proceedings of the 2017 CHI Conference</i>, <span class=\"NLM_fpage\">6658</span>\u2013<span class=\"NLM_lpage\">6671</span>. doi:<span class=\"NLM_pub-id\">10.1145/3025453.3025609</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0042&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3025453.3025609\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=6658-6671&author=C.+G.+Hill&author=M.+Haag&author=A.+Oleson&author=C.+Mendez&author=N.+Marsden&author=A.+Sarma&author=M.+Burnett&title=Gender-Inclusiveness+Personas+vs.+Stereotyping%3A+Can+We+Have+It+Both+Ways%3F\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0043\"><span><span class=\"hlFld-ContribAuthor\">Holz, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Dragone</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G. M.</span> O\u2019Hare</span>. <span class=\"NLM_year\">2009</span>. \u201c<span class=\"NLM_article-title\">Where Robots and Virtual Agents Meet</span>.\u201d <i>International Journal of Social Robotics</i> 1 (1): <span class=\"NLM_fpage\">83</span>\u2013<span class=\"NLM_lpage\">93</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0043&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2Fs12369-008-0002-2\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0043&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000208892900008\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=2009&pages=83-93&issue=1&author=T.+Holz&author=M.+Dragone&author=G.+M.+O%E2%80%99Hare&title=Where+Robots+and+Virtual+Agents+Meet\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0044\"><span><span class=\"hlFld-ContribAuthor\">Hong, <span class=\"NLM_given-names\">B. B.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Bohemia</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Neubauer</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Santamaria</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Designing for Users: The Global Studio</span>.\u201d In <i>DS 93: Proceedings of the 20th International Conference on Engineering and Product Design Education (E&amp;PDE 2018), Dyson School of Engineering, Imperial College, London. 6th-7th September 2018</i>, <span class=\"NLM_fpage\">738</span>\u2013<span class=\"NLM_lpage\">743</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=738-743&author=B.+B.+Hong&author=E.+Bohemia&author=R.+Neubauer&author=L.+Santamaria&title=Designing+for+Users%3A+The+Global+Studio\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0045\"><span><span class=\"hlFld-ContribAuthor\">Huang, <span class=\"NLM_given-names\">W.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Weber</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Vieweg</span>. <span class=\"NLM_year\">2014</span>. \u201cInferring Nationalities of Twitter Users and Studying Inter-National Linking.\u201d <i>ACM HyperText Conference</i>. <a class=\"ext-link\" href=\"https://works.bepress.com/vieweg/18/\" target=\"_blank\">https://works.bepress.com/vieweg/18/</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Huang%2C+W.%2C+I.+Weber%2C+and+S.+Vieweg.+2014.+%E2%80%9CInferring+Nationalities+of+Twitter+Users+and+Studying+Inter-National+Linking.%E2%80%9D+ACM+HyperText+Conference.+https%3A%2F%2Fworks.bepress.com%2Fvieweg%2F18%2F.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0046\"><span><span class=\"hlFld-ContribAuthor\">Idoughi, <span class=\"NLM_given-names\">D.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Seffah</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Kolski</span>. <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_article-title\">Adding User Experience into the Interactive Service Design Loop: A Persona-Based Approach</span>.\u201d <i>Behaviour &amp; Information Technology</i> 31 (3): <span class=\"NLM_fpage\">287</span>\u2013<span class=\"NLM_lpage\">303</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2011.563799</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0046&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2011.563799&amp;tollfreelink=2_18_e0030495c235aaeaf3bc43e532c419a5c00a4f3b3ea53f90f2c9d504a60f19db\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0046&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000300849400008\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=31&publication_year=2012&pages=287-303&issue=3&author=D.+Idoughi&author=A.+Seffah&author=C.+Kolski&title=Adding+User+Experience+into+the+Interactive+Service+Design+Loop%3A+A+Persona-Based+Approach&doi=10.1080%2F0144929X.2011.563799\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0047\"><span><span class=\"hlFld-ContribAuthor\">Iizuka, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Simo-Serra</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Ishikawa</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_article-title\">Globally and Locally Consistent Image Completion</span>.\u201d <i>ACM Transactions on Graphics</i> 36 (4): <span class=\"NLM_fpage\">107:1</span>\u2013<span class=\"NLM_lpage\">107:14</span>. doi:<span class=\"NLM_pub-id\">10.1145/3072959.3073659</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0047&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3072959.3073659\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0047&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000406432100075\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=36&publication_year=2017&pages=107%3A1-107%3A14&issue=4&author=S.+Iizuka&author=E.+Simo-Serra&author=H.+Ishikawa&title=Globally+and+Locally+Consistent+Image+Completion&doi=10.1145%2F3072959.3073659\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0048\"><span><span class=\"hlFld-ContribAuthor\">Isola, <span class=\"NLM_given-names\">P.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-Y.</span> Zhu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Zhou</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. A.</span> Efros</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Image-to-Image Translation with Conditional Adversarial Networks</span>.\u201d In <i>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>, <span class=\"NLM_fpage\">IEEE, Honolulu, HI. 5967</span>\u2013<span class=\"NLM_lpage\">5976</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0048&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2017.632\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=IEEE%2C+Honolulu%2C+HI.+5967-5976&author=P.+Isola&author=J.-Y.+Zhu&author=T.+Zhou&author=A.+A.+Efros&title=Image-to-Image+Translation+with+Conditional+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0049\"><span><span class=\"hlFld-ContribAuthor\">Jansen, <span class=\"NLM_given-names\">B. J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. O.</span> Salminen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Data-Driven Personas for Enhanced User Understanding: Combining Empathy with Rationality for Better Insights to Analytics</span>.\u201d <i>Data and Information Management</i> 4 (1): <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">17</span>. doi:<span class=\"NLM_pub-id\">10.2478/dim-2020-0005</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0049&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.2478%2Fdim-2020-0005\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=4&publication_year=2020&pages=1-17&issue=1&author=B.+J.+Jansen&author=J.+O.+Salminen&author=S.-G.+Jung&title=Data-Driven+Personas+for+Enhanced+User+Understanding%3A+Combining+Empathy+with+Rationality+for+Better+Insights+to+Analytics&doi=10.2478%2Fdim-2020-0005\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0050\"><span><span class=\"hlFld-ContribAuthor\">Jansen, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Van Mechelen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Slegers</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Personas and Behavioral Theories: A Case Study Using Self-Determination Theory to Construct Overweight Personas</span>.\u201d In <i>Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">2127</span>\u2013<span class=\"NLM_lpage\">2136</span>. doi:<span class=\"NLM_pub-id\">10.1145/3025453.3026003</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0050&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3025453.3026003\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=2127-2136&author=A.+Jansen&author=M.+Van+Mechelen&author=K.+Slegers&title=Personas+and+Behavioral+Theories%3A+A+Case+Study+Using+Self-Determination+Theory+to+Construct+Overweight+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0051\"><span><span class=\"hlFld-ContribAuthor\">Jeffreys, <span class=\"NLM_given-names\">H.</span></span> <span class=\"NLM_year\">1998</span>. <i>Theory of Probability</i>. <span class=\"NLM_edition\">3rd ed.</span> <span class=\"NLM_publisher-name\">Oxford: Oxford University Press</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=1998&author=H.+Jeffreys&title=Theory+of+Probability\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0052\"><span><span class=\"hlFld-ContribAuthor\">Jenkinson, <span class=\"NLM_given-names\">A.</span></span> <span class=\"NLM_year\">1994</span>. \u201c<span class=\"NLM_article-title\">Beyond Segmentation</span>.\u201d <i>Journal of Targeting, Measurement and Analysis for Marketing</i> 3 (1): <span class=\"NLM_fpage\">60</span>\u2013<span class=\"NLM_lpage\">72</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=3&publication_year=1994&pages=60-72&issue=1&author=A.+Jenkinson&title=Beyond+Segmentation\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0053\"><span><span class=\"hlFld-ContribAuthor\">Jung, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018a</span>. \u201c<span class=\"NLM_chapter-title\">Automatically Conceptualizing Social Media Analytics Data via Personas</span>.\u201d In <i>Proceedings of the International AAAI Conference on Web and Social Media (ICWSM 2018), June 25</i>. International AAAI Conference on Web and Social Media (ICWSM 2018), San Francisco, California, USA.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018a&author=S.+Jung&author=J.+Salminen&author=J.+An&author=H.+Kwak&author=B.+J.+Jansen&title=Automatically+Conceptualizing+Social+Media+Analytics+Data+via+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0054\"><span><span class=\"hlFld-ContribAuthor\">Jung, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018b</span>. \u201c<span class=\"NLM_chapter-title\">Automatic Persona Generation (APG): A Rationale and Demonstration</span>.\u201d In <i>Proceedings of the ACM, 2018 Conference on Human Information Interaction &amp; Retrieval</i>, ACM, New Brunswick, NJ., <span class=\"NLM_fpage\">321</span>\u2013<span class=\"NLM_lpage\">324</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0054&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3176349.3176893\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018b&pages=321-324&author=S.+Jung&author=J.+Salminen&author=H.+Kwak&author=J.+An&author=B.+J.+Jansen&title=Automatic+Persona+Generation+%28APG%29%3A+A+Rationale+and+Demonstration\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0055\"><span><span class=\"hlFld-ContribAuthor\">Karras, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Laine</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Aila</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">A Style-Based Generator Architecture for Generative Adversarial Networks</span>.\u201d In <i>2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, <span class=\"NLM_fpage\">4396</span>\u2013<span class=\"NLM_lpage\">4405</span>. doi:<span class=\"NLM_pub-id\">10.1109/CVPR.2019.00453</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0055&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2019.00453\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=4396-4405&author=T.+Karras&author=S.+Laine&author=T.+Aila&title=A+Style-Based+Generator+Architecture+for+Generative+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0056\"><span><span class=\"hlFld-ContribAuthor\">King, <span class=\"NLM_given-names\">A. J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. J.</span> Lazard</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. R.</span> White</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">The Influence of Visual Complexity on Initial User Impressions: Testing the Persuasive Model of Web Design</span>.\u201d <i>Behaviour &amp; Information Technology</i> 39 (5): <span class=\"NLM_fpage\">497</span>\u2013<span class=\"NLM_lpage\">510</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1602167</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0056&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1602167&amp;tollfreelink=2_18_ba37b055a7024e5d28beab14ac2ff075170172009f68d0a1577d775393707d08\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0056&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000465783000001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=39&publication_year=2020&pages=497-510&issue=5&author=A.+J.+King&author=A.+J.+Lazard&author=S.+R.+White&title=The+Influence+of+Visual+Complexity+on+Initial+User+Impressions%3A+Testing+the+Persuasive+Model+of+Web+Design&doi=10.1080%2F0144929X.2019.1602167\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0057\"><span><span class=\"hlFld-ContribAuthor\">Lee, <span class=\"NLM_given-names\">M. D.</span></span> <span class=\"NLM_year\">2014</span>. <i>Bayesian Cognitive Modeling: A Practical Course</i>.<span class=\"NLM_publisher-name\">Cambridge: Cambridge University Press</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&author=M.+D.+Lee&title=Bayesian+Cognitive+Modeling%3A+A+Practical+Course\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0058\"><span><span class=\"hlFld-ContribAuthor\">Lee, <span class=\"NLM_given-names\">H.-Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.-Y.</span> Tseng</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.-B.</span> Huang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. K.</span> Singh</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.-H.</span> Yang</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Diverse Image-to-Image Translation via Disentangled Representations</span>.\u201d In <i>Computer Vision \u2013 ECCV 2018. ECCV 2018 (Vol. 11205)</i>. <span class=\"NLM_publisher-loc\">Cham</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-01246-5_3</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0058&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-01246-5_3\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&author=H.-Y.+Lee&author=H.-Y.+Tseng&author=J.-B.+Huang&author=M.+K.+Singh&author=M.-H.+Yang&title=Diverse+Image-to-Image+Translation+via+Disentangled+Representations\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0059\"><span><span class=\"hlFld-ContribAuthor\">Li, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Qian</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Dong</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Liu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Q.</span> Yan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Zhu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Lin</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">BeautyGAN: Instance-Level Facial Makeup Transfer with Deep Generative Adversarial Network</span>.\u201d In <i>Proceedings of the 26th ACM International Conference on Multimedia</i>, <span class=\"NLM_fpage\">645</span>\u2013<span class=\"NLM_lpage\">653</span>. doi:<span class=\"NLM_pub-id\">10.1145/3240508.3240618</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0059&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3240508.3240618\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=645-653&author=T.+Li&author=R.+Qian&author=C.+Dong&author=S.+Liu&author=Q.+Yan&author=W.+Zhu&author=L.+Lin&title=BeautyGAN%3A+Instance-Level+Facial+Makeup+Transfer+with+Deep+Generative+Adversarial+Network\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0060\"><span><span class=\"hlFld-ContribAuthor\">Lin, <span class=\"NLM_given-names\">C. H.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.-C.</span> Chang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.-S.</span> Chen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.-C.</span> Juan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Wei</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.-T.</span> Chen</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">COCO-GAN: Generation by Parts via Conditional Coordinating</span>.\u201d In <i>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</i>, <span class=\"NLM_fpage\">4511</span>\u2013<span class=\"NLM_lpage\">4520</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICCV.2019.00461</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0060&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICCV.2019.00461\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=4511-4520&author=C.+H.+Lin&author=C.-C.+Chang&author=Y.-S.+Chen&author=D.-C.+Juan&author=W.+Wei&author=H.-T.+Chen&title=COCO-GAN%3A+Generation+by+Parts+via+Conditional+Coordinating\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0061\"><span><span class=\"hlFld-ContribAuthor\">Liu, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Qin</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Wan</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Luo</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_article-title\">Auto-painter: Cartoon Image Generation from Sketch by Using Conditional Wasserstein Generative Adversarial Networks</span>.\u201d <i>Neurocomputing</i> 311: <span class=\"NLM_fpage\">78</span>\u2013<span class=\"NLM_lpage\">87</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.neucom.2018.05.045</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0061&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.neucom.2018.05.045\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0061&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000438313100008\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=311&publication_year=2018&pages=78-87&author=Y.+Liu&author=Z.+Qin&author=T.+Wan&author=Z.+Luo&title=Auto-painter%3A+Cartoon+Image+Generation+from+Sketch+by+Using+Conditional+Wasserstein+Generative+Adversarial+Networks&doi=10.1016%2Fj.neucom.2018.05.045\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0062\"><span><span class=\"hlFld-ContribAuthor\">Liu, <span class=\"NLM_given-names\">S.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Sun</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Zhu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Bao</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Wang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Shu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Yan</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Face Aging with Contextual Generative Adversarial Nets</span>.\u201d In <i>Proceedings of the 25th ACM International Conference on Multimedia</i>, <span class=\"NLM_fpage\">82</span>\u2013<span class=\"NLM_lpage\">90</span>. doi:<span class=\"NLM_pub-id\">10.1145/3123266.3123431</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0062&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3123266.3123431\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=82-90&author=S.+Liu&author=Y.+Sun&author=D.+Zhu&author=R.+Bao&author=W.+Wang&author=X.+Shu&author=S.+Yan&title=Face+Aging+with+Contextual+Generative+Adversarial+Nets\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0063\"><span><span class=\"hlFld-ContribAuthor\">Long, <span class=\"NLM_given-names\">F.</span></span> <span class=\"NLM_year\">2009</span>. \u201c<span class=\"NLM_chapter-title\">Real or Imaginary: The Effectiveness of Using Personas in Product Design</span>.\u201d In <i>Proceedings of the Irish Ergonomics Society Annual Conference</i>, <span class=\"NLM_fpage\">Dublin, IR., 14</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2009&pages=Dublin%2C+IR.%2C+14&author=F.+Long&title=Real+or+Imaginary%3A+The+Effectiveness+of+Using+Personas+in+Product+Design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0064\"><span><span class=\"hlFld-ContribAuthor\">Lu, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.-W.</span> Tai</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.-K.</span> Tang</span>. <span class=\"NLM_year\">2017</span>. \u201cConditional Cyclegan for Attribute Guided Face Image Generation.\u201d <i>ArXiv Preprint ArXiv:1705.09966</i>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Lu%2C+Y.%2C+Y.-W.+Tai%2C+and+C.-K.+Tang.+2017.+%E2%80%9CConditional+Cyclegan+for+Attribute+Guided+Face+Image+Generation.%E2%80%9D+ArXiv+Preprint+ArXiv%3A1705.09966.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0065\"><span><span class=\"hlFld-ContribAuthor\">Matthews, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Judge</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Whittaker</span>. <span class=\"NLM_year\">2012</span>. \u201c<span class=\"NLM_chapter-title\">How Do Designers and User Experience Professionals Actually Perceive and Use Personas?</span>\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">1219</span>\u2013<span class=\"NLM_lpage\">1228</span>. doi:<span class=\"NLM_pub-id\">10.1145/2207676.2208573</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0065&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2207676.2208573\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2012&pages=1219-1228&author=T.+Matthews&author=T.+Judge&author=S.+Whittaker&title=How+Do+Designers+and+User+Experience+Professionals+Actually+Perceive+and+Use+Personas%3F\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0066\"><span><span class=\"hlFld-ContribAuthor\">McGinn, <span class=\"NLM_given-names\">J. J.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Kotamraju</span>. <span class=\"NLM_year\">2008</span>. \u201c<span class=\"NLM_chapter-title\">Data-Driven Persona Development</span>.\u201d In <i>ACM Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">ACM, New York, NY, USA, 1521</span>\u2013<span class=\"NLM_lpage\">1524</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0066&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F1357054.1357292\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2008&pages=ACM%2C+New+York%2C+NY%2C+USA%2C+1521-1524&author=J.+J.+McGinn&author=N.+Kotamraju&title=Data-Driven+Persona+Development\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0067\"><span><span class=\"hlFld-ContribAuthor\">Neumann, <span class=\"NLM_given-names\">A.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Pyromallis</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Alexander</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Evolution of Images with Diversity and Constraints Using a Generative Adversarial Network</span>.\u201dCheng L., Leung A., Ozawa S. (eds). In <i>International Conference on Neural Information Processing</i>, <span class=\"NLM_fpage\">452</span>\u2013<span class=\"NLM_lpage\">465</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0067&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-04224-0_39\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=452-465&author=A.+Neumann&author=C.+Pyromallis&author=B.+Alexander&title=Evolution+of+Images+with+Diversity+and+Constraints+Using+a+Generative+Adversarial+Network\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0068\"><span><span class=\"hlFld-ContribAuthor\">Nie, <span class=\"NLM_given-names\">D.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Trullo</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Lian</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Petitjean</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Ruan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Q.</span> Wang</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Shen</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Medical Image Synthesis with Context-Aware Generative Adversarial Networks</span>.\u201dDescoteaux M., Maier-Hein L., Franz A., Jannin P., Collins D., Duchesne S. (eds). In <i>International Conference on Medical Image Computing and Computer-Assisted Intervention</i>, <span class=\"NLM_fpage\">417</span>\u2013<span class=\"NLM_lpage\">425</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0068&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-319-66179-7_48\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=417-425&author=D.+Nie&author=R.+Trullo&author=J.+Lian&author=C.+Petitjean&author=S.+Ruan&author=Q.+Wang&author=D.+Shen&title=Medical+Image+Synthesis+with+Context-Aware+Generative+Adversarial+Networks\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0069\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span> <span class=\"NLM_year\">2019</span>. <i>Personas\u2014User Focused Design</i>. <span class=\"NLM_edition\">2nd ed.</span> <span class=\"NLM_publisher-name\">Springer</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0069&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-1-4471-7427-1\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&author=L.+Nielsen&title=Personas%E2%80%94User+Focused+Design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0070\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. S.</span> Hansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Stage</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Billestrup</span>. <span class=\"NLM_year\">2015</span>. \u201c<span class=\"NLM_article-title\">A Template for Design Personas: Analysis of 47 Persona Descriptions from Danish Industries and Organizations</span>.\u201d <i>International Journal of Sociotechnology and Knowledge Development</i> 7 (1): <span class=\"NLM_fpage\">45</span>\u2013<span class=\"NLM_lpage\">61</span>. doi:<span class=\"NLM_pub-id\">10.4018/ijskd.2015010104</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0070&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.4018%2Fijskd.2015010104\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=7&publication_year=2015&pages=45-61&issue=1&author=L.+Nielsen&author=K.+S.+Hansen&author=J.+Stage&author=J.+Billestrup&title=A+Template+for+Design+Personas%3A+Analysis+of+47+Persona+Descriptions+from+Danish+Industries+and+Organizations&doi=10.4018%2Fijskd.2015010104\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0071\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Who Are Your Users?: Comparing Media Professionals\u2019 Preconception of Users to Data-Driven Personas</span>.\u201d In <i>Proceedings of the 29th Australian Conference on Computer-Human Interaction</i>, <span class=\"NLM_fpage\">602</span>\u2013<span class=\"NLM_lpage\">606</span>. doi:<span class=\"NLM_pub-id\">10.1145/3152771.3156178</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0071&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3152771.3156178\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=602-606&author=L.+Nielsen&author=S.-G.+Jung&author=J.+An&author=J.+Salminen&author=H.+Kwak&author=B.+J.+Jansen&title=Who+Are+Your+Users%3F%3A+Comparing+Media+Professionals%E2%80%99+Preconception+of+Users+to+Data-Driven+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0072\"><span><span class=\"hlFld-ContribAuthor\">Nielsen, <span class=\"NLM_given-names\">L.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Storgaard Hansen</span>. <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_chapter-title\">Personas Is Applicable: A Study on the Use of Personas in Denmark</span>.\u201d In <i>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">ACM, New York, NY.. 1665</span>\u2013<span class=\"NLM_lpage\">1674</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0072&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2556288.2557080\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2014&pages=ACM%2C+New+York%2C+NY..+1665-1674&author=L.+Nielsen&author=K.+Storgaard+Hansen&title=Personas+Is+Applicable%3A+A+Study+on+the+Use+of+Personas+in+Denmark\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0073\"><span><span class=\"hlFld-ContribAuthor\">\u00d6zmen, <span class=\"NLM_given-names\">M. U.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Yucel</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Handling of Online Information by Users: Evidence from TED Talks</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1309</span>\u2013<span class=\"NLM_lpage\">1323</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1584244</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0073&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1584244&amp;tollfreelink=2_18_a5e4c4277f85c350fbaa76e5abe368745e9ba9adacefeaf6da7827e143b803d5\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0073&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200009\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1309-1323&issue=12&author=M.+U.+%C3%96zmen&author=E.+Yucel&title=Handling+of+Online+Information+by+Users%3A+Evidence+from+TED+Talks&doi=10.1080%2F0144929X.2019.1584244\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0074\"><span><span class=\"hlFld-ContribAuthor\">Palan, <span class=\"NLM_given-names\">S.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Schitter</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_article-title\">Prolific. Ac\u2014A Subject Pool for Online Experiments</span>.\u201d <i>Journal of Behavioral and Experimental Finance</i> 17: <span class=\"NLM_fpage\">22</span>\u2013<span class=\"NLM_lpage\">27</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0074&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.jbef.2017.12.004\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0074&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000427997000004\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=17&publication_year=2018&pages=22-27&author=S.+Palan&author=C.+Schitter&title=Prolific.+Ac%E2%80%94A+Subject+Pool+for+Online+Experiments\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0075\"><span><span class=\"hlFld-ContribAuthor\">Pitk\u00e4nen, <span class=\"NLM_given-names\">L.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Salminen</span>. <span class=\"NLM_year\">2013</span>. \u201c<span class=\"NLM_chapter-title\">Managing the Crowd: A Study on Videography Application</span>.\u201d In <i>Proceedings of Applied Business and Entrepreneurship Association International (ABEAI), November</i>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2013&author=L.+Pitk%C3%A4nen&author=J.+Salminen&title=Managing+the+Crowd%3A+A+Study+on+Videography+Application\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0076\"><span><span class=\"hlFld-ContribAuthor\">Probster, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M. E.</span> Haque</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">N.</span> Marsden</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Perceptions of Personas: The Role of Instructions</span>.\u201d In <i>2018 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)</i>, <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">8</span>. doi:<span class=\"NLM_pub-id\">10.1109/ICE.2018.8436339</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0076&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICE.2018.8436339\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=1-8&author=M.+Probster&author=M.+E.+Haque&author=N.+Marsden&title=Perceptions+of+Personas%3A+The+Role+of+Instructions\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0077\"><span><span class=\"hlFld-ContribAuthor\">Pruitt, <span class=\"NLM_given-names\">J.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Adlin</span>. <span class=\"NLM_year\">2006</span>. <i>The Persona Lifecycle: Keeping People in Mind Throughout Product Design</i>. <span class=\"NLM_edition\">1st ed.</span> <span class=\"NLM_publisher-name\">Morgan Kaufmann</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2006&author=J.+Pruitt&author=T.+Adlin&title=The+Persona+Lifecycle%3A+Keeping+People+in+Mind+Throughout+Product+Design\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0078\"><span><span class=\"hlFld-ContribAuthor\">R\u00f6nkk\u00f6, <span class=\"NLM_given-names\">K.</span></span> <span class=\"NLM_year\">2005</span>. \u201c<span class=\"NLM_chapter-title\">An Empirical Study Demonstrating How Different Design Constraints, Project Organization and Contexts Limited the Utility of Personas</span>.\u201d In <i>Proceedings of the Proceedings of the 38th Annual Hawaii International Conference on System Sciences \u2013 Volume 08</i>. doi:<span class=\"NLM_pub-id\">10.1109/HICSS.2005.85</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0078&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FHICSS.2005.85\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2005&author=K.+R%C3%B6nkk%C3%B6&title=An+Empirical+Study+Demonstrating+How+Different+Design+Constraints%2C+Project+Organization+and+Contexts+Limited+the+Utility+of+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0079\"><span><span class=\"hlFld-ContribAuthor\">R\u00f6nkk\u00f6, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Hellman</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B.</span> Kilander</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Dittrich</span>. <span class=\"NLM_year\">2004</span>. \u201c<span class=\"NLM_chapter-title\">Personas Is Not Applicable: Local Remedies Interpreted in a Wider Context</span>.\u201d In <i>Proceedings of the Eighth Conference on Participatory Design: Artful Integration: Interweaving Media, Materials and Practices \u2013 Volume 1</i>, <span class=\"NLM_fpage\">112</span>\u2013<span class=\"NLM_lpage\">120</span>. doi:<span class=\"NLM_pub-id\">10.1145/1011870.1011884</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0079&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F1011870.1011884\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2004&pages=112-120&author=K.+R%C3%B6nkk%C3%B6&author=M.+Hellman&author=B.+Kilander&author=Y.+Dittrich&title=Personas+Is+Not+Applicable%3A+Local+Remedies+Interpreted+in+a+Wider+Context\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0080\"><span><span class=\"hlFld-ContribAuthor\">Salimans, <span class=\"NLM_given-names\">T.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Goodfellow</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Zaremba</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V.</span> Cheung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Radford</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Chen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Chen</span>. <span class=\"NLM_year\">2016</span>. \u201c<span class=\"NLM_chapter-title\">Improved Techniques for Training GANs</span>.\u201d In <i>Advances in Neural Information Processing Systems 29</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D. D.</span> Lee</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Sugiyama</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">U. V.</span> Luxburg</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">I.</span> Guyon</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Garnett</span>, <span class=\"NLM_fpage\">2234</span>\u2013<span class=\"NLM_lpage\">2242</span>. <span class=\"NLM_publisher-name\">Curran Associates, Inc</span>. <a class=\"ext-link\" href=\"http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf\" target=\"_blank\">http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2016&pages=2234-2242&author=T.+Salimans&author=I.+Goodfellow&author=W.+Zaremba&author=V.+Cheung&author=A.+Radford&author=X.+Chen&author=X.+Chen&title=Improved+Techniques+for+Training+GANs\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0081\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Almerekhi</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Dey</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018a</span>. \u201c<span class=\"NLM_chapter-title\">Inter-Rater Agreement for Social Computing Studies</span>.\u201d In <i>Proceedings of The Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS \u2013 2018), October 15</i>. The Fifth International Conference on Social Networks Analysis, Management and Security (SNAMS \u2013 2018), Valencia, Spain.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0081&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FSNAMS.2018.8554744\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018a&author=J.+Salminen&author=H.+Almerekhi&author=P.+Dey&author=B.+J.+Jansen&title=Inter-Rater+Agreement+for+Social+Computing+Studies\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0082\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>. <span class=\"NLM_year\">2018b</span>. \u201c<span class=\"NLM_article-title\">Are Personas Done? Evaluating Their Usefulness in the Age of Digital Analytics</span>.\u201d <i>Persona Studies</i> 4 (2): <span class=\"NLM_fpage\">47</span>\u2013<span class=\"NLM_lpage\">65</span>. doi:<span class=\"NLM_pub-id\">10.21153/psj2018vol4no2art737</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0082&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.21153%2Fpsj2018vol4no2art737\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=4&publication_year=2018b&pages=47-65&issue=2&author=J.+Salminen&author=B.+J.+Jansen&author=J.+An&author=H.+Kwak&author=S.+Jung&title=Are+Personas+Done%3F+Evaluating+Their+Usefulness+in+the+Age+of+Digital+Analytics&doi=10.21153%2Fpsj2018vol4no2art737\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0083\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>. <span class=\"NLM_year\">2019a</span>. \u201c<span class=\"NLM_chapter-title\">Automatic Persona Generation for Online Content Creators: Conceptual Rationale and a Research Agenda</span>.\u201d In <i>Personas\u2014User Focused Design</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Nielsen</span>, <span class=\"NLM_fpage\">135</span>\u2013<span class=\"NLM_lpage\">160</span>. <span class=\"NLM_publisher-loc\">London</span>: <span class=\"NLM_publisher-name\">Springer</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-1-4471-7427-1_8</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0083&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-1-4471-7427-1_8\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019a&pages=135-160&author=J.+Salminen&author=B.+J.+Jansen&author=J.+An&author=H.+Kwak&author=S.-G.+Jung&title=Automatic+Persona+Generation+for+Online+Content+Creators%3A+Conceptual+Rationale+and+a+Research+Agenda\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0084\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Nielsen</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019b</span>. \u201c<span class=\"NLM_article-title\">Confusion and Information Triggered by Photos in Persona Profiles</span>.\u201d <i>International Journal of Human-Computer Studies</i> 129: <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">14</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ijhcs.2019.03.005</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0084&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ijhcs.2019.03.005\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0084&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000472687700001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=129&publication_year=2019b&pages=1-14&author=J.+Salminen&author=S.+Jung&author=J.+An&author=H.+Kwak&author=L.+Nielsen&author=B.+J.+Jansen&title=Confusion+and+Information+Triggered+by+Photos+in+Persona+Profiles&doi=10.1016%2Fj.ijhcs.2019.03.005\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0085\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019c</span>. \u201c<span class=\"NLM_chapter-title\">Detecting Demographic Bias in Automatically Generated Personas</span>.\u201d In <i>Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">LBW0122:1</span>\u2013<span class=\"NLM_lpage\">LBW0122:6</span>. doi:<span class=\"NLM_pub-id\">10.1145/3290607.3313034</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0085&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3290607.3313034\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019c&pages=LBW0122%3A1-LBW0122%3A6&author=J.+Salminen&author=S.-G.+Jung&author=B.+J.+Jansen&title=Detecting+Demographic+Bias+in+Automatically+Generated+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0086\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S. G.</span> Jung</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019d</span>. \u201c<span class=\"NLM_chapter-title\">The Future of Data-Driven Personas: A Marriage of Online Analytics Numbers and Human Attributes</span>.\u201d In <i>ICEIS 2019 \u2013 Proceedings of the 21st International Conference on Enterprise Information Systems</i>, <span class=\"NLM_fpage\">596</span>\u2013<span class=\"NLM_lpage\">603</span>. <a class=\"ext-link\" href=\"https://pennstate.pure.elsevier.com/en/publications/the-future-of-data-driven-personas-a-marriage-of-online-analytics\" target=\"_blank\">https://pennstate.pure.elsevier.com/en/publications/the-future-of-data-driven-personas-a-marriage-of-online-analytics</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0086&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.5220%2F0007744706080615\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019d&pages=596-603&author=J.+Salminen&author=S.+G.+Jung&author=B.+J.+Jansen&title=The+Future+of+Data-Driven+Personas%3A+A+Marriage+of+Online+Analytics+Numbers+and+Human+Attributes\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0087\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019e</span>. \u201c<span class=\"NLM_chapter-title\">The Effect of Smiling Pictures on Perceptions of Personas</span>.\u201d In <i>UMAP\u201919 Adjunct: Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization</i>. doi:<span class=\"NLM_pub-id\">10.1145/3314183.3324973</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0087&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3314183.3324973\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019e&author=J.+Salminen&author=S.-G.+Jung&author=J.+M.+Santos&author=B.+J.+Jansen&title=The+Effect+of+Smiling+Pictures+on+Perceptions+of+Personas\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0088\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019f</span>. \u201c<span class=\"NLM_article-title\">Does a Smile Matter if the Person Is Not Real?: The Effect of a Smile and Stock Photos on Persona Perceptions</span>.\u201d <i>International Journal of Human\u2013Computer Interaction</i> 0 (0): <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">23</span>. doi:<span class=\"NLM_pub-id\">10.1080/10447318.2019.1664068</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0088&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F10447318.2019.1664068&amp;tollfreelink=2_18_a0332bf1d9c1df8b5aabef314d6cb14b7620b3d836adee079ccd7dbe4924dcd3\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=0&publication_year=2019f&pages=1-23&issue=0&author=J.+Salminen&author=S.-G.+Jung&author=J.+M.+Santos&author=B.+J.+Jansen&title=Does+a+Smile+Matter+if+the+Person+Is+Not+Real%3F%3A+The+Effect+of+a+Smile+and+Stock+Photos+on+Persona+Perceptions&doi=10.1080%2F10447318.2019.1664068\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0089\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018c</span>. \u201c<span class=\"NLM_chapter-title\">Persona Perception Scale: Developing and Validating an Instrument for Human-Like Representations of Data</span>.\u201d In <i>CHI\u201918 Extended Abstracts: CHI Conference on Human Factors in Computing Systems Extended Abstracts Proceedings</i>. CHI 2018 Extended Abstracts, Montr\u00e9al, Canada. doi:<span class=\"NLM_pub-id\">10.1145/3170427.3188461</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0089&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3170427.3188461\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018c&author=J.+Salminen&author=H.+Kwak&author=J.+M.+Santos&author=S.-G.+Jung&author=J.+An&author=B.+J.+Jansen&title=Persona+Perception+Scale%3A+Developing+and+Validating+an+Instrument+for+Human-Like+Representations+of+Data\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0090\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Nielsen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2018d</span>. \u201c<span class=\"NLM_chapter-title\">\u2018Is More Better?\u2019: Impact of Multiple Photos on Perception of Persona Profiles</span>.\u201d In <i>Proceedings of ACM CHI Conference on Human Factors in Computing Systems (CHI2018), April 21</i>. ACM CHI Conference on Human Factors in Computing Systems (CHI2018), ACM, Montr\u00e9al, Canada. Paper 317.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0090&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3173574.3173891\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018d&author=J.+Salminen&author=L.+Nielsen&author=S.-G.+Jung&author=J.+An&author=H.+Kwak&author=B.+J.+Jansen&title=%E2%80%98Is+More+Better%3F%E2%80%99%3A+Impact+of+Multiple+Photos+on+Perception+of+Persona+Profiles\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0091\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J. M.</span> Santos</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.-G.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Eslami</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>. <span class=\"NLM_year\">2019g</span>. \u201c<span class=\"NLM_article-title\">Persona Transparency: Analyzing the Impact of Explanations on Perceptions of Data-Driven Personas</span>.\u201d <i>International Journal of Human\u2013Computer Interaction</i> 0 (0): <span class=\"NLM_fpage\">1</span>\u2013<span class=\"NLM_lpage\">13</span>. doi:<span class=\"NLM_pub-id\">10.1080/10447318.2019.1688946</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0091&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F10447318.2019.1688946&amp;tollfreelink=2_18_26ba6235947b0ff6baa1ec4656b5d6ca46f3fc6043f40ca45417f4bc50784fb7\">[Taylor &amp; Francis Online]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=0&publication_year=2019g&pages=1-13&issue=0&author=J.+Salminen&author=J.+M.+Santos&author=S.-G.+Jung&author=M.+Eslami&author=B.+J.+Jansen&title=Persona+Transparency%3A+Analyzing+the+Impact+of+Explanations+on+Perceptions+of+Data-Driven+Personas&doi=10.1080%2F10447318.2019.1688946\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0092\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> \u015eeng\u00fcn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Vieweg</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Harrell</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">Generating Cultural Personas from Social Data: A Perspective of Middle Eastern Users</span>.\u201d In <i>Proceedings of The Fourth International Symposium on Social Networks Analysis, Management and Security (SNAMS-2017)</i>. The Fourth International Symposium on Social Networks Analysis, Management and Security (SNAMS-2017), IEEE. Prague, Czech Republic, 1-8.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0092&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FFiCloudW.2017.97\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&author=J.+Salminen&author=S.+%C5%9Eeng%C3%BCn&author=H.+Kwak&author=B.+J.+Jansen&author=J.+An&author=S.+Jung&author=S.+Vieweg&author=F.+Harrell&title=Generating+Cultural+Personas+from+Social+Data%3A+A+Perspective+of+Middle+Eastern+Users\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0093\"><span><span class=\"hlFld-ContribAuthor\">Salminen, <span class=\"NLM_given-names\">J.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> \u015eeng\u00fcn</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Kwak</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">B. J.</span> Jansen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> An</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Jung</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Vieweg</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">F.</span> Harrell</span>. <span class=\"NLM_year\">2018e</span>. \u201c<span class=\"NLM_article-title\">From 2,772 Segments to Five Personas: Summarizing a Diverse Online Audience by Generating Culturally Adapted Personas</span>.\u201d <i>First Monday</i> 23 (6). doi:<span class=\"NLM_pub-id\">10.5210/fm.v23i6.8415</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0093&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.5210%2Ffm.v23i6.8415\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=23&publication_year=2018e&issue=6&author=J.+Salminen&author=S.+%C5%9Eeng%C3%BCn&author=H.+Kwak&author=B.+J.+Jansen&author=J.+An&author=S.+Jung&author=S.+Vieweg&author=F.+Harrell&title=From+2%2C772+Segments+to+Five+Personas%3A+Summarizing+a+Diverse+Online+Audience+by+Generating+Culturally+Adapted+Personas&doi=10.5210%2Ffm.v23i6.8415\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0094\"><span><span class=\"hlFld-ContribAuthor\">\u015eeng\u00fcn, <span class=\"NLM_given-names\">S.</span></span> <span class=\"NLM_year\">2014</span>. \u201c<span class=\"NLM_article-title\">A Semiotic Reading of Digital Avatars and Their Role of Uncertainty Reduction in Digital Communication</span>.\u201d <i>Journal of Media Critiques</i> 1 (Special): <span class=\"NLM_fpage\">149</span>\u2013<span class=\"NLM_lpage\">162</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0094&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.17349%2Fjmc114311\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=1&publication_year=2014&pages=149-162&issue=Special&author=S.+%C5%9Eeng%C3%BCn&title=A+Semiotic+Reading+of+Digital+Avatars+and+Their+Role+of+Uncertainty+Reduction+in+Digital+Communication\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0095\"><span><span class=\"hlFld-ContribAuthor\">Seng\u00fcn, <span class=\"NLM_given-names\">S.</span></span> <span class=\"NLM_year\">2015</span>. \u201c<span class=\"NLM_article-title\">Why Do I Fall for the Elf, When I am No Orc Myself? The Impl\u0131cat\u0131ons of V\u0131rtual Avatars \u0131n D\u0131g\u0131tal Commun\u0131cat\u0131on</span>.\u201d <i>Comunica\u00e7\u00e3o e Sociedade</i> 27: <span class=\"NLM_fpage\">181</span>\u2013<span class=\"NLM_lpage\">193</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0095&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.17231%2Fcomsoc.27%282015%29.2096\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=27&publication_year=2015&pages=181-193&author=S.+Seng%C3%BCn&title=Why+Do+I+Fall+for+the+Elf%2C+When+I+am+No+Orc+Myself%3F+The+Impl%C4%B1cat%C4%B1ons+of+V%C4%B1rtual+Avatars+%C4%B1n+D%C4%B1g%C4%B1tal+Commun%C4%B1cat%C4%B1on\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0096\"><span><span class=\"hlFld-ContribAuthor\">Shmelkov, <span class=\"NLM_given-names\">K.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Schmid</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Alahari</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">How Good Is My GAN?</span>\u201d In <i>Computer Vision \u2013 ECCV 2018</i>, edited by <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">V.</span> Ferrari</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">M.</span> Hebert</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C.</span> Sminchisescu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Weiss</span>, <span class=\"NLM_fpage\">218</span>\u2013<span class=\"NLM_lpage\">234</span>. <span class=\"NLM_publisher-name\">Springer International Publishing</span>. doi:<span class=\"NLM_pub-id\">10.1007/978-3-030-01216-8_14</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0096&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1007%2F978-3-030-01216-8_14\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=218-234&author=K.+Shmelkov&author=C.+Schmid&author=K.+Alahari&title=How+Good+Is+My+GAN%3F\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0097\"><span><span class=\"hlFld-ContribAuthor\">Shmueli-Scheuer, <span class=\"NLM_given-names\">M.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Sandbank</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Konopnicki</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">O. P.</span> Nakash</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">Exploring the Universe of Egregious Conversations in Chatbots</span>.\u201d In <i>Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion</i>, <span class=\"NLM_fpage\">ACM, New York, NY, Article 16</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0097&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3180308.3180324\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=ACM%2C+New+York%2C+NY%2C+Article+16&author=M.+Shmueli-Scheuer&author=T.+Sandbank&author=D.+Konopnicki&author=O.+P.+Nakash&title=Exploring+the+Universe+of+Egregious+Conversations+in+Chatbots\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0098\"><span><span class=\"hlFld-ContribAuthor\">Song, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Li</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Wang</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Qi</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">Talking Face Generation by Conditional Recurrent Adversarial Network</span>.\u201d In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, <span class=\"NLM_fpage\">Macao, China. 919</span>\u2013<span class=\"NLM_lpage\">925</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0098&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.24963%2Fijcai.2019%2F129\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=Macao%2C+China.+919-925&author=Y.+Song&author=D.+Li&author=A.+Wang&author=H.+Qi&title=Talking+Face+Generation+by+Conditional+Recurrent+Adversarial+Network\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0099\"><span><span class=\"hlFld-ContribAuthor\">Tan, <span class=\"NLM_given-names\">W. R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">C. S.</span> Chan</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H. E.</span> Aguirre</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K.</span> Tanaka</span>. <span class=\"NLM_year\">2017</span>. \u201c<span class=\"NLM_chapter-title\">ArtGAN: Artwork Synthesis with Conditional Categorical GANs</span>.\u201d In <i>2017 IEEE International Conference on Image Processing (ICIP)</i>, <span class=\"NLM_fpage\">IEEE. 3760</span>\u2013<span class=\"NLM_lpage\">3764</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0099&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICIP.2017.8296985\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2017&pages=IEEE.+3760-3764&author=W.+R.+Tan&author=C.+S.+Chan&author=H.+E.+Aguirre&author=K.+Tanaka&title=ArtGAN%3A+Artwork+Synthesis+with+Conditional+Categorical+GANs\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0100\"><span><span class=\"hlFld-ContribAuthor\">Weiss, <span class=\"NLM_given-names\">J. K.</span></span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E. L.</span> Cohen</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_article-title\">Clicking for Change: The Role of Empathy and Negative Affect on Engagement with a Charitable Social Media Campaign</span>.\u201d <i>Behaviour &amp; Information Technology</i> 38 (12): <span class=\"NLM_fpage\">1185</span>\u2013<span class=\"NLM_lpage\">1193</span>. doi:<span class=\"NLM_pub-id\">10.1080/0144929X.2019.1578827</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0100&amp;dbid=20&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1080%2F0144929X.2019.1578827&amp;tollfreelink=2_18_bfdbadcca82097fb41a86f42767975539644af3031ac0e88267a6b1c8854be0e\">[Taylor &amp; Francis Online]</a>, <a href=\"/servlet/linkout?suffix=CIT0100&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000495349200001\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=38&publication_year=2019&pages=1185-1193&issue=12&author=J.+K.+Weiss&author=E.+L.+Cohen&title=Clicking+for+Change%3A+The+Role+of+Empathy+and+Negative+Affect+on+Engagement+with+a+Charitable+Social+Media+Campaign&doi=10.1080%2F0144929X.2019.1578827\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0101\"><span><span class=\"hlFld-ContribAuthor\">Wongpakaran, <span class=\"NLM_given-names\">N.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Wongpakaran</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">D.</span> Wedding</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">K. L.</span> Gwet</span>. <span class=\"NLM_year\">2013</span>. \u201c<span class=\"NLM_article-title\">A Comparison of Cohen\u2019s Kappa and Gwet\u2019s AC1 when Calculating Inter-Rater Reliability Coefficients: A Study Conducted with Personality Disorder Samples</span>.\u201d <i>BMC Medical Research Methodology</i> 13 (1): <span class=\"NLM_fpage\">61</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0101&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1186%2F1471-2288-13-61\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0101&amp;dbid=8&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=23627889\" target=\"_blank\">[PubMed]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=13&publication_year=2013&pages=61&issue=1&author=N.+Wongpakaran&author=T.+Wongpakaran&author=D.+Wedding&author=K.+L.+Gwet&title=A+Comparison+of+Cohen%E2%80%99s+Kappa+and+Gwet%E2%80%99s+AC1+when+Calculating+Inter-Rater+Reliability+Coefficients%3A+A+Study+Conducted+with+Personality+Disorder+Samples\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0102\"><span><span class=\"hlFld-ContribAuthor\">Yang, <span class=\"NLM_given-names\">X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Li</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">S.</span> Lyu</span>. <span class=\"NLM_year\">2019</span>. \u201c<span class=\"NLM_chapter-title\">Exposing Deep Fakes Using Inconsistent Head Poses</span>.\u201d In <i>ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</i>, <span class=\"NLM_fpage\">IEEE, 8261</span>\u2013<span class=\"NLM_lpage\">8265</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0102&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FICASSP.2019.8683164\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019&pages=IEEE%2C+8261-8265&author=X.+Yang&author=Y.+Li&author=S.+Lyu&title=Exposing+Deep+Fakes+Using+Inconsistent+Head+Poses\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0103\"><span><span class=\"hlFld-ContribAuthor\">Yin, <span class=\"NLM_given-names\">W.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Fu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">L.</span> Sigal</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Xue</span>. <span class=\"NLM_year\">2017</span>. \u201cSemi-Latent GAN: Learning to Generate and Modify Facial Images from Attributes.\u201d <i>ArXiv:1704.02166 [Cs]</i>. <a class=\"ext-link\" href=\"http://arxiv.org/abs/1704.02166\" target=\"_blank\">http://arxiv.org/abs/1704.02166</a>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"></span><span class=\"googleScholar-container\"><a class=\"google-scholar\" href=\"http://scholar.google.com/scholar?hl=en&q=Yin%2C+W.%2C+Y.+Fu%2C+L.+Sigal%2C+and+X.+Xue.+2017.+%E2%80%9CSemi-Latent+GAN%3A+Learning+to+Generate+and+Modify+Facial+Images+from+Attributes.%E2%80%9D+ArXiv%3A1704.02166+%5BCs%5D.+http%3A%2F%2Farxiv.org%2Fabs%2F1704.02166.\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0104\"><span><span class=\"hlFld-ContribAuthor\">Yuan, <span class=\"NLM_given-names\">Y.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Su</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Liu</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">G.</span> Zeng</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Locally and Multiply Distorted Image Quality Assessment via Multi-Stage CNNs</span>.\u201d <i>Information Processing &amp; Management</i> 57 (4): <span class=\"NLM_fpage\">102175</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ipm.2019.102175</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0104&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ipm.2019.102175\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0104&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000531082800005\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=57&publication_year=2020&pages=102175&issue=4&author=Y.+Yuan&author=H.+Su&author=J.+Liu&author=G.+Zeng&title=Locally+and+Multiply+Distorted+Image+Quality+Assessment+via+Multi-Stage+CNNs&doi=10.1016%2Fj.ipm.2019.102175\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0105\"><span><span class=\"hlFld-ContribAuthor\">Zhang, <span class=\"NLM_given-names\">X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.-F.</span> Brown</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A.</span> Shankar</span>. <span class=\"NLM_year\">2016</span>. \u201c<span class=\"NLM_chapter-title\">Data-driven Personas: Constructing Archetypal Users with Clickstreams and User Telemetry</span>.\u201d In <i>Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems</i>, <span class=\"NLM_fpage\">ACM, New York, NY, 5350</span>\u2013<span class=\"NLM_lpage\">5359</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0105&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F2858036.2858523\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2016&pages=ACM%2C+New+York%2C+NY%2C+5350-5359&author=X.+Zhang&author=H.-F.+Brown&author=A.+Shankar&title=Data-driven+Personas%3A+Constructing+Archetypal+Users+with+Clickstreams+and+User+Telemetry\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0106\"><span><span class=\"hlFld-ContribAuthor\">Zhang, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Isola</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">A. A.</span> Efros</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">E.</span> Shechtman</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">O.</span> Wang</span>. <span class=\"NLM_year\">2018</span>. \u201c<span class=\"NLM_chapter-title\">The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</span>.\u201d In <i>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</i>, <span class=\"NLM_fpage\">586</span>\u2013<span class=\"NLM_lpage\">595</span>. doi:<span class=\"NLM_pub-id\">10.1109/CVPR.2018.00068</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0106&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1109%2FCVPR.2018.00068\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2018&pages=586-595&author=R.+Zhang&author=P.+Isola&author=A.+A.+Efros&author=E.+Shechtman&author=O.+Wang&title=The+Unreasonable+Effectiveness+of+Deep+Features+as+a+Perceptual+Metric\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0107\"><span><span class=\"hlFld-ContribAuthor\">Zhao, <span class=\"NLM_given-names\">R.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Xue</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">J.</span> Cai</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Gao</span>. <span class=\"NLM_year\">2020</span>. \u201c<span class=\"NLM_article-title\">Parsing Human Image by Fusing Semantic and Spatial Features: A Deep Learning Approach</span>.\u201d <i>Information Processing &amp; Management</i> 57 (6): <span class=\"NLM_fpage\">102306</span>. doi:<span class=\"NLM_pub-id\">10.1016/j.ipm.2020.102306</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0107&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1016%2Fj.ipm.2020.102306\" target=\"_blank\">[Crossref]</a>, <a href=\"/servlet/linkout?suffix=CIT0107&amp;dbid=128&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=000582206800034\" target=\"_blank\">[Web of Science &#0174;]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=57&publication_year=2020&pages=102306&issue=6&author=R.+Zhao&author=Y.+Xue&author=J.+Cai&author=Z.+Gao&title=Parsing+Human+Image+by+Fusing+Semantic+and+Spatial+Features%3A+A+Deep+Learning+Approach&doi=10.1016%2Fj.ipm.2020.102306\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0108\"><span><span class=\"hlFld-ContribAuthor\">Zhou, <span class=\"NLM_given-names\">M. X.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">W.</span> Chen</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Xiao</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">H.</span> Yang</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">T.</span> Chi</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">R.</span> Williams</span>. <span class=\"NLM_year\">2019a</span>. \u201c<span class=\"NLM_chapter-title\">Getting Virtually Personal: Chatbots who Actively Listen to You and Infer Your Personality</span>.\u201d In <i>Proceedings of the 24th International Conference on Intelligent User Interfaces: Companion</i>, <span class=\"NLM_fpage\">ACM, New York, NY, 123</span>\u2013<span class=\"NLM_lpage\">124</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0108&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1145%2F3308557.3308667\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&publication_year=2019a&pages=ACM%2C+New+York%2C+NY%2C+123-124&author=M.+X.+Zhou&author=W.+Chen&author=Z.+Xiao&author=H.+Yang&author=T.+Chi&author=R.+Williams&title=Getting+Virtually+Personal%3A+Chatbots+who+Actively+Listen+to+You+and+Infer+Your+Personality\" target=\"_blank\">[Google Scholar]</a></span></span></span></li><li id=\"CIT0109\"><span><span class=\"hlFld-ContribAuthor\">Zhou, <span class=\"NLM_given-names\">H.</span></span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Y.</span> Liu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">Z.</span> Liu</span>, <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">P.</span> Luo</span>, and <span class=\"hlFld-ContribAuthor\"><span class=\"NLM_given-names\">X.</span> Wang</span>. <span class=\"NLM_year\">2019b</span>. \u201c<span class=\"NLM_article-title\">Talking Face Generation by Adversarially Disentangled Audio-Visual Representation</span>.\u201d <i>Proceedings of the AAAI Conference on Artificial Intelligence</i> 33 (01): <span class=\"NLM_fpage\">9299</span>\u2013<span class=\"NLM_lpage\">9306</span>. doi:<span class=\"NLM_pub-id\">10.1609/aaai.v33i01.33019299</span>.<span class=\"refLink-block\">\u00a0<span class=\"xlinks-container\"><a href=\"/servlet/linkout?suffix=CIT0109&amp;dbid=16&amp;doi=10.1080%2F0144929X.2020.1838610&amp;key=10.1609%2Faaai.v33i01.33019299\" target=\"_blank\">[Crossref]</a></span><span class=\"googleScholar-container\">,\u00a0<a class=\"google-scholar\" href=\"http://scholar.google.com/scholar_lookup?hl=en&volume=33&publication_year=2019b&pages=9299-9306&issue=01&author=H.+Zhou&author=Y.+Liu&author=Z.+Liu&author=P.+Luo&author=X.+Wang&title=Talking+Face+Generation+by+Adversarially+Disentangled+Audio-Visual+Representation&doi=10.1609%2Faaai.v33i01.33019299\" target=\"_blank\">[Google Scholar]</a></span></span></span></li></ul></div><div class=\"response\"><div class=\"sub-article-title\"></div></div>", "</article>", "</div>", "<div class=\"tab tab-pane\" id=\"relatedContent\">", "</div>", "<div class=\"tab tab-pane \" id=\"metrics-content\">", "<div class=\"articleMetaDrop publicationContentDropZone publicationContentDropZoneMetrics\" data-pb-dropzone=\"publicationContentDropZoneMetrics\">", "<div class=\"widget literatumArticleMetricsWidget none  widget-none\" id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"ajaxWidget\" data-ajax-widget=\"literatumArticleMetricsWidget\" data-ajax-widget-id=\"00886058-9b49-4cdf-9f1e-deb78b7818c3\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"access__limit\" data-pb-dropzone=\"accessLimitPage\">", "</div>", "</div>", "</div>", "</div>", "<input id=\"viewLargeImageCaption\" type=\"hidden\" value=\"View Large Image\" /></div>", "</div>", "</div>", "<div class=\"widget gql-alternative-widget none  widget-none  widget-compact-horizontal\" id=\"fb6dfe6a-7c21-4a53-ab68-74b96f8a474d\">", "<div class=\"wrapped \">", "<h2 class=\"widget-header header-none  header-compact-horizontal\">Alternative formats</h2>", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"tabs\">", "<ul class=\"tab-nav no-stick\" role=\"tablist\">", "<li><a href=\"https://www.tandfonline.com/doi/pdf/10.1080/0144929X.2020.1838610\" class=\"show-pdf\" role=\"button\" aria-selected=\"false\" target=\"_blank\"> PDF</a></li>", "<li><a href=\"https://www.tandfonline.com/doi/epub/10.1080/0144929X.2020.1838610\" class=\"show-epub\" role=\"button\" aria-selected=\"false\" target=\"_blank\"> EPUB</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-4 \">", "<div class=\"contents\" data-pb-dropzone=\"contents2\">", "<div class=\"widget general-bookmark-share none  widget-none  widget-compact-all\" id=\"c8494935-e102-4ff5-9395-4ffa44a77f1c\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\">", "<ul>", "<li>", "<div class=\"addthis_toolbox addthis_20x20_style\">", "<div class=\"custom_images\">", "<a class=\"addthis_button_twitter\">", "<span class=\"at-icon-twitter\"></span>", "</a>", "<a class=\"addthis_button_facebook\">", "<span class=\"at-icon-facebook\"></span>", "</a>", "<a class=\"addthis_button_email\">", "<span class=\"at-icon-email\"></span>", "</a>", "<a class=\"addthis_button_none\">", "<span class=\"at-icon-none\"></span>", "</a>", "<a class=\"addthis_button_compact\" tabindex=\"-1\"><span class=\"at-icon-wrapper\"></span>", "<span aria-describedby=\"shareOptions-description\">", "<span class=\"off-screen\" id=\"shareOptions-description\">More Share Options</span>", "</span>", "</a>", "</div>", "</div>", "</li>", "</ul>", "<script type=\"text/javascript\">", "    ", "    var script = document.createElement('script');", "    script.type='text/javascript';", "    script.src='//s7.addthis.com/js/250/addthis_widget.js#pubid=xa-4faab26f2cff13a7';", "    script.async = true;", "    $('head').append(script)", "</script>", "</div>", "</div>", "</div>", "<div class=\"widget general-html none  widget-none\" id=\"16111d74-c554-42b2-a277-f2727ad2b285\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \">&nbsp;</div>", "</div>", "</div>", "<div class=\"widget layout-tabs none further-fonts collapsed-view further-tab-margin collapsed-sticky widget-none  widget-compact-vertical\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"layout-tabs-dropzone\" data-pb-dropzone=\"layoutTabsDropzone\">", "<div class=\"widget general-html none furtherReadingTitle widget-none  widget-compact-all\" id=\"982b80ad-6fe6-4b98-9675-9b6eef03d365\">", "<div class=\"wrapped \">", "<h2 class=\"widget-header header-none  header-compact-all\">Related research <span class=\"tooltip-collapse\"></span></h2>", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"info hide\">", "<p><b>People also read</b> lists articles that other readers of this article have read.</p>", "<p><b>Recommended articles</b> lists articles that we recommend and is powered by our AI driven recommendation engine.</p>", "<p><b>Cited by</b> lists all citing articles based on Crossref citations.<br />Articles with the Crossref icon will open in a new tab.</p>", "</div></div>", "</div>", "</div>", "</div>", "<div class=\"tabs tabs-widget \" aria-live=\"polite\" aria-atomic=\"true\" aria-relevant=\"additions\">", "<ul class=\"tab-nav\" role=\"tablist\">", "<li class=\"active\" role=\"tab\">", "<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" title=\"show People also read\" class=\"\">People also read</a>", "</li>", "<li class=\"\" role=\"tab\">", "<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\" title=\"show Recommended articles\" class=\"\">Recommended articles</a>", "</li>", "<li class=\"\" role=\"tab\">", "<a href=\"#2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" title=\"show Cited by\" class=\"frwidget-tabs--cby\">Cited by</a>", "</li>", "</ul>", "<div class=\"tab-content\">", "<div class=\"tab-pane active\" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\">", "<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-58132d06-cf2f-4e31-a696-f4f2aa0cdd9a\" data-pb-dropzone-name=\"People also read\">", "<div class=\"widget ajaxCFCRWidget none  widget-none  widget-compact-all\" id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCFCRWidget\" data-ajax-widget-id=\"6627c593-d1c6-462c-b7cd-68e0d712409e\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-b6de7b7c-de82-45a5-9538-313dd15c6659\">", "<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-b6de7b7c-de82-45a5-9538-313dd15c6659\" data-pb-dropzone-name=\"Recommended articles\">", "<div class=\"widget ajaxAtmCRWidget none  widget-none  widget-compact-all\" id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxAtmCRWidget\" data-ajax-widget-id=\"a1515c7b-51b6-4fe5-aa95-c2e2bf11bcd4\" data-ajax-spin=\"true\" data-ajax-observe=\"true\"></div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"tab-pane \" id=\"2b85d6ca-6520-4a3d-8e4a-aa9f2ee3f33d-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\">", "<div class=\"tab-pane-content\" data-pb-dropzone=\"tab-357c6cfb-53ba-4fa0-8e6b-e69fc2b8ce9f\" data-pb-dropzone-name=\"Cited by\">", "<div class=\"widget ajaxCitedByWidget none  widget-none  widget-compact-all\" id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"ajaxWidget\" data-ajax-widget=\"ajaxCitedByWidget\" data-ajax-widget-id=\"0a7f4ac8-dc04-4b80-9e62-9325b4a9e708\" data-ajax-spin=\"true\" data-ajax-observe=\"true\">", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "<div class=\"widget pageFooter none  widget-none  widget-compact-all\" id=\"d97c173f-d838-4de1-bbd7-ed69f0d36a91\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><footer class=\"page-footer\">", "<div data-pb-dropzone=\"main\">", "<div class=\"widget responsive-layout none footer-subjects hidden-xs hidden-sm widget-none  widget-compact-all\" id=\"1f15adc0-4a59-4d27-93fe-8cbb14a5108a\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"container\">", "<div class=\"row row-md gutterless \">", "<div class=\"col-md-1-1 fit-padding\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget pbOptimizerWidget none  widget-none  widget-compact-all\" id=\"af788167-0054-4892-bd47-5de7cbd64256\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div data-optimizer data-widget-id=\"af788167-0054-4892-bd47-5de7cbd64256\" id=\"widget-af788167-0054-4892-bd47-5de7cbd64256\" data-observer>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"64a44adf-45ed-4da3-be26-ef25beb9dbee\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"6918e9df-910a-4206-9bd0-1a02bc17f740\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container-fluid\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-1-2 footer_left_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"aa9510dd-52ed-4b74-8211-fb510cd9468e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Information for</h3>", "<ul>", "<li><a href=\"https://authorservices.taylorandfrancis.com/\">Authors</a></li>", "<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/\">Corporate partners</a></li>", "<li><a href=\"https://editorresources.taylorandfrancisgroup.com/\">Editors</a></li>", "<li><a href=\"/page/librarians\">Librarians</a></li>", "<li><a href=\"/societies\">Societies</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-sm-1-2 footer_right_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"ac8a1c0f-9427-44dd-96be-4f2a6ff4ffce\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Open access</h3>", "<ul>", "<li><a href=\"/openaccess\">Overview</a></li>", "<li><a href=\"/openaccess/openjournals\">Open journals</a></li>", "<li><a href=\"/openaccess/openselect\">Open Select</a></li>", "<li><a href=\"/openaccess/dove\">Dove Medical Press</a></li>", "<li><a href=\"/openaccess/f1000\">F1000Research</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"fc564559-f496-499c-87c7-d851f371f061\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container-fluid\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-1-2 footer_left_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"cdd1a577-15dc-4271-8941-33a105ec6510\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Opportunities</h3>", "<ul>", "<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/marketing/\">Reprints and e-prints</a></li>", "<li><a href=\"https://taylorandfrancis.com/partnership/commercial/advertising-solutions/\" class=\"footer-ad-click\">Advertising solutions</a></li>", "<li><a href=\"https://taylorandfrancis.com/partnership/commercial/accelerated-publication/\">Accelerated publication</a></li>", "<li><a href=\"https://taylorandfrancis.com/who-we-serve/industry-government/business/purchasing-options/\">Corporate access solutions</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-sm-1-2 footer_right_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"f3fb3d36-db42-4373-9d0e-432958bf2fbc\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-info-list\">", "<h3>Help and information</h3>", "<ul>", "<li><a href=\"https://help.tandfonline.com\">Help and contact</a></li>", "<li><a href=\"https://newsroom.taylorandfrancisgroup.com/\">Newsroom</a></li>", "<li><a href=\"/action/showPublications?pubType=journal\">All journals</a></li>", "<li><a href=\"https://www.routledge.com/?utm_source=website&amp;utm_medium=banner&amp;utm_campaign=B004808_em1_10p_5ec_d713_footeradspot\">Books</a></li>", "</ul>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none footer-links widget-none  widget-compact-horizontal\" id=\"b2eecf80-9109-455e-a805-028552718986\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">", "<div class=\"row row-md  \">", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget responsive-layout none footer-responsive-container widget-none\" id=\"b997c64c-ce48-41ce-b3d6-9cb2d1c99131\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none \"><div class=\"container-fluid\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-1-2 footer_left_col\">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none  widget-none  widget-compact-all\" id=\"914433f6-0ea6-4a47-9781-07564061be86\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"footer-social-label\">", "<h3>Keep up to date</h3>", "</div>", "<div class=\"font-size-correction-sml\">Register to receive personalised research and resources by email</div>", "<div class=\"bs\">", "<div class=\"pull-left links font-size-correction\">", "<a class=\"font-size-correction-link\" href=\"https://taylorandfrancis.formstack.com/forms/tfoguest_signup\"><i class=\"fa fa-envelope-square\" title=\"Register to receive personalised research and resources by email\"></i>Sign me up</a>", "</div></div></div>", "</div>", "</div>", "<div class=\"widget literatumSocialLinks none  widget-none  widget-compact-all\" id=\"3b6a5e53-cd62-452f-adc1-92e187a0849d\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"bs\">", "<div class=\"pull-left links\">", "<a href=\"http://facebook.com/TaylorandFrancisGroup\">", "<i class=\"icon-facebook\" title=\"Taylor and Francis Group Facebook page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"fb-description\">", "<span class=\"off-screen\" id=\"fb-description\">Taylor and Francis Group Facebook page</span>", "</span>", "</a>", "</div>", "<div class=\"pull-left links\">", "<a href=\"https://twitter.com/tandfonline\">", "<i class=\"fa fa-twitter-square\" title=\"Taylor and Francis Group Twitter page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"twitter-description\">", "<span class=\"off-screen\" id=\"twitter-description\">Taylor and Francis Group Twitter page</span>", "</span>", "</a>", "</div>", "<div class=\"pull-left links\">", "<a href=\"http://linkedin.com/company/taylor-&-francis-group\">", "<i class=\"fa fa-linkedin-square\" title=\"Taylor and Francis Group LinkedIn page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"linkedin-description\">", "<span class=\"off-screen\" id=\"linkedin-description\">Taylor and Francis Group Linkedin page</span>", "</span>", "</a>", "</div>", "<div class=\"clearfix\"></div>", "<div class=\"pull-left links\">", "<a href=\"https://www.youtube.com/user/TaylorandFrancis\">", "<i class=\"fa fa-youtube-square\" title=\"Taylor and Francis Group YouTube page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"youtube-description\">", "<span class=\"off-screen\" id=\"youtube-description\">Taylor and Francis Group Youtube page</span>", "</span>", "</a>", "</div>", "<div class=\"pull-left links\">", "<a href=\"http://www.weibo.com/tandfchina\">", "<i class=\"fa fa-weibo\" title=\"Taylor and Francis Group Weibo page\" aria-hidden=\"true\" role=\"button\"></i>", "<span aria-describedby=\"weibo-description\">", "<span class=\"off-screen\" id=\"weibo-description\">Taylor and Francis Group Weibo page</span>", "</span>", "</a>", "</div>", "<div class=\"clearfix\"></div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-sm-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-md-1-2 \">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget responsive-layout none  widget-none  widget-compact-horizontal\" id=\"8d803f96-081d-4768-ab7d-280a77af723b\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-horizontal\"><div class=\"container\">", "<div class=\"row row-sm  \">", "<div class=\"col-sm-3-4 \">", "<div class=\"contents\" data-pb-dropzone=\"contents0\">", "<div class=\"widget general-html none footer-info-container widget-none  widget-compact-vertical\" id=\"b247ecb9-84c9-4762-b270-20f8be1f0ae4\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><div class=\"informa-group-info\">", "<span>Copyright \u00a9 2021 Informa UK Limited</span>", "<span><a href=\"https://informa.com/privacy-policy/\">Privacy policy</a></span>", "<span><a href=\"/cookies\">Cookies</a></span>", "<span><a href=\"/terms-and-conditions\">Terms & conditions</a></span>", "<span><a href=\"/accessibility\">Accessibility</a></span>", "<p>Registered in England & Wales No. 3099067<br />", "5 Howick Place | London | SW1P 1WG</p>", "</div></div>", "</div>", "</div>", "</div>", "</div>", "<div class=\"col-sm-1-4 footer_tandf_logo\">", "<div class=\"contents\" data-pb-dropzone=\"contents1\">", "<div class=\"widget general-image none  widget-none  widget-compact-vertical\" id=\"b6bde365-079b-454f-94f6-1841291656a1\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-vertical\"><a href=\"http://taylorandfrancis.com/\" title=\"Taylor and Francis Group\">", "<img src=\"/pb-assets/Global/Group-logo-white-on-transparent-1468512845090.png\" alt=\"Taylor and Francis Group\" />", "</a></div>", "</div>", "</div>", "</div>", "</div>", "</div>", "</div></div>", "</div>", "</div>", "<div class=\"widget cookiePolicy none  widget-none  widget-compact-all\" id=\"cea739ac-da2c-4d77-9cf1-cb3e0da7e31e\">", "<div class=\"wrapped \">", "<div class=\"widget-body body body-none  body-compact-all\"><div class=\"banner\">", "<a href=\"#\" class=\"btn\">Accept</a>", "<p class=\"message\">We use cookies to improve your website experience. To learn about our use of cookies and how you can manage your cookie settings, please see our <a href=\"/cookies\">Cookie Policy.</a> By closing this message, you are consenting to our use of cookies.</p>", "</div></div>", "</div>", "</div>", "</div>", "</footer></div>", "</div>", "</div>", "</div>", "</div>", "<script type=\"text/javascript\" src=\"/wro/kriw~product.js\"></script>", "<script>", "loadCSS(\"/wro/kriw~lastInBody-css.css\");", "loadCSS(\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\");", "window.scriptSettings=[{js: \"/wro/kriw~jwplayer.js\",selector:'.mediaThumbnailContainer'},", "{js:'/wro/kriw~ajax-widgets.js',css:\"/wro/kriw~ajax-widgets.css\",selector:'.ajaxWidget'},", "{js: '/wro/kriw~loi-api.js',selector:'.toc-fns,.literatumListOfIssuesResponsiveWidget,.literatumListOfIssuesWidget'}", ",{js:\"/wro/kriw~seamless-access-fn.js\",selector: \".seamlessAccess_wrapper,.institutional-login\"}];", "window.addEventListener('load',TandfUtils.scriptLoader);", "</script>", "<noscript>", "    <link rel=\"stylesheet\" href=\"/wro/kriw~lastInBody-css.css\">", "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Droid%20Serif:bold,bolditalic,italic,regular&display=swap\">", "</noscript>", "<script defer src=\"https://static.cloudflareinsights.com/beacon.min.js/v64f9daad31f64f81be21cbef6184a5e31634941392597\" integrity=\"sha512-gV/bogrUTVP2N3IzTDKzgP0Js1gg4fbwtYB6ftgLbKQu/V8yH2+lrKCfKHelh4SO3DPzKj4/glTO+tNJGDnb0A==\" data-cf-beacon='{\"rayId\":\"6b66c92829ef6b18\",\"token\":\"b6951d00f50a499ab38e94f58955e14d\",\"version\":\"2021.11.0\",\"si\":100}' crossorigin=\"anonymous\"></script>", "</body>", "</html>"]}